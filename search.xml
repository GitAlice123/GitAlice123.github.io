<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CUDA编程模型</title>
    <url>/2024/12/08/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p><a class="link"   href="https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/" >https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<ul>
<li>一组<strong>线程</strong>组成一个<strong>CUDA block</strong></li>
<li>一组<strong>CUDA block</strong>组成一个<strong>CUDA grid</strong></li>
</ul>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/CUDAkernel.png"
                      width="50%"
                >

<ul>
<li>每个CUDA block只能在一个CUDA SM上执行，不可以跨SM</li>
<li>每个SM可以跑多个并发的CUDA block</li>
<li>每个线程可以用一个三维的索引来标识自己的位置，比如<code>threadIdx.x</code>, <code>threadIdx.y</code>, <code>threadIdx.z</code></li>
</ul>
<h2 id="一些概念辨析"><a href="#一些概念辨析" class="headerlink" title="一些概念辨析"></a>一些概念辨析</h2><ul>
<li>一直对SM, grid, thread block, warp, CUDA core几个概念有点晕，今天梳理了一下</li>
<li>感觉单独解释有点乱，打算给出一些正确的陈述句，有点像判断题的格式（笑</li>
</ul>
<hr>
<ul>
<li>warp是GPU执行的最小单位<ul>
<li>GPU在执行指令的时候，它会在一个时钟周期里面执行同一条指令的32个线程，也就是一个warp，一个warp就包含32个线程</li>
<li>这个可以看GPU的结构来理解，warp是从GPU硬件的角度来说的，他就一次性执行一个warp</li>
<li>同在一个warp的线程，以<strong>不同的数据执行相同的指令</strong>，这就是SIMT</li>
</ul>
</li>
<li>thread block是分配资源的最小单位<ul>
<li>一个thread block内的资源是共享的，比如<strong>共享内存</strong></li>
</ul>
</li>
<li>CUDA core就有点像CPU里面的ALU，同一个时钟周期内只执行一条指令</li>
<li>grid是所有线程块的集合<ul>
<li>是整个计算任务的所有线程块集合</li>
<li>一个CUDA kernel启动，实际上启动了一整个grid</li>
</ul>
</li>
<li>一个线程块一定只能在一个SM里面执行，不能跨SM，要不然还怎么用共享内存</li>
<li>如果一个线程块里面的线程数量少于32个，那仍然会分配一个32线程的warp，只不过剩下的线程就浪费掉了</li>
<li>程序员在开发时，通过设定block的属性，告诉GPU硬件，我有多少个线程，线程怎么组织。而具体怎么调度由sm的warps scheduler负责，block一旦被分配好SM，该block就会一直驻留在该SM中，直到执行结束。（参考：<a class="link"   href="https://blog.csdn.net/junparadox/article/details/50540602%EF%BC%89" >https://blog.csdn.net/junparadox/article/details/50540602）<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>我之前一直搞不懂，都有了warp了还要block这个概念干啥，其实block的概念更多的是在实际应用中起作用，比如我们算两个矩阵的加法，那么就很自然的会希望每个线程都去处理一个矩阵元素，那就会<strong>希望将负责的线程组织成一个二维的线程块</strong>，正好一一映射过去。那这一个block里面的线程自然是执行相同的代码，这一个block都会进入一个SM，然后可以通过共享内存通信。</li>
</ul>
]]></content>
      <categories>
        <category>CUDA</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
      </tags>
  </entry>
  <entry>
    <title>Makefile一个使用纠错记录</title>
    <url>/2024/12/11/Makefile%E4%B8%80%E4%B8%AA%E4%BD%BF%E7%94%A8%E7%BA%A0%E9%94%99%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<ul>
<li>今天写课设的Makefile，遇到了一个问题，记录一下。<div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 包含路径设置</span></span><br><span class="line">INCLUDES := -IC:/source/ignisos/inc -IC:/source/ignisos/inc -I./user/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 源文件和目标文件</span></span><br><span class="line">SRCS := user/syscall_lib.c user/syscall_wrap.S init/main.c</span><br><span class="line">OBJS := $(SRCS:.c=.o) $(SRCS:.S=.o)</span><br><span class="line">OFiles := user/*.o \</span><br><span class="line">		init/*.o </span><br><span class="line"><span class="comment"># 输出目录和 ELF 文件</span></span><br><span class="line">OUTPUT_DIR := elf</span><br><span class="line">OUTPUT_ELF := <span class="variable">$(OUTPUT_DIR)</span>/user_program.elf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译规则</span></span><br><span class="line"><span class="section">%.o: %.c</span></span><br><span class="line">	<span class="variable">$(CC)</span> <span class="variable">$(CFLAGS)</span> <span class="variable">$(INCLUDES)</span> -std=gnu11 -c -fno-builtin -o <span class="variable">$@</span> <span class="variable">$&lt;</span></span><br><span class="line"></span><br><span class="line"><span class="section">%.o: %.S</span></span><br><span class="line">	<span class="variable">$(CC)</span> <span class="variable">$(CFLAGS)</span> <span class="variable">$(INCLUDES)</span> -c -fno-builtin -o <span class="variable">$@</span> <span class="variable">$&lt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认目标</span></span><br><span class="line"><span class="section">all: <span class="variable">$(OUTPUT_ELF)</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 链接规则</span></span><br><span class="line"><span class="variable">$(OUTPUT_ELF)</span>: <span class="variable">$(OBJS)</span></span><br><span class="line">	@mkdir -p <span class="variable">$(OUTPUT_DIR)</span></span><br><span class="line">	<span class="variable">$(LD)</span> <span class="variable">$(LDFLAGS)</span> -EL -msoft-float -march=m14kc -flto -nostartfiles -nostdlib -static -T user.lds -o <span class="variable">$@</span> <span class="variable">$(OFiles)</span></span><br><span class="line">	<span class="variable">$(OC)</span> --remove-section .MIPS.abiflags --remove-section .reginfo <span class="variable">$@</span></span><br><span class="line">	<span class="variable">$(SZ)</span> <span class="variable">$@</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 清理目标</span></span><br><span class="line"><span class="section">clean:</span></span><br><span class="line">	rm -f user/*.o init/*.o <span class="variable">$(OUTPUT_DIR)</span>/*</span><br><span class="line"></span><br><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: all clean</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">include</span> <span class="keyword">include</span>.mk</span><br></pre></td></tr></table></figure></div></li>
<li>一直在链接的时候出错，老是报类似这样的错误：<div class="code-container" data-rel="Shell"><figure class="iseeu highlight shell"><table><tr><td class="code"><pre><span class="line">init/main.c:1:17: fatal error: lib.h: No such file or directory</span><br><span class="line"><span class="meta prompt_"> #</span><span class="language-bash">include <span class="string">&quot;lib.h&quot;</span></span></span><br><span class="line">                 ^</span><br><span class="line">compilation terminated.</span><br><span class="line">make: *** [init/main.o] Error 1</span><br></pre></td></tr></table></figure></div></li>
<li>然后拿给我对象看，聪明的他发现了问题，在make打印出来的编译命令中，最后链接的这句长这样：<div class="code-container" data-rel="Shell"><figure class="iseeu highlight shell"><table><tr><td class="code"><pre><span class="line">mips-mti-elf-gcc  -nostdlib -static -T user.lds -o elf/user_program.elf user/syscall_lib.o user/syscall_wrap.S init/main.o user/syscall_lib.c user/syscall_wrap.o init/main.c</span><br></pre></td></tr></table></figure></div></li>
<li>我们用的mips套件，里面的链接器用不了，所以只能用gcc</li>
<li>但这里面应该只链接.o文件才对，这句命令里面有.c和.S文件，gcc碰到这几个文件就会开心的跑去编译，结果编译的时候得找头文件啊，我们看看指定头文件路径的编译命令：<div class="code-container" data-rel="Shell"><figure class="iseeu highlight shell"><table><tr><td class="code"><pre><span class="line">mips-mti-elf-gcc -EL -g -march=m14kc -msoft-float -O1 -G0 -IC:/source/ignisos/inc -IC:/source/ignisos/inc -I./user/ -c -fno-builtin -o user/syscall_wrap.o user/syscall_wrap.S</span><br></pre></td></tr></table></figure></div></li>
<li>这里面-I指定了头文件路径，但是我们看上面那个链接的命令里面，是没有的，于是gcc跑去编译的时候，找不到头文件，就报错了</li>
</ul>
]]></content>
      <categories>
        <category>实验室实践</category>
      </categories>
      <tags>
        <tag>Makefile</tag>
      </tags>
  </entry>
  <entry>
    <title>NCCL代码阅读-03</title>
    <url>/2024/12/09/NCCL%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB-03/</url>
    <content><![CDATA[<h2 id="通信组创建和销毁-官网给的例子，解释看注释"><a href="#通信组创建和销毁-官网给的例子，解释看注释" class="headerlink" title="通信组创建和销毁(官网给的例子，解释看注释)"></a>通信组创建和销毁(官网给的例子，解释看注释)</h2><h3 id="一个进程，一个线程，多个设备"><a href="#一个进程，一个线程，多个设备" class="headerlink" title="一个进程，一个线程，多个设备"></a>一个进程，一个线程，多个设备</h3><ul>
<li>在这种单进程的场景下，可以使用ncclCommInitAll()<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> *argv[])</span></span><br><span class="line">&#123;</span><br><span class="line">    ncclComm_t comms[<span class="number">4</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// managing 4 devices</span></span><br><span class="line">    <span class="type">int</span> nDev = <span class="number">4</span>;</span><br><span class="line">    <span class="type">int</span> size = <span class="number">32</span> * <span class="number">1024</span> * <span class="number">1024</span>;</span><br><span class="line">    <span class="type">int</span> devs[<span class="number">4</span>] = &#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这部分的解释见《NCCL代码阅读-02》的cudaAlloc部分</span></span><br><span class="line">    <span class="comment">// allocating and initializing device buffers</span></span><br><span class="line">    <span class="type">float</span> **sendbuff = (<span class="type">float</span> **)<span class="built_in">malloc</span>(nDev * <span class="keyword">sizeof</span>(<span class="type">float</span> *));</span><br><span class="line">    <span class="type">float</span> **recvbuff = (<span class="type">float</span> **)<span class="built_in">malloc</span>(nDev * <span class="keyword">sizeof</span>(<span class="type">float</span> *));</span><br><span class="line">    cudaStream_t *s = (cudaStream_t *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(cudaStream_t) * nDev);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nDev; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        CUDACHECK(cudaSetDevice(i));</span><br><span class="line">        CUDACHECK(cudaMalloc((<span class="type">void</span> **)sendbuff + i, size * <span class="keyword">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">        CUDACHECK(cudaMalloc((<span class="type">void</span> **)recvbuff + i, size * <span class="keyword">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">        CUDACHECK(cudaMemset(sendbuff[i], <span class="number">1</span>, size * <span class="keyword">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">        CUDACHECK(cudaMemset(recvbuff[i], <span class="number">0</span>, size * <span class="keyword">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">        CUDACHECK(cudaStreamCreate(s + i));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// initializing NCCL</span></span><br><span class="line">    NCCLCHECK(ncclCommInitAll(comms, nDev, devs));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// calling NCCL communication API. Group API is required when using</span></span><br><span class="line">    <span class="comment">// multiple devices per thread</span></span><br><span class="line">    <span class="comment">// 单线程控制多个GPU时必须要用group API，否则会死锁</span></span><br><span class="line">    NCCLCHECK(ncclGroupStart());</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nDev; ++i)</span><br><span class="line">        NCCLCHECK(ncclAllReduce((<span class="type">const</span> <span class="type">void</span> *)sendbuff[i], (<span class="type">void</span> *)recvbuff[i], size, ncclFloat, ncclSum,</span><br><span class="line">                                comms[i], s[i]));</span><br><span class="line">    NCCLCHECK(ncclGroupEnd());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// synchronizing on CUDA streams to wait for completion of NCCL operation</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nDev; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        CUDACHECK(cudaSetDevice(i));</span><br><span class="line">        CUDACHECK(cudaStreamSynchronize(s[i]));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// free device buffers</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nDev; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        CUDACHECK(cudaSetDevice(i));</span><br><span class="line">        CUDACHECK(cudaFree(sendbuff[i]));</span><br><span class="line">        CUDACHECK(cudaFree(recvbuff[i]));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// finalizing NCCL</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nDev; ++i)</span><br><span class="line">        ncclCommDestroy(comms[i]);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Success \n&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></li>
</ul>
<h2 id="一个具体的调用路径：sendrecv操作"><a href="#一个具体的调用路径：sendrecv操作" class="headerlink" title="一个具体的调用路径：sendrecv操作"></a>一个具体的调用路径：sendrecv操作</h2><ul>
<li>不得不说nccl的调用真是够复杂的……</li>
<li>我们就从nccl-test（官方给的测试代码）入手，看看sendrecv这个最简单的操作是怎么做的</li>
</ul>
<h3 id="SendRecvRunColl"><a href="#SendRecvRunColl" class="headerlink" title="SendRecvRunColl"></a>SendRecvRunColl</h3><ul>
<li>这个函数是nccl-test里面的一个测试函数，用于测试sendrecv操作<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">testResult_t <span class="title function_">SendRecvRunColl</span><span class="params">(<span class="type">void</span> *sendbuff, <span class="type">void</span> *recvbuff, <span class="type">size_t</span> count, ncclDataType_t type, ncclRedOp_t op, <span class="type">int</span> root, ncclComm_t comm, cudaStream_t stream)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> nRanks;</span><br><span class="line">    NCCLCHECK(ncclCommCount(comm, &amp;nRanks));</span><br><span class="line">    <span class="type">int</span> rank;</span><br><span class="line">    NCCLCHECK(ncclCommUserRank(comm, &amp;rank));</span><br><span class="line">    <span class="type">int</span> recvPeer = (rank - <span class="number">1</span> + nRanks) % nRanks;</span><br><span class="line">    <span class="type">int</span> sendPeer = (rank + <span class="number">1</span>) % nRanks;</span><br><span class="line"></span><br><span class="line">    NCCLCHECK(ncclGroupStart());</span><br><span class="line">    NCCLCHECK(ncclSend(sendbuff, count, type, sendPeer, comm, stream));</span><br><span class="line">    NCCLCHECK(ncclRecv(recvbuff, count, type, recvPeer, comm, stream));</span><br><span class="line">    NCCLCHECK(ncclGroupEnd());</span><br><span class="line">    <span class="keyword">return</span> testSuccess;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></li>
<li>这里可以很明显的看到，接收方和发送方，都要显式调用一次操作，ncclSend和ncclRecv</li>
<li>ncclSend和ncclRecv被包裹在了ncclGroupStart和ncclGroupEnd里面</li>
</ul>
<h3 id="ncclSend和ncclRecv（以ncclSend为例）"><a href="#ncclSend和ncclRecv（以ncclSend为例）" class="headerlink" title="ncclSend和ncclRecv（以ncclSend为例）"></a>ncclSend和ncclRecv（以ncclSend为例）</h3><div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">ncclResult_t <span class="title function_">ncclSend</span><span class="params">(<span class="type">const</span> <span class="type">void</span> *sendbuff, <span class="type">size_t</span> count, ncclDataType_t datatype, <span class="type">int</span> peer,</span></span><br><span class="line"><span class="params">                      ncclComm_t comm, cudaStream_t stream)</span></span><br><span class="line">&#123;</span><br><span class="line">    NvtxParamsSendRecv payload&#123;count * <span class="title function_">ncclTypeSize</span><span class="params">(datatype)</span>, peer&#125;;</span><br><span class="line">    NVTX3_FUNC_WITH_PARAMS(Send, SendRecvSchema, payload)</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclInfo</span> <span class="title">info</span> =</span> &#123;ncclFuncSend, <span class="string">&quot;Send&quot;</span>,</span><br><span class="line">                            <span class="literal">NULL</span>, (<span class="type">void</span> *)sendbuff, count, datatype, ncclSum, peer, comm, stream, <span class="comment">/* Args */</span></span><br><span class="line">                            <span class="number">1</span>, <span class="number">1</span>&#125;;</span><br><span class="line">    ncclResult_t ret;</span><br><span class="line">    NCCLCHECK(ncclGroupStart());</span><br><span class="line">    NCCLCHECKGOTO(ncclEnqueueCheck(&amp;info), ret, <span class="built_in">exit</span>);</span><br><span class="line"><span class="built_in">exit</span>:</span><br><span class="line">    NCCLCHECK(ncclGroupEnd());</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<ul>
<li>这边可以看到，在ncclSend里面有一个很重要的数据结构<strong>ncclInfo</strong>，这个结构体里面包含了这次通信的所有信息，其中<strong>操作种类</strong>是被第一个参数传进去的，ncclInfo这个数据结构的介绍见《NCCL代码阅读-02》</li>
</ul>
<h3 id="ncclEnqueueCheck"><a href="#ncclEnqueueCheck" class="headerlink" title="ncclEnqueueCheck"></a>ncclEnqueueCheck</h3><div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">ncclResult_t <span class="title function_">ncclEnqueueCheck</span><span class="params">(<span class="keyword">struct</span> ncclInfo *info)</span></span><br><span class="line">&#123;</span><br><span class="line">    NCCLCHECK(ncclGroupStartInternal());</span><br><span class="line">    ncclResult_t ret = ncclSuccess;</span><br><span class="line">    <span class="type">int</span> devOld = <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    NCCLCHECKGOTO(CommCheck(info-&gt;comm, info-&gt;opName, <span class="string">&quot;comm&quot;</span>), ret, fail);</span><br><span class="line">    <span class="comment">// Check whether communicator is ready to communicate</span></span><br><span class="line">    NCCLCHECKGOTO(ncclCommEnsureReady(info-&gt;comm), ret, fail);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (info-&gt;comm-&gt;checkPointers)</span><br><span class="line">    &#123;</span><br><span class="line">        CUDACHECKGOTO(cudaGetDevice(&amp;devOld), ret, fail);</span><br><span class="line">        CUDACHECKGOTO(cudaSetDevice(info-&gt;comm-&gt;cudaDev), ret, fail);</span><br><span class="line">    &#125;</span><br><span class="line">    NCCLCHECKGOTO(ArgsCheck(info), ret, fail);</span><br><span class="line"></span><br><span class="line">    INFO(NCCL_COLL, <span class="string">&quot;%s: opCount %lx sendbuff %p recvbuff %p count %zu datatype %d op %d root %d comm %p [nranks=%d] stream %p&quot;</span>,</span><br><span class="line">         info-&gt;opName, info-&gt;comm-&gt;opCount, info-&gt;sendbuff, info-&gt;recvbuff, info-&gt;count,</span><br><span class="line">         info-&gt;datatype, info-&gt;op, info-&gt;root, info-&gt;comm, info-&gt;comm-&gt;nRanks, info-&gt;stream);</span><br><span class="line">    TRACE_CALL(<span class="string">&quot;nccl%s(%&quot;</span> PRIx64 <span class="string">&quot;,%&quot;</span> PRIx64 <span class="string">&quot;,%zu,%d,%d,%d,%p,%p)&quot;</span>, info-&gt;opName, reinterpret_cast&lt;<span class="type">int64_t</span>&gt;(info-&gt;sendbuff), reinterpret_cast&lt;<span class="type">int64_t</span>&gt;(info-&gt;recvbuff), info-&gt;count, info-&gt;datatype, info-&gt;op, info-&gt;root, info-&gt;comm, info-&gt;stream);</span><br><span class="line"></span><br><span class="line">    NCCLCHECKGOTO(taskAppend(info-&gt;comm, info), ret, fail);</span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span>:</span><br><span class="line">    <span class="keyword">if</span> (devOld != <span class="number">-1</span>)</span><br><span class="line">        CUDACHECK(cudaSetDevice(devOld));</span><br><span class="line">    ncclGroupErrCheck(ret);</span><br><span class="line">    NCCLCHECK(ncclGroupEndInternal());</span><br><span class="line">    <span class="comment">/* if depth is 1, ncclGroupEndInternal() will trigger group ops. The state can change</span></span><br><span class="line"><span class="comment">     * so we have to check state here. */</span></span><br><span class="line">    <span class="keyword">if</span> (info-&gt;comm &amp;&amp; !info-&gt;comm-&gt;config.blocking)</span><br><span class="line">    &#123;</span><br><span class="line">        NCCLCHECK(ncclCommGetAsyncError(info-&gt;comm, &amp;ret));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">fail:</span><br><span class="line">    <span class="keyword">if</span> (info-&gt;comm &amp;&amp; !info-&gt;comm-&gt;config.blocking)</span><br><span class="line">        (<span class="type">void</span>)ncclCommSetAsyncError(info-&gt;comm, ret);</span><br><span class="line">    <span class="keyword">goto</span> <span class="built_in">exit</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<ul>
<li>前面就是做了一些入队的检查，真正进行入队操作的是<strong>taskAppend</strong>函数</li>
<li>taskAppend函数将info转换成了一个task，并且将这个task放入对应comm的comm-&gt;planner中，这个planner，即ncclKernelPlanner，是一个比较复杂的数据结构，简要来说就是一个comm上任务的调度器，这个数据结构的介绍后续会放入《NCCL代码阅读-02》</li>
<li>在<strong>ncclGroupEndInternal</strong>里面，调用了<strong>groupLaunch</strong>，<strong>groupLaunch</strong>中的<strong>doLaunches</strong>调用了**ncclLaunchKernel(comm, plan)**，这个函数就是真正的调用CUDA kernel的地方</li>
</ul>
<h3 id="ncclLaunchKernel"><a href="#ncclLaunchKernel" class="headerlink" title="ncclLaunchKernel"></a>ncclLaunchKernel</h3><div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">ncclResult_t <span class="title function_">ncclLaunchKernel</span><span class="params">(<span class="keyword">struct</span> ncclComm *comm, <span class="keyword">struct</span> ncclKernelPlan *plan)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclKernelPlanner</span> *<span class="title">planner</span> =</span> &amp;comm-&gt;planner;</span><br><span class="line">    <span class="type">int</span> nChannels = countOneBits(plan-&gt;channelMask);</span><br><span class="line">    <span class="type">void</span> *sym = plan-&gt;kernelFn;</span><br><span class="line">    dim3 grid = &#123;(<span class="type">unsigned</span>)nChannels, <span class="number">1</span>, <span class="number">1</span>&#125;;</span><br><span class="line">    dim3 block = &#123;(<span class="type">unsigned</span>)plan-&gt;threadPerBlock, <span class="number">1</span>, <span class="number">1</span>&#125;;</span><br><span class="line">    <span class="type">int</span> smem = ncclShmemDynamicSize(comm-&gt;cudaArch);</span><br><span class="line">    cudaStream_t launchStream = planner-&gt;streams-&gt;stream;</span><br><span class="line">    <span class="type">void</span> *extra[] = &#123;</span><br><span class="line">        CU_LAUNCH_PARAM_BUFFER_POINTER, plan-&gt;kernelArgs,</span><br><span class="line">        CU_LAUNCH_PARAM_BUFFER_SIZE, &amp;plan-&gt;kernelArgsSize,</span><br><span class="line">        CU_LAUNCH_PARAM_END&#125;;</span><br><span class="line"></span><br><span class="line">    CUfunction fn;</span><br><span class="line">    CUDACHECK(cudaGetFuncBySymbol(&amp;fn, sym));</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">if</span> CUDART_VERSION &gt;= 11080</span></span><br><span class="line">    <span class="type">int</span> driverVersion;</span><br><span class="line">    NCCLCHECK(ncclCudaDriverVersion(&amp;driverVersion));</span><br><span class="line">    <span class="keyword">if</span> (driverVersion &gt;= <span class="number">11080</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> compCap = comm-&gt;compCap;</span><br><span class="line">        <span class="type">unsigned</span> <span class="type">int</span> clusterSize = (compCap == <span class="number">90</span>) ? comm-&gt;config.cgaClusterSize : <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        CUlaunchConfig launchConfig = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">        CUlaunchAttribute launchAttrs[<span class="number">3</span>];</span><br><span class="line">        <span class="type">int</span> attrs = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">/* Cooperative Group Array (CGA)</span></span><br><span class="line"><span class="comment">         * On sm90 and later we have an extra level of hierarchy where we</span></span><br><span class="line"><span class="comment">         * can group together several blocks within the Grid, called</span></span><br><span class="line"><span class="comment">         * Thread Block Clusters.</span></span><br><span class="line"><span class="comment">         * Clusters enable multiple thread blocks running concurrently</span></span><br><span class="line"><span class="comment">         * across multiple SMs to synchronize and collaboratively fetch</span></span><br><span class="line"><span class="comment">         * and exchange data. A cluster of blocks are guaranteed to be</span></span><br><span class="line"><span class="comment">         * concurrently scheduled onto a group of SMs.</span></span><br><span class="line"><span class="comment">         * The maximum value is 8 and it must be divisible into the grid dimensions</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">if</span> (clusterSize)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// Grid dimension must be divisible by clusterSize</span></span><br><span class="line">            <span class="keyword">if</span> (grid.x % clusterSize)</span><br><span class="line">                clusterSize = <span class="number">1</span>;</span><br><span class="line">            launchAttrs[attrs].id = CU_LAUNCH_ATTRIBUTE_CLUSTER_DIMENSION;</span><br><span class="line">            launchAttrs[attrs++].value.clusterDim = &#123;clusterSize, <span class="number">1</span>, <span class="number">1</span>&#125;;</span><br><span class="line">            launchAttrs[attrs].id = CU_LAUNCH_ATTRIBUTE_CLUSTER_SCHEDULING_POLICY_PREFERENCE;</span><br><span class="line">            launchAttrs[attrs++].value.clusterSchedulingPolicyPreference = CU_CLUSTER_SCHEDULING_POLICY_SPREAD;</span><br><span class="line">        &#125;</span><br><span class="line"><span class="meta">#<span class="keyword">if</span> CUDART_VERSION &gt;= 12000</span></span><br><span class="line">        <span class="keyword">if</span> (compCap &gt;= <span class="number">90</span> &amp;&amp; driverVersion &gt;= <span class="number">12000</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// Set the NCCL Mem Sync domain on CUDA 12.0 and later (sm90)</span></span><br><span class="line">            launchAttrs[attrs].id = CU_LAUNCH_ATTRIBUTE_MEM_SYNC_DOMAIN;</span><br><span class="line">            launchAttrs[attrs++].value.memSyncDomain = (CUlaunchMemSyncDomain)ncclParamMemSyncDomain();</span><br><span class="line">        &#125;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">        launchConfig.gridDimX = grid.x;</span><br><span class="line">        launchConfig.gridDimY = grid.y;</span><br><span class="line">        launchConfig.gridDimZ = grid.z;</span><br><span class="line">        launchConfig.blockDimX = block.x;</span><br><span class="line">        launchConfig.blockDimY = block.y;</span><br><span class="line">        launchConfig.blockDimZ = block.z;</span><br><span class="line">        launchConfig.sharedMemBytes = smem;</span><br><span class="line">        launchConfig.attrs = launchAttrs;</span><br><span class="line">        launchConfig.numAttrs = attrs;</span><br><span class="line">        launchConfig.hStream = launchStream;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// CUDACHECK(cudaLaunchKernelExC(&amp;launchConfig, fnAddr, args));</span></span><br><span class="line">        CUCHECK(cuLaunchKernelEx(&amp;launchConfig, fn, nullptr, extra));</span><br><span class="line">        <span class="keyword">return</span> ncclSuccess;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">    <span class="comment">// Standard kernel launch</span></span><br><span class="line">    CUCHECK(cuLaunchKernel(fn, grid.x, grid.y, grid.z, block.x, block.y, block.z, smem, launchStream, nullptr, extra));</span><br><span class="line">    <span class="comment">// CUDACHECK(cudaLaunchKernel(fnAddr, grid, block, args, smem, launchStream));</span></span><br><span class="line">    <span class="keyword">return</span> ncclSuccess;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<ul>
<li>这里面最后调用的是cuLaunchKernel，其中fn是通过cudaGetFuncBySymbol获取的，这个symbol就是nccl的kernel函数</li>
<li>这个sym来自plan-&gt;kernelFn，它在<strong>scheduleP2pTasksToPlan</strong>中被赋值:<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (!plan-&gt;kernelSpecialized)</span><br><span class="line">&#123;</span><br><span class="line">    plan-&gt;kernelFn = ncclDevKernelForFunc[ncclDevFuncId_P2p()];</span><br><span class="line">    plan-&gt;kernelSpecialized = ncclDevKernelForFuncIsSpecialized[ncclDevFuncId_P2p()];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></li>
<li>其中ncclDevFunId_P2p()是这样的：<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">int</span> <span class="title function_">ncclDevFuncId_P2p</span><span class="params">()</span> &#123; <span class="keyword">return</span> ncclDevFuncRowToId[<span class="number">0</span>]; &#125;</span><br></pre></td></tr></table></figure></div></li>
<li>这个ncclDevFuncRowToId是一个映射表，填写这个映射表的位置还挺难找，在nccl&#x2F;src&#x2F;device&#x2F;下面，有一个<strong>generate.py</strong>，会在build里面生成一个nccl&#x2F;build&#x2F;obj&#x2F;device&#x2F;gensrc&#x2F;host_table.cc</li>
<li>那CUDA是怎么通过fn去找到对应的kernel函数的呢？我们仍然要看generate.py，这个脚本还会生成一组文件，其中一个是nccl&#x2F;build&#x2F;obj&#x2F;device&#x2F;gensrc&#x2F;sendrecv.cu，这里面的内容是这样的：<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;common.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;sendrecv.h&quot;</span></span></span><br><span class="line">DEFINE_ncclDevKernel(SendRecv, ncclFuncSendRecv, FuncCopy, <span class="type">int8_t</span>, NCCL_ALGO_RING, NCCL_PROTO_SIMPLE, <span class="number">589</span>)</span><br><span class="line">DEFINE_ncclDevFunc(SendRecv, ncclFuncSendRecv, FuncCopy, <span class="type">int8_t</span>, NCCL_ALGO_RING, NCCL_PROTO_SIMPLE)</span><br></pre></td></tr></table></figure></div></li>
<li>这里面的两个宏定义在nccl&#x2F;src&#x2F;device&#x2F;common.h里面：<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> DEFINE_ncclDevKernel(suffix, coll, redop, ty, algo, proto, specializedFnId)                    \</span></span><br><span class="line"><span class="meta">    __global__ void ncclDevKernel_##suffix(ncclDevKernelArgs4K NCCL_GRID_CONSTANT const args4K)        \</span></span><br><span class="line"><span class="meta">    &#123;                                                                                                  \</span></span><br><span class="line"><span class="meta">        ncclKernelMain<span class="string">&lt;specializedFnId, RunWorkBatch&lt;coll, ty, redop&lt;ty&gt;</span>, algo, proto&gt;&gt;(&amp;args4K.args); \</span></span><br><span class="line"><span class="meta">    &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DEFINE_ncclDevFunc(suffix, coll, redop, ty, algo, proto) \</span></span><br><span class="line"><span class="meta">    __device__ void ncclDevFunc_##suffix()                       \</span></span><br><span class="line"><span class="meta">    &#123;                                                            \</span></span><br><span class="line"><span class="meta">        RunWorkBatch<span class="string">&lt;coll, ty, redop&lt;ty&gt;</span>, algo, proto&gt;().run();  \</span></span><br><span class="line"><span class="meta">    &#125;</span></span><br></pre></td></tr></table></figure></div></li>
<li>CUDA会通过查找刚刚的<strong>host_table.cc</strong>找到这个<strong>ncclDevKernel_SendRecv</strong>，然后通过这个函数去调用真正的kernel函数（去文件里面搜一下”ncclDevKernel_SendRecv”看一下就大概知道了）</li>
<li>下面我们看看RunWorkBatch是什么东西</li>
</ul>
<h3 id="RunWorkBatch"><a href="#RunWorkBatch" class="headerlink" title="RunWorkBatch"></a>RunWorkBatch</h3><div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">template &lt;ncclFunc_t Fn, typename T, typename RedOp, <span class="type">int</span> Algo, <span class="type">int</span> Proto&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">RunWorkBatch</span>;</span></span><br></pre></td></tr></table></figure></div>
<ul>
<li>这是他的最初原型，在nccl&#x2F;src&#x2F;device&#x2F;sendrecv.h里面，我们可以看到他的一个针对sendrecv的特化：<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">template &lt;typename T, typename RedOp&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">RunWorkBatch</span>&lt;</span>ncclFuncSendRecv, T, RedOp, NCCL_ALGO_RING, NCCL_PROTO_SIMPLE&gt;</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">static_assert</span>(<span class="keyword">sizeof</span>(T) == <span class="number">1</span>, <span class="string">&quot;SendRecv only works on single byte types T.&quot;</span>);</span><br><span class="line"></span><br><span class="line">    template &lt;typename Proto&gt;</span><br><span class="line">    __device__ <span class="type">void</span> <span class="title function_">runSend</span><span class="params">(<span class="type">int</span> tid, <span class="type">int</span> tn, <span class="type">int</span> group, <span class="keyword">struct</span> ncclDevWorkP2p *work)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">size_t</span> bytes = work-&gt;sendBytes;</span><br><span class="line">        <span class="type">int</span> chunkSize = work-&gt;sendIpcReg &amp;&amp; ncclShmem.comm.isNvlink ? (<span class="number">1</span> &lt;&lt; <span class="number">30</span>) : u32fp8Decode(work-&gt;sendChunkSize_u32fp8);</span><br><span class="line">        Primitives&lt;T, RedOp, FanAsymmetric&lt;<span class="number">0</span>, <span class="number">1</span>&gt;, <span class="number">1</span>, Proto, <span class="number">1</span>&gt;</span><br><span class="line">            prims(tid, tn, nullptr, &amp;work-&gt;sendRank, work-&gt;sendAddr, nullptr,</span><br><span class="line">                  <span class="comment">/*redOpArg(ignored)=*/</span><span class="number">0</span>, group, <span class="number">1</span>, <span class="number">1</span>, nullptr,</span><br><span class="line">                  <span class="comment">/*ipcReg=*/</span>work-&gt;sendIpcReg, <span class="comment">/*netReg=*/</span>work-&gt;sendRegistered, ncclShmem.comm.p2pChunkSize);</span><br><span class="line">        <span class="type">size_t</span> cursor = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> n = min(<span class="type">size_t</span>(chunkSize), bytes - cursor);</span><br><span class="line">            prims.directSend(cursor, cursor, n);</span><br><span class="line">            cursor += n;</span><br><span class="line">        &#125; <span class="keyword">while</span> (cursor &lt; bytes &amp;&amp; work-&gt;sendRegistered == <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    template &lt;typename Proto&gt;</span><br><span class="line">    __device__ <span class="type">void</span> <span class="title function_">runRecv</span><span class="params">(<span class="type">int</span> tid, <span class="type">int</span> tn, <span class="type">int</span> group, <span class="keyword">struct</span> ncclDevWorkP2p *work)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">size_t</span> bytes = work-&gt;recvBytes;</span><br><span class="line">        <span class="type">int</span> chunkSize = work-&gt;recvIpcReg &amp;&amp; ncclShmem.comm.isNvlink ? (<span class="number">1</span> &lt;&lt; <span class="number">30</span>) : u32fp8Decode(work-&gt;recvChunkSize_u32fp8);</span><br><span class="line">        Primitives&lt;T, RedOp, FanAsymmetric&lt;<span class="number">1</span>, <span class="number">0</span>&gt;, <span class="number">1</span>, Proto, <span class="number">1</span>&gt;</span><br><span class="line">            prims(tid, tn, &amp;work-&gt;recvRank, nullptr, nullptr, work-&gt;recvAddr,</span><br><span class="line">                  <span class="comment">/*redOpArg(ignored)=*/</span><span class="number">0</span>, group, <span class="number">1</span>, <span class="number">1</span>, nullptr,</span><br><span class="line">                  <span class="comment">/*ipcReg=*/</span>work-&gt;recvIpcReg, <span class="comment">/*netReg=*/</span>work-&gt;recvRegistered, ncclShmem.comm.p2pChunkSize);</span><br><span class="line">        <span class="type">size_t</span> cursor = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> n = min(<span class="type">size_t</span>(chunkSize), bytes - cursor);</span><br><span class="line">            prims.directRecv(cursor, cursor, n);</span><br><span class="line">            cursor += n;</span><br><span class="line">        &#125; <span class="keyword">while</span> (cursor &lt; bytes &amp;&amp; work-&gt;recvRegistered == <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    __device__ __forceinline__ <span class="type">void</span> <span class="title function_">run</span><span class="params">()</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> tn = blockDim.x;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> wid = tid / WARP_SIZE;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> nWarps = tn / WARP_SIZE;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> lane = tid % WARP_SIZE;</span><br><span class="line"></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">Shared</span></span></span><br><span class="line"><span class="class">        &#123;</span></span><br><span class="line">            <span class="type">uint32_t</span> workSendMask; <span class="comment">// bitmasks of which work indices have send/recv</span></span><br><span class="line">            <span class="type">uint32_t</span> workRecvMask;</span><br><span class="line">        &#125;;</span><br><span class="line">        Shared *shared = (Shared *)ncclScratchForWarp(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ncclDevWorkP2p</span> *<span class="title">works</span> =</span> (ncclDevWorkP2p *)ncclShmem.workStorage;</span><br><span class="line">        <span class="type">int</span> nWorks = ncclShmem.nWorks;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (wid == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// Modify the memory range of each work[] to reflect this channel&#x27;s</span></span><br><span class="line">            <span class="comment">// partition of the work. Since integer divides are very heavy it&#x27;s</span></span><br><span class="line">            <span class="comment">// best to do them all in one warp.</span></span><br><span class="line">            <span class="type">int</span> workIx = lane % <span class="number">16</span>;</span><br><span class="line">            <span class="type">int</span> isSend = lane &lt; <span class="number">16</span> ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">            <span class="type">bool</span> hasWork = <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">if</span> (workIx &lt; nWorks)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="class"><span class="keyword">struct</span> <span class="title">ncclDevWorkP2p</span> *<span class="title">work</span> =</span> &amp;works[workIx];</span><br><span class="line">                <span class="type">size_t</span> bytes = isSend ? work-&gt;sendBytes : work-&gt;recvBytes;</span><br><span class="line">                <span class="type">int</span> nParts = isSend ? work-&gt;nSendChannels : work-&gt;nRecvChannels;</span><br><span class="line">                <span class="type">int</span> part = ncclP2pChannelToPart(work-&gt;nP2pChannels, work-&gt;channelBase, ncclShmem.channelId);</span><br><span class="line">                hasWork = (part &lt; nParts);</span><br><span class="line">                <span class="keyword">if</span> (nParts != <span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="type">size_t</span> partBeg, partEnd;</span><br><span class="line">                    ncclP2pPartBounds(nParts, part, bytes, &amp;partBeg, &amp;partEnd);</span><br><span class="line">                    (isSend ? work-&gt;sendAddr : work-&gt;recvAddr) = (<span class="type">char</span> *)(isSend ? work-&gt;sendAddr : work-&gt;recvAddr) + partBeg;</span><br><span class="line">                    (isSend ? work-&gt;sendBytes : work-&gt;recvBytes) = partEnd - partBeg;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// Coverity reports a possible thread divergence due to not all threads participating in the collective.</span></span><br><span class="line">            <span class="comment">// However, the code ensures that the participation is on a per-warp basis.</span></span><br><span class="line">            <span class="comment">// coverity[device_thread_diverged:FALSE]</span></span><br><span class="line">            <span class="type">uint32_t</span> mask = __ballot_sync(~<span class="number">0u</span>, hasWork);</span><br><span class="line">            <span class="keyword">if</span> (lane == <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                shared-&gt;workSendMask = mask &gt;&gt; <span class="number">16</span>;</span><br><span class="line">                shared-&gt;workRecvMask = mask &amp; <span class="number">0xffff</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// The fastest way to compute a warp uniform division x/y in [0,32) is to</span></span><br><span class="line">        <span class="comment">// use each lane to guess a solution and count the ones that don&#x27;t exceed</span></span><br><span class="line">        <span class="comment">// the numerator:</span></span><br><span class="line">        <span class="comment">//   __popc(__ballot_sync(~0u, y*(lane+1) &lt;= x))</span></span><br><span class="line">        <span class="comment">// That takes 1/3 the time of standard division and about 3/4 the time of</span></span><br><span class="line">        <span class="comment">// approximate floating point division:</span></span><br><span class="line">        <span class="comment">//   __float2int_rd(__fdividef(float(x),float(y))).</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// nWarpPerWork = nWarps/nWorks</span></span><br><span class="line">        <span class="type">int</span> nWarpPerWork = __popc(__ballot_sync(~<span class="number">0u</span>, nWorks * (lane + <span class="number">1</span>) &lt;= nWarps));</span><br><span class="line">        <span class="type">int</span> nRecvWarpPerWork = nWarpPerWork &lt;= <span class="number">4</span> ? nWarpPerWork / <span class="number">2</span> : (nWarpPerWork - <span class="number">1</span>) / <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> nSendWarpPerWork = nWarpPerWork &lt;= <span class="number">4</span> ? nRecvWarpPerWork : nRecvWarpPerWork + <span class="number">1</span>;</span><br><span class="line">        <span class="comment">// This might reduce nWarpPerWork which is probably desirable. It is better</span></span><br><span class="line">        <span class="comment">// to have a balanced number of reading and writing threads even if that</span></span><br><span class="line">        <span class="comment">// leaves warps unused.</span></span><br><span class="line">        nWarpPerWork = nSendWarpPerWork + nRecvWarpPerWork;</span><br><span class="line">        <span class="comment">// The work index this warp belongs to: workIx = wid/nWarpPerWork</span></span><br><span class="line">        <span class="type">int</span> workIx = __popc(__ballot_sync(~<span class="number">0u</span>, (lane + <span class="number">1</span>) * nWarpPerWork &lt;= wid));</span><br><span class="line"></span><br><span class="line">        __syncthreads(); <span class="comment">// Wait for works[] and shared-&gt;* to be updated by warp=0</span></span><br><span class="line"></span><br><span class="line">        <span class="type">uint32_t</span> workSendMask = shared-&gt;workSendMask;</span><br><span class="line">        <span class="type">uint32_t</span> workRecvMask = shared-&gt;workRecvMask;</span><br><span class="line"></span><br><span class="line">        __syncthreads(); <span class="comment">// release scratch space used by shared-&gt;*</span></span><br><span class="line">        <span class="keyword">if</span> (nWorks &lt;= workIx)</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Thread range for whole work (send &amp; recv combined)</span></span><br><span class="line">        <span class="type">int</span> subtid = tid - workIx * nWarpPerWork * WARP_SIZE;</span><br><span class="line">        <span class="type">int</span> subtn = nWarpPerWork * WARP_SIZE;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// A send primtive of sufficient size requires 2 cuda barrier ids.</span></span><br><span class="line">        <span class="type">constexpr</span> <span class="type">int</span> nSendWarpsForExtraGroup = NCCL_SIMPLE_EXTRA_GROUP_IF_NTHREADS_GE / WARP_SIZE;</span><br><span class="line">        <span class="comment">// Count up all group ids used below this workIx:</span></span><br><span class="line">        <span class="type">int</span> group, extra;</span><br><span class="line">        <span class="comment">// Each recv gets one group id:</span></span><br><span class="line">        group = __popc(workRecvMask &amp; ((<span class="number">1</span> &lt;&lt; workIx) - <span class="number">1</span>));</span><br><span class="line">        <span class="comment">// Sends accompanying recvs get one and maybe an extra:</span></span><br><span class="line">        extra = (nSendWarpPerWork &gt;= nSendWarpsForExtraGroup) ? <span class="number">1</span> : <span class="number">0</span>;</span><br><span class="line">        group += __popc((workSendMask &amp; workRecvMask) &amp; ((<span class="number">1</span> &lt;&lt; workIx) - <span class="number">1</span>)) * (<span class="number">1</span> + extra);</span><br><span class="line">        <span class="comment">// Sends without recvs use more warps so compute extra accordingly:</span></span><br><span class="line">        extra = (nWarpPerWork &gt;= nSendWarpsForExtraGroup) ? <span class="number">1</span> : <span class="number">0</span>;</span><br><span class="line">        group += __popc((workSendMask &amp; ~workRecvMask) &amp; ((<span class="number">1</span> &lt;&lt; workIx) - <span class="number">1</span>)) * (<span class="number">1</span> + extra);</span><br><span class="line"></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ncclDevWorkP2p</span> *<span class="title">work</span> =</span> &amp;works[workIx];</span><br><span class="line">        <span class="type">bool</span> hasSend = <span class="number">1</span> &amp; (workSendMask &gt;&gt; workIx);</span><br><span class="line">        <span class="type">bool</span> hasRecv = <span class="number">1</span> &amp; (workRecvMask &gt;&gt; workIx);</span><br><span class="line">        <span class="type">bool</span> isCopy = work-&gt;sendRank == ncclShmem.comm.rank;</span><br><span class="line">        <span class="type">bool</span> isSend = !hasRecv || (hasSend &amp;&amp; subtid &lt; nSendWarpPerWork * WARP_SIZE);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!isCopy &amp;&amp; hasSend &amp;&amp; hasRecv)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// Translate thread ids to reflect just this send or recv as opposed to whole work.</span></span><br><span class="line">            <span class="keyword">if</span> (isSend)</span><br><span class="line">            &#123;</span><br><span class="line">                subtn = nSendWarpPerWork * WARP_SIZE;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                subtid -= nSendWarpPerWork * WARP_SIZE;</span><br><span class="line">                subtn = nRecvWarpPerWork * WARP_SIZE;</span><br><span class="line">                group += <span class="number">1</span> + (nSendWarpPerWork &gt;= nSendWarpsForExtraGroup ? <span class="number">1</span> : <span class="number">0</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (isCopy)</span><br><span class="line">        &#123;</span><br><span class="line">            reduceCopy&lt;COLL_UNROLL, RedOp, T, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="comment">/*PreOpSrcs=*/</span><span class="number">0</span>&gt;(subtid, subtn, <span class="number">0</span>, nullptr, <span class="literal">false</span>, <span class="number">1</span>, &amp;work-&gt;sendAddr, <span class="number">1</span>, &amp;work-&gt;recvAddr, (<span class="type">ssize_t</span>)work-&gt;sendBytes);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (isSend)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (work-&gt;sendProtoLL)</span><br><span class="line">            &#123;</span><br><span class="line">                runSend&lt;ProtoLL&gt;(subtid, subtn, group, work);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                runSend&lt;ProtoSimple&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;(subtid, subtn, group, work);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (work-&gt;recvProtoLL)</span><br><span class="line">            &#123;</span><br><span class="line">                runRecv&lt;ProtoLL&gt;(subtid, subtn, group, work);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                runRecv&lt;ProtoSimple&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;(subtid, subtn, group, work);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></div></li>
<li>run里面调用了runSend,runRecv，里面调用了primitives原语，接下来可以前往nccl&#x2F;src&#x2F;device&#x2F;prims_ll128.h等文件查看相关内容</li>
</ul>
]]></content>
      <categories>
        <category>NCCL</category>
      </categories>
      <tags>
        <tag>NCCL</tag>
        <tag>代码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>NCCL代码阅读-04</title>
    <url>/2024/12/15/NCCL%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB-04/</url>
    <content><![CDATA[<p>好的，再更具体一步，假设我们在 <strong>两个 GPU 做 AllReduce</strong> 时，数据的具体划分、传输，以及每一阶段的变化都会明确说明。</p>
<hr>
<h3 id="场景再具体化"><a href="#场景再具体化" class="headerlink" title="场景再具体化"></a><strong>场景再具体化</strong></h3><p>假设：</p>
<ul>
<li><strong>数据大小：16 个 <code>float</code> 元素</strong>（64 字节，总数据量很小，便于解释）。</li>
<li>**数据类型：<code>float</code>**，每个元素 4 字节。</li>
<li><strong>2 个 GPU</strong>（<code>nranks = 2</code>），使用环形拓扑（Ring）。</li>
<li><strong>1 个通道</strong>（<code>nChannels = 1</code>，即所有数据由一个通道处理）。</li>
<li><strong>每个 chunk 大小：8 个元素</strong>（<code>chunkCount = 8</code> 个元素，每个 chunk 为 32 字节）。</li>
</ul>
<h3 id="起始数据分布"><a href="#起始数据分布" class="headerlink" title="起始数据分布"></a><strong>起始数据分布</strong></h3><ul>
<li>每个 GPU 初始有一份独立的数据：<ul>
<li>GPU 0 的数据：<code>[A0, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15]</code></li>
<li>GPU 1 的数据：<code>[B0, B1, B2, B3, B4, B5, B6, B7, B8, B9, B10, B11, B12, B13, B14, B15]</code></li>
</ul>
</li>
</ul>
<p>目标：完成 <strong>AllReduce（sum）</strong>，让每个 GPU 最终都得到：</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">[ A0+B0, A1+B1, ..., A15+B15 ]</span><br></pre></td></tr></table></figure></div>

<hr>
<h3 id="数据划分"><a href="#数据划分" class="headerlink" title="数据划分"></a><strong>数据划分</strong></h3><ol>
<li><p><strong>划分到通道：</strong></p>
<ul>
<li>只有一个通道（<code>nChannels = 1</code>），所有数据量（16 个元素）分配给这个通道。</li>
<li><strong><code>channelCount = 16</code> 个元素</strong>。</li>
</ul>
</li>
<li><p><strong>划分到 chunk：</strong></p>
<ul>
<li>每个 chunk 处理 <code>chunkCount = 8</code> 个元素。</li>
<li><code>channelCount = 16</code> 被划分为 2 个 chunk：<ul>
<li><strong>Chunk 0：<code>[A0, A1, ..., A7]</code></strong></li>
<li><strong>Chunk 1：<code>[A8, A9, ..., A15]</code></strong></li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
<h3 id="运行代码的细节"><a href="#运行代码的细节" class="headerlink" title="运行代码的细节"></a><strong>运行代码的细节</strong></h3><h4 id="第一轮：elemOffset-0，处理第-1-个-chunk"><a href="#第一轮：elemOffset-0，处理第-1-个-chunk" class="headerlink" title="第一轮：elemOffset = 0，处理第 1 个 chunk"></a>第一轮：<code>elemOffset = 0</code>，处理第 1 个 chunk</h4><ul>
<li><strong>数据：Chunk 0（前 8 个元素）</strong><ul>
<li>GPU 0：<code>[A0, A1, ..., A7]</code></li>
<li>GPU 1：<code>[B0, B1, ..., B7]</code></li>
</ul>
</li>
</ul>
<h5 id="1-GPU-0-发送-chunk-0"><a href="#1-GPU-0-发送-chunk-0" class="headerlink" title="(1) GPU 0 发送 chunk 0"></a><strong>(1) GPU 0 发送 chunk 0</strong></h5><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="code"><pre><span class="line">prims.<span class="built_in">directSend</span>(offset, offset, nelem);</span><br></pre></td></tr></table></figure></div>
<ul>
<li>GPU 0 把 <code>[A0, A1, ..., A7]</code> 发送到 GPU 1。</li>
</ul>
<h5 id="2-GPU-1-接收并归约"><a href="#2-GPU-1-接收并归约" class="headerlink" title="(2) GPU 1 接收并归约"></a><strong>(2) GPU 1 接收并归约</strong></h5><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="code"><pre><span class="line">prims.<span class="built_in">directRecvReduceDirectSend</span>(offset, offset, nelem);</span><br></pre></td></tr></table></figure></div>
<ul>
<li>GPU 1 接收 GPU 0 的数据 <code>[A0, A1, ..., A7]</code>，并和自己的数据 <code>[B0, B1, ..., B7]</code> 做归约（<code>sum</code>），结果为：<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">[A0+B0, A1+B1, ..., A7+B7]</span><br></pre></td></tr></table></figure></div></li>
<li>GPU 1 将归约后的结果 <code>[A0+B0, A1+B1, ..., A7+B7]</code> 发送回 GPU 0。</li>
</ul>
<h5 id="3-GPU-0-接收归约结果"><a href="#3-GPU-0-接收归约结果" class="headerlink" title="(3) GPU 0 接收归约结果"></a><strong>(3) GPU 0 接收归约结果</strong></h5><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="code"><pre><span class="line">prims.<span class="built_in">directRecv</span>(offset, offset, nelem);</span><br></pre></td></tr></table></figure></div>
<ul>
<li>GPU 0 接收归约后的数据 <code>[A0+B0, A1+B1, ..., A7+B7]</code>，存入自己的接收缓冲区。</li>
</ul>
<h4 id="第一轮完成，GPU-数据状态："><a href="#第一轮完成，GPU-数据状态：" class="headerlink" title="第一轮完成，GPU 数据状态："></a><strong>第一轮完成，GPU 数据状态：</strong></h4><ul>
<li>GPU 0：<code>[A0+B0, A1+B1, ..., A7+B7, A8, A9, ..., A15]</code></li>
<li>GPU 1：<code>[A0+B0, A1+B1, ..., A7+B7, B8, B9, ..., B15]</code></li>
</ul>
<hr>
<h4 id="第二轮：elemOffset-8，处理第-2-个-chunk"><a href="#第二轮：elemOffset-8，处理第-2-个-chunk" class="headerlink" title="第二轮：elemOffset = 8，处理第 2 个 chunk"></a>第二轮：<code>elemOffset = 8</code>，处理第 2 个 chunk</h4><ul>
<li><strong>数据：Chunk 1（后 8 个元素）</strong><ul>
<li>GPU 0：<code>[A8, A9, ..., A15]</code></li>
<li>GPU 1：<code>[B8, B9, ..., B15]</code></li>
</ul>
</li>
</ul>
<h5 id="1-GPU-0-发送-chunk-1"><a href="#1-GPU-0-发送-chunk-1" class="headerlink" title="(1) GPU 0 发送 chunk 1"></a><strong>(1) GPU 0 发送 chunk 1</strong></h5><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="code"><pre><span class="line">prims.<span class="built_in">directSend</span>(offset, offset, nelem);</span><br></pre></td></tr></table></figure></div>
<ul>
<li>GPU 0 把 <code>[A8, A9, ..., A15]</code> 发送到 GPU 1。</li>
</ul>
<h5 id="2-GPU-1-接收并归约-1"><a href="#2-GPU-1-接收并归约-1" class="headerlink" title="(2) GPU 1 接收并归约"></a><strong>(2) GPU 1 接收并归约</strong></h5><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="code"><pre><span class="line">prims.<span class="built_in">directRecvReduceDirectSend</span>(offset, offset, nelem);</span><br></pre></td></tr></table></figure></div>
<ul>
<li>GPU 1 接收 GPU 0 的数据 <code>[A8, A9, ..., A15]</code>，并和自己的数据 <code>[B8, B9, ..., B15]</code> 做归约（<code>sum</code>），结果为：<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">[A8+B8, A9+B9, ..., A15+B15]</span><br></pre></td></tr></table></figure></div></li>
<li>GPU 1 将归约后的结果 <code>[A8+B8, A9+B9, ..., A15+B15]</code> 发送回 GPU 0。</li>
</ul>
<h5 id="3-GPU-0-接收归约结果-1"><a href="#3-GPU-0-接收归约结果-1" class="headerlink" title="(3) GPU 0 接收归约结果"></a><strong>(3) GPU 0 接收归约结果</strong></h5><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="code"><pre><span class="line">prims.<span class="built_in">directRecv</span>(offset, offset, nelem);</span><br></pre></td></tr></table></figure></div>
<ul>
<li>GPU 0 接收归约后的数据 <code>[A8+B8, A9+B9, ..., A15+B15]</code>，存入自己的接收缓冲区。</li>
</ul>
<h4 id="第二轮完成，GPU-数据状态："><a href="#第二轮完成，GPU-数据状态：" class="headerlink" title="第二轮完成，GPU 数据状态："></a><strong>第二轮完成，GPU 数据状态：</strong></h4><ul>
<li>GPU 0：<code>[A0+B0, A1+B1, ..., A15+B15]</code></li>
<li>GPU 1：<code>[A0+B0, A1+B1, ..., A15+B15]</code></li>
</ul>
<hr>
<h3 id="结果验证"><a href="#结果验证" class="headerlink" title="结果验证"></a><strong>结果验证</strong></h3><p>经过两轮通信，每个 GPU 的数据已经完成了 AllReduce 操作，最终数据是：</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">[A0+B0, A1+B1, ..., A15+B15]</span><br></pre></td></tr></table></figure></div>

<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><h4 id="channelCount-是什么？"><a href="#channelCount-是什么？" class="headerlink" title="channelCount 是什么？"></a><strong><code>channelCount</code> 是什么？</strong></h4><ul>
<li><code>channelCount</code> 表示当前通道需要处理的数据总量（以数据元素为单位）。</li>
<li>在本例中：<ul>
<li>数据总量是 <code>16</code> 个 <code>float</code> 元素。</li>
<li>因为只有 1 个通道，所以 <code>channelCount = 16</code>。</li>
</ul>
</li>
</ul>
<h4 id="chunk-是什么？"><a href="#chunk-是什么？" class="headerlink" title="chunk 是什么？"></a><strong><code>chunk</code> 是什么？</strong></h4><ul>
<li><code>chunk</code> 是通道内划分的最小数据单位。</li>
<li>在本例中：<ul>
<li>每个 <code>chunk</code> 包含 <code>8</code> 个 <code>float</code> 元素。</li>
<li><code>channelCount = 16</code> 被划分为 2 个 <code>chunk</code>。</li>
</ul>
</li>
</ul>
<h4 id="通信的过程："><a href="#通信的过程：" class="headerlink" title="通信的过程："></a><strong>通信的过程：</strong></h4><ol>
<li>每个通道负责一部分数据（<code>channelCount</code>）。</li>
<li>数据进一步被划分为多个 <code>chunk</code>，每次传输一个或多个 <code>chunk</code>。</li>
<li>每个 <code>chunk</code> 在环形拓扑中依次传递，完成发送、接收、归约和最终存储。</li>
</ol>
<p>这样划分的好处是：</p>
<ul>
<li><strong>并行化：</strong> 通道级和 chunk 级的并行分工可以高效利用带宽。</li>
<li><strong>负载均衡：</strong> 每个 GPU 按 <code>chunk</code> 处理一部分数据，避免了单点瓶颈。</li>
</ul>
<p>希望这个具体的例子可以帮助你完全理解 <code>channelCount</code> 和 <code>chunk</code> 的含义以及它们在 AllReduce 中的具体作用！</p>
<ul>
<li><p>directRecvReduceDirectSend</p>
</li>
<li><p>recvReduceSend</p>
</li>
<li><p>GenericOp</p>
</li>
<li><p>directSend有两种：</p>
<ul>
<li>prims_simple走的是:  <div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">  __device__ __forceinline__ <span class="type">void</span> <span class="title function_">directSend</span><span class="params">(<span class="type">intptr_t</span> inpIx, <span class="type">intptr_t</span> outIx, <span class="type">int</span> eltN)</span> &#123;</span><br><span class="line">  genericOp&lt;<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, Input, <span class="number">-1</span>&gt;(inpIx, outIx, eltN, <span class="literal">false</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></li>
<li>ll和ll128走的是:  <div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">  __device__ <span class="type">void</span> <span class="title function_">directSend</span><span class="params">(<span class="type">intptr_t</span> inpIx, <span class="type">intptr_t</span> outIx, <span class="type">int</span> eltN)</span> &#123;</span><br><span class="line">  static_cast&lt;RealPrimitives*&gt;(this)-&gt;send(inpIx, eltN);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">  __device__ <span class="type">void</span> <span class="title function_">send</span><span class="params">(<span class="type">intptr_t</span> inpIx, <span class="type">int</span> eltN)</span> &#123;</span><br><span class="line">  <span class="keyword">return</span> GenericOp&lt;<span class="number">0</span>, <span class="number">1</span>, Input, <span class="number">-1</span>&gt;(inpIx, <span class="number">-1</span>, eltN, <span class="literal">false</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></li>
</ul>
</li>
<li><p>调用reduceCopy的：</p>
<ul>
<li>sendrecv.h</li>
<li>prims_simple.h</li>
<li>reduce_scatter.h</li>
<li>all_gather.h</li>
</ul>
</li>
<li><p>runRing里面调用了</p>
<ul>
<li>prims.directSend(offset, offset, nelem);</li>
<li>prims.directRecvReduceDirectSend(offset, offset, nelem);</li>
<li>prims.directRecvReduceCopyDirectSend(offset, offset, nelem, &#x2F;<em>postOp&#x3D;</em>&#x2F;true);</li>
<li>prims.directRecvCopyDirectSend(offset, nelem);</li>
<li>prims.directRecv(offset, offset, nelem);</li>
</ul>
</li>
<li><p>情况一，采用prims_simple</p>
<ul>
<li>directSend:  <div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">  __device__ __forceinline__ <span class="type">void</span> <span class="title function_">directSend</span><span class="params">(<span class="type">intptr_t</span> inpIx, <span class="type">intptr_t</span> outIx, <span class="type">int</span> eltN)</span> &#123;</span><br><span class="line">  genericOp&lt;<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, Input, <span class="number">-1</span>&gt;(inpIx, outIx, eltN, <span class="literal">false</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
  然后调用genericOp  <div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">  template &lt;<span class="type">int</span> DirectRecv1, <span class="type">int</span> DirectSend1, <span class="type">int</span> Recv, <span class="type">int</span> Send, <span class="type">int</span> SrcBuf, <span class="type">int</span> DstBuf&gt;</span><br><span class="line">__device__ __forceinline__ <span class="type">void</span> <span class="title function_">genericOp</span><span class="params">(</span></span><br><span class="line"><span class="params">    <span class="type">intptr_t</span> srcIx, <span class="type">intptr_t</span> dstIx, <span class="type">int</span> nelem, <span class="type">bool</span> postOp</span></span><br><span class="line"><span class="params">  )</span> &#123;</span><br><span class="line">  <span class="type">constexpr</span> <span class="type">int</span> DirectRecv = <span class="number">1</span> &amp;&amp; Direct &amp;&amp; DirectRecv1;</span><br><span class="line">  <span class="type">constexpr</span> <span class="type">int</span> DirectSend = <span class="number">1</span> &amp;&amp; Direct &amp;&amp; DirectSend1;</span><br><span class="line">  <span class="type">constexpr</span> <span class="type">int</span> Src = SrcBuf != <span class="number">-1</span>;</span><br><span class="line">  <span class="type">constexpr</span> <span class="type">int</span> Dst = DstBuf != <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">  nelem = nelem &lt; <span class="number">0</span> ? <span class="number">0</span> : nelem;</span><br><span class="line">  <span class="type">int</span> sliceSize = stepSize*StepPerSlice;</span><br><span class="line">  sliceSize = max(divUp(nelem, <span class="number">16</span>*SlicePerChunk)*<span class="number">16</span>, sliceSize/<span class="number">32</span>);</span><br><span class="line">  <span class="type">int</span> slice = <span class="number">0</span>;</span><br><span class="line">  <span class="type">int</span> offset = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (tid &lt; nworkers &amp;&amp; offset &lt; nelem &amp;&amp; ((flags &amp; NetRegMode) == <span class="number">0</span>)) &#123;</span><br><span class="line">    <span class="comment">// Worker-only loop for non-empty slices. Non-workers and empty slices are</span></span><br><span class="line">    <span class="comment">// processed in the loop following this if block. The benefit of splitting</span></span><br><span class="line">    <span class="comment">// the loop like this is we pull two branches out of the critical path.</span></span><br><span class="line">    <span class="comment">// Using &quot;number of branch insns (taken or not) encountered dynamically&quot;</span></span><br><span class="line">    <span class="comment">// as the performance metric, then:</span></span><br><span class="line">    <span class="comment">//   perf_orig = 2*numslices</span></span><br><span class="line">    <span class="comment">//   perf_new = 2+numslices</span></span><br><span class="line">    <span class="comment">// So the new code and old code behave the same for numslices=2, and for</span></span><br><span class="line">    <span class="comment">// numslices&gt;2 the new code is superior. And note that in the case</span></span><br><span class="line">    <span class="comment">// numslices=1, the loop is trivially unrollable (single iteration) so we</span></span><br><span class="line">    <span class="comment">// don&#x27;t incur that that tail branch and we still have perf_new=2.</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// ORIGINAL CODE:</span></span><br><span class="line">    <span class="comment">//   unrolled for(slices) &#123;</span></span><br><span class="line">    <span class="comment">//     if(worker) &#123; // This branch removed</span></span><br><span class="line">    <span class="comment">//       wait();</span></span><br><span class="line">    <span class="comment">//       subBarrier();</span></span><br><span class="line">    <span class="comment">//       if(slice not empty) // This branch removed</span></span><br><span class="line">    <span class="comment">//         ReduceCopyMulti();</span></span><br><span class="line">    <span class="comment">//     &#125;</span></span><br><span class="line">    <span class="comment">//     barrier();</span></span><br><span class="line">    <span class="comment">//     post();</span></span><br><span class="line">    <span class="comment">//   &#125; // Since we no longer unroll, new branch added here</span></span><br><span class="line">    <span class="meta">#<span class="keyword">if</span> __CUDA_ARCH__ &lt; 700</span></span><br><span class="line">      <span class="comment">// Above doesn&#x27;t matter on older hardware.</span></span><br><span class="line">      <span class="meta">#<span class="keyword">pragma</span> unroll SlicePerChunk</span></span><br><span class="line">    <span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">      <span class="meta">#<span class="keyword">pragma</span> unroll 1</span></span><br><span class="line">    <span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">      sliceSize = sliceSize &lt; nelem-offset ? sliceSize : nelem-offset;</span><br><span class="line">      <span class="keyword">if</span> (tid == <span class="number">0</span>) &#123;</span><br><span class="line">        T* userInput = (T*)ncclShmem.groups[group].userInput;</span><br><span class="line">        T* userOutput = (T*)ncclShmem.groups[group].userOutput;</span><br><span class="line">        <span class="keyword">if</span> (Src) ncclShmem.groups[group].srcs[<span class="number">0</span>] = (SrcBuf==Input ? userInput : userOutput) + srcIx + offset;</span><br><span class="line">        <span class="keyword">if</span> (Dst) ncclShmem.groups[group].dsts[<span class="number">0</span>] = (DstBuf==Input ? userInput : userOutput) + dstIx + offset;</span><br><span class="line">      &#125;</span><br><span class="line">      waitPeer&lt;DirectRecv, DirectSend, Recv, Send, Src, Dst&gt;(srcIx, dstIx, offset, sliceSize);</span><br><span class="line">      subBarrier();</span><br><span class="line">      <span class="comment">/* if user abort the kernel, we don&#x27;t need to actually perform copy/reduce; just set size</span></span><br><span class="line"><span class="comment">       * to 0 to avoid unnecessary workload. */</span></span><br><span class="line">      <span class="type">int</span> workSize = ncclShmem.aborted ? <span class="number">0</span> : sliceSize;</span><br><span class="line">      <span class="keyword">if</span> (flags &amp; AnyNetDeviceUnpack) &#123;</span><br><span class="line">        ncclNetDeviceUnpack&lt;Recv&gt;(tid, tidInBlock, nworkers, group, ncclShmem.groups[group].devicePlugin.unpack.unpackNetDeviceIndexMask, Src, workSize);</span><br><span class="line">        <span class="comment">// Sync here to make sure all workers are reading from the updated srcs)</span></span><br><span class="line">        subBarrier();</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (DirectRecv &amp;&amp; ncclShmem.groups[group].srcs[<span class="number">0</span>] == ncclShmem.groups[group].dsts[<span class="number">0</span>]</span><br><span class="line">          <span class="comment">/* NVLS can have srcs[0] == dsts[0], but we cannot enter this &quot;if branch&quot;,</span></span><br><span class="line"><span class="comment">           * so we need to check whether MultimemSrcs and MultimemDsts are 0. */</span></span><br><span class="line">          &amp;&amp; MultimemSrcs == <span class="number">0</span> &amp;&amp; MultimemDsts == <span class="number">0</span> &amp;&amp; !Src) &#123;</span><br><span class="line">        <span class="comment">// We can only have one direct receive. Since srcs[0] == dstPtr+offset, skip one copy</span></span><br><span class="line">        <span class="keyword">if</span> (Send) &#123;</span><br><span class="line">          reduceCopy&lt;Unroll, RedOp, T, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, MaxSend, <span class="comment">/*PreOpSrcs*/</span><span class="number">0</span>&gt;</span><br><span class="line">            (tid, nworkers, <span class="comment">/*redArg*/</span><span class="number">0</span>, <span class="comment">/*preOpArgs*/</span>nullptr, <span class="comment">/*postOp*/</span><span class="literal">false</span>,</span><br><span class="line">             <span class="number">1</span>, ncclShmem.groups[group].srcs,</span><br><span class="line">             fan.nsend(), ncclShmem.groups[group].dsts+<span class="number">1</span>,</span><br><span class="line">             workSize);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (DirectSend &amp;&amp; !DirectRecv &amp;&amp; SrcBuf != Input &amp;&amp; ncclShmem.groups[group].dsts[Dst] == nullptr) &#123;</span><br><span class="line">        <span class="comment">// For broadcast in CollNet to do empty send</span></span><br><span class="line">        reduceCopy&lt;Unroll, RedOp, T, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="comment">/*PreOpSrcs*/</span><span class="number">0</span>&gt;</span><br><span class="line">          (tid, nworkers, ncclShmem.redOpArgs[<span class="number">0</span>],  nullptr, postOp,</span><br><span class="line">           Recv, ncclShmem.groups[group].srcs,</span><br><span class="line">           Dst, ncclShmem.groups[group].dsts,</span><br><span class="line">           workSize);</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ncclShmem.groups[group].srcs[<span class="number">0</span>] &amp;&amp; ncclShmem.groups[group].dsts[<span class="number">0</span>]) &#123;</span><br><span class="line">        <span class="type">constexpr</span> <span class="type">int</span> PreOpSrcs = SrcBuf != Input ? <span class="number">0</span> :</span><br><span class="line">                                  DirectRecv*MaxRecv == NCCL_MAX_DIRECT_ARITY ? (<span class="number">1</span>+NCCL_MAX_DIRECT_ARITY) : <span class="number">1</span>;</span><br><span class="line">        reduceCopy&lt;Unroll, RedOp, T,</span><br><span class="line">          MultimemSrcs, Recv+Src, Recv*MaxRecv+Src,</span><br><span class="line">          MultimemDsts, Send+Dst, Send*MaxSend+Dst, PreOpSrcs&gt;</span><br><span class="line">          (tid, nworkers, ncclShmem.redOpArgs[<span class="number">0</span>], ncclShmem.redOpArgs, postOp,</span><br><span class="line">           Recv*fan.nrecv()+Src, ncclShmem.groups[group].srcs,</span><br><span class="line">           Send*fan.nsend()+Dst, ncclShmem.groups[group].dsts,</span><br><span class="line">           workSize);</span><br><span class="line">      &#125;</span><br><span class="line">      barrier(); <span class="comment">// This barrier has a counterpart in following loop</span></span><br><span class="line">      postPeer&lt;Recv, Send&gt;(<span class="number">0</span> &lt; sliceSize);</span><br><span class="line">      offset += sliceSize;</span><br><span class="line">      slice += <span class="number">1</span>;</span><br><span class="line">      <span class="comment">// Yes, for some template arguments this code will be unreachable.  That&#x27;s fine.</span></span><br><span class="line">      <span class="comment">// coverity[dead_error_line]</span></span><br><span class="line">    &#125; <span class="keyword">while</span> (slice &lt; SlicePerChunk &amp;&amp; offset &lt; nelem);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Non-workers come straight here. Workers too but only once the remaining</span></span><br><span class="line">  <span class="comment">// slices are all empty. Since empty slices are the uncommon case, and</span></span><br><span class="line">  <span class="comment">// worker perf is the limiter, perf-wise this loop is effectively unentered,</span></span><br><span class="line">  <span class="comment">// hence just a single branch insn.</span></span><br><span class="line">  <span class="meta">#<span class="keyword">pragma</span> unroll 1</span></span><br><span class="line">  <span class="keyword">while</span> (slice &lt; SlicePerChunk) &#123;</span><br><span class="line">    sliceSize = sliceSize &lt; nelem-offset ? sliceSize : nelem-offset;</span><br><span class="line">    &#123; <span class="comment">// Only workers could have Wait roles so we know the slice must be empty</span></span><br><span class="line">      <span class="comment">// since we&#x27;ve exited the loop above.</span></span><br><span class="line">      waitPeer&lt;DirectRecv, DirectSend, Recv, Send, Src, Dst&gt;(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    barrier(); <span class="comment">// Has couterpart in preceding worker-only loop.</span></span><br><span class="line">    postPeer&lt;Recv, Send&gt;(<span class="number">0</span> &lt; sliceSize);</span><br><span class="line">    offset += sliceSize;</span><br><span class="line">    slice += <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
  然后里面directSend的条件下，调用了reduceCopy，进而调用reduceCopyPacks</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>NCCL</category>
      </categories>
      <tags>
        <tag>NCCL</tag>
        <tag>代码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>NCCL代码阅读-nccltest篇</title>
    <url>/2024/12/28/NCCL%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB-nccltest%E7%AF%87/</url>
    <content><![CDATA[<h2 id="本篇内容简介"><a href="#本篇内容简介" class="headerlink" title="本篇内容简介"></a>本篇内容简介</h2><ul>
<li>我们以AllReduce操作为例，来说明整个nccl-test的调用栈</li>
<li>我们的场景是单机，两卡，两个进程（一个进程管一个卡）</li>
<li>我们输入的shell命令主要如下：<div class="code-container" data-rel="Shell"><figure class="iseeu highlight shell"><table><tr><td class="code"><pre><span class="line">mpirun --allow-run-as-root -n 2 ./build/all_reduce_perf -f 2 -b 1m -e 1m -C 1</span><br><span class="line">mpirun --allow-run-as-root -n 2 ./build/all_reduce_perf -f 2 -b 1m -e 1m</span><br></pre></td></tr></table></figure></div></li>
<li>解释一下各参数<ul>
<li>-f是factor，数据大小每次增长倍数</li>
<li>-b是begin，-b 1m表示数据大小从1m开始增长</li>
<li>-e是end，-e 1m表示数据大小增长到1m</li>
<li>-C是cpu时间，-C为1表示最终数据会打印CPU的时间（同步部分），不含下面GPU异步执行的时间，CPU做完他的工作就返回了；反之就显示等待GPU完成工作的总时间</li>
</ul>
</li>
</ul>
<h2 id="main"><a href="#main" class="headerlink" title="main"></a>main</h2><ul>
<li>common.cu中的main函数，是整个程序的入口</li>
<li>根据命令parse之后，写了这么几个参数<ul>
<li>nThread（我们这个命令下就是1）</li>
<li>nGpus（我们这个命令下就是1）</li>
<li>minBytes（就是-b的值换成B为单位）</li>
<li>maxBytes（就是-e的值换成B为单位）</li>
<li>step（-f）</li>
<li>warmup iters（默认是5）</li>
<li>iters（默认是20）</li>
<li>agg iters（默认是1）</li>
<li>validation（默认是1） </li>
<li>graph（默认是0）</li>
<li>还有一些其他的参数，我们的命令里面都没加，有需要的去README里面对着看看</li>
</ul>
</li>
<li>然后进入run函数</li>
</ul>
<h2 id="run"><a href="#run" class="headerlink" title="run"></a>run</h2><h3 id="初始化MPI环境和NCCL配置"><a href="#初始化MPI环境和NCCL配置" class="headerlink" title="初始化MPI环境和NCCL配置"></a>初始化MPI环境和NCCL配置</h3><ul>
<li>会让每个MPI进程获取到自己的rank</li>
<li>还会按照color对进程分组（这个我们的命令用不到）</li>
</ul>
<h3 id="设备初始化和内存分配"><a href="#设备初始化和内存分配" class="headerlink" title="设备初始化和内存分配"></a>设备初始化和内存分配</h3><ul>
<li>打印设备信息</li>
<li>计算每个进程可以使用的最大内存，由设备当前内存来确定</li>
</ul>
<h3 id="NCCL初始化"><a href="#NCCL初始化" class="headerlink" title="NCCL初始化"></a>NCCL初始化</h3><ul>
<li><strong>生成NCCL唯一的ID</strong>，由第一个进程生成这个ID，然后用MPI广播给所有进程</li>
<li>这边另外提一下，每个MPI进程里面的comm数量&#x3D;nthreads*nGPUs（每个进程要管的GPU数量），nGPUs不是物理上有多少个GPU，而是一个线程要管一个GPU的话，就要有一个comm来作为控制块</li>
<li>每个进程分配的sendBuffer和recvBuffer的数量也是nthreads*nGPUs</li>
<li>根据 ncclProc（当前进程在 NCCL 子集中的 rank）来初始化 NCCL 通信。对于多个进程，需要通过 ncclCommInitRank 初始化每个进程的 NCCL 通信，主要就是初始化comm结构体</li>
</ul>
<h3 id="内存分配"><a href="#内存分配" class="headerlink" title="内存分配"></a>内存分配</h3><ul>
<li>为每个 GPU 分配内存，用于存储发送和接收数据的缓冲区（sendbuffs 和 recvbuffs）</li>
<li>expected部分用来验证传输的数据结果是否正确，开启dataCheck的时候有用</li>
</ul>
<h3 id="设置线程的运行参数"><a href="#设置线程的运行参数" class="headerlink" title="设置线程的运行参数"></a>设置线程的运行参数</h3><ul>
<li>要设置的就是这些线程：testThread threads[nThreads]</li>
<li>设置threads[t].args的值，这个args会被作为所有任务的信息传到nccl中执行</li>
</ul>
<h3 id="进入threadRunTests"><a href="#进入threadRunTests" class="headerlink" title="进入threadRunTests"></a>进入threadRunTests</h3><h2 id="threadRunTests"><a href="#threadRunTests" class="headerlink" title="threadRunTests"></a>threadRunTests</h2><ul>
<li>cudaSetDevice指定一下后面要用的GPU是哪张</li>
<li>进入ncclTestEngine.runTest，这里allreduce会进入allReduceEngine的runTest<ul>
<li>是怎么确定要进入allReduceEngine的请见《pragma-weak初遇》</li>
</ul>
</li>
</ul>
<h2 id="AllReduceRunTest"><a href="#AllReduceRunTest" class="headerlink" title="AllReduceRunTest"></a>AllReduceRunTest</h2><ul>
<li>主要就是进入TimeTest，传的最重要的参数还是刚刚上面的args</li>
<li>顺便提一下，nccltype（后面的type等，总之是数据类型）默认是ncclFloat（查一下表就是float），ncclop默认是ncclSum</li>
</ul>
<h2 id="TimeTest"><a href="#TimeTest" class="headerlink" title="TimeTest"></a>TimeTest</h2><ul>
<li>跑几个warmup的iter</li>
<li>进入BenchTime，跑两个实验，一个是in-place，一个是相反的</li>
</ul>
<h2 id="BenchTime"><a href="#BenchTime" class="headerlink" title="BenchTime"></a>BenchTime</h2><ul>
<li>首先会算一个count，一共有多少个<strong>数据元素</strong><div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">size_t</span> count = args-&gt;nbytes / wordSize(type);</span><br></pre></td></tr></table></figure></div></li>
<li>然后开始startColl，这就是真正要下去执行的地方了</li>
<li>注意下面有两个时间<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">double</span> cputimeSec = tim.elapsed()/(iters*agg_iters);</span><br><span class="line">TESTCHECK(completeColl(args));</span><br><span class="line"></span><br><span class="line"><span class="type">double</span> deltaSec = tim.elapsed();</span><br><span class="line">deltaSec = deltaSec/(iters*agg_iters);</span><br><span class="line"><span class="keyword">if</span> (cudaGraphLaunches &gt;= <span class="number">1</span>) deltaSec = deltaSec/cudaGraphLaunches;</span><br><span class="line">Allreduce(args, &amp;deltaSec, average);</span><br></pre></td></tr></table></figure></div></li>
<li>cputimeSec测的是在CPU把任务下发之后就返回的时间，而deltaSec测的是等到completeColl结束之后的总时间，要等待GPU完成</li>
</ul>
<h2 id="startColl"><a href="#startColl" class="headerlink" title="startColl"></a>startColl</h2><ul>
<li>上面会对in-place的方法给出一些内存的偏移，防止重叠</li>
<li>主要函数就是这个：<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">TESTCHECK(args-&gt;collTest-&gt;runColl(</span><br><span class="line">      (<span class="type">void</span>*)(in_place ? recvBuff + args-&gt;sendInplaceOffset*rank : sendBuff),</span><br><span class="line">      (<span class="type">void</span>*)(in_place ? recvBuff + args-&gt;recvInplaceOffset*rank : recvBuff),</span><br><span class="line">    count, type, op, root, args-&gt;comms[i], args-&gt;streams[i]));</span><br></pre></td></tr></table></figure></div></li>
</ul>
<h2 id="runColl"><a href="#runColl" class="headerlink" title="runColl"></a>runColl</h2><ul>
<li>相同的父类实例化方法，往下找会找到，在AllReduce中我们实际调用的是AllReduceRunColl</li>
<li>接下来调用<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">NCCLCHECK(ncclAllReduce(sendbuff, recvbuff, count, type, op, comm, stream));</span><br></pre></td></tr></table></figure></div></li>
<li>正式进入NCCL的逻辑！</li>
</ul>
]]></content>
      <categories>
        <category>NCCL</category>
      </categories>
      <tags>
        <tag>NCCL</tag>
        <tag>代码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo常用命令</title>
    <url>/2024/12/04/hexo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h2 id="新建一篇文章"><a href="#新建一篇文章" class="headerlink" title="新建一篇文章"></a>新建一篇文章</h2><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">hexo new <span class="string">&quot;文章标题&quot;</span></span><br></pre></td></tr></table></figure></div>

<h2 id="本地重新生成静态文件"><a href="#本地重新生成静态文件" class="headerlink" title="本地重新生成静态文件"></a>本地重新生成静态文件</h2><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br></pre></td></tr></table></figure></div>

<h2 id="发布到github-io"><a href="#发布到github-io" class="headerlink" title="发布到github.io"></a>发布到github.io</h2><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure></div>

<h2 id="在文章里面插入图片"><a href="#在文章里面插入图片" class="headerlink" title="在文章里面插入图片"></a>在文章里面插入图片</h2><ul>
<li>首先把图片放到source&#x2F;images下面</li>
<li>然后文章里插入：<div class="code-container" data-rel="Markdown"><figure class="iseeu highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;/images/图片文件名&quot;</span> <span class="attr">width</span>=<span class="string">&quot;50%&quot;</span>&gt;</span></span></span><br></pre></td></tr></table></figure></div></li>
</ul>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>部署</tag>
      </tags>
  </entry>
  <entry>
    <title>NCCL代码阅读-01</title>
    <url>/2024/11/28/NCCL%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB-01/</url>
    <content><![CDATA[<h2 id="nccl源码结构"><a href="#nccl源码结构" class="headerlink" title="nccl源码结构"></a>nccl源码结构</h2><h3 id="整体结构概览"><a href="#整体结构概览" class="headerlink" title="整体结构概览"></a>整体结构概览</h3><div class="code-container" data-rel="Shell"><figure class="iseeu highlight shell"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── bootstrap.cc</span><br><span class="line">├── channel.cc</span><br><span class="line">├── collectives.cc</span><br><span class="line">├── debug.cc</span><br><span class="line">├── device</span><br><span class="line">│   ├── all_gather.h</span><br><span class="line">│   ├── all_reduce.h</span><br><span class="line">│   ├── broadcast.h</span><br><span class="line">│   ├── common.cu</span><br><span class="line">│   ├── common.h</span><br><span class="line">│   ├── common_kernel.h</span><br><span class="line">│   ├── generate.py</span><br><span class="line">│   ├── Makefile</span><br><span class="line">│   ├── network</span><br><span class="line">│   │   └── unpack</span><br><span class="line">│   │       ├── unpack_defs.h</span><br><span class="line">│   │       └── unpack.h</span><br><span class="line">│   ├── onerank.cu</span><br><span class="line">│   ├── op128.h</span><br><span class="line">│   ├── primitives.h</span><br><span class="line">│   ├── prims_ll128.h</span><br><span class="line">│   ├── prims_ll.h</span><br><span class="line">│   ├── prims_simple.h</span><br><span class="line">│   ├── reduce.h</span><br><span class="line">│   ├── reduce_kernel.h</span><br><span class="line">│   ├── reduce_scatter.h</span><br><span class="line">│   └── sendrecv.h</span><br><span class="line">├── enhcompat.cc</span><br><span class="line">├── enqueue.cc</span><br><span class="line">├── graph</span><br><span class="line">│   ├── connect.cc</span><br><span class="line">│   ├── paths.cc</span><br><span class="line">│   ├── rings.cc</span><br><span class="line">│   ├── rings.h</span><br><span class="line">│   ├── search.cc</span><br><span class="line">│   ├── topo.cc</span><br><span class="line">│   ├── topo.h</span><br><span class="line">│   ├── trees.cc</span><br><span class="line">│   ├── tuning.cc</span><br><span class="line">│   ├── xml.cc</span><br><span class="line">│   └── xml.h</span><br><span class="line">├── group.cc</span><br><span class="line">├── include</span><br><span class="line">│   ├── 略</span><br><span class="line">├── init.cc</span><br><span class="line">├── init_nvtx.cc</span><br><span class="line">├── Makefile</span><br><span class="line">├── misc</span><br><span class="line">│   ├── argcheck.cc</span><br><span class="line">│   ├── cudawrap.cc</span><br><span class="line">│   ├── gdrwrap.cc</span><br><span class="line">│   ├── ibvsymbols.cc</span><br><span class="line">│   ├── ibvwrap.cc</span><br><span class="line">│   ├── ipcsocket.cc</span><br><span class="line">│   ├── nvmlwrap.cc</span><br><span class="line">│   ├── param.cc</span><br><span class="line">│   ├── profiler.cc</span><br><span class="line">│   ├── shmutils.cc</span><br><span class="line">│   ├── socket.cc</span><br><span class="line">│   ├── strongstream.cc</span><br><span class="line">│   ├── tuner.cc</span><br><span class="line">│   └── utils.cc</span><br><span class="line">├── nccl.h.in</span><br><span class="line">├── nccl.pc.in</span><br><span class="line">├── net.cc</span><br><span class="line">├── proxy.cc</span><br><span class="line">├── register.cc</span><br><span class="line">├── transport</span><br><span class="line">│   ├── coll_net.cc</span><br><span class="line">│   ├── generic.cc</span><br><span class="line">│   ├── net.cc</span><br><span class="line">│   ├── net_ib.cc</span><br><span class="line">│   ├── net_socket.cc</span><br><span class="line">│   ├── nvls.cc</span><br><span class="line">│   ├── p2p.cc</span><br><span class="line">│   └── shm.cc</span><br><span class="line">└── transport.cc</span><br></pre></td></tr></table></figure></div>
<h3 id="device"><a href="#device" class="headerlink" title="device"></a>device</h3><ul>
<li>device目录里面就是给GPU执行的代码了，可以看到里面函数前面的__device__等标志</li>
</ul>
<h4 id="common-h"><a href="#common-h" class="headerlink" title="common.h"></a>common.h</h4><ul>
<li>定义了ncclShmemGroup和ncclShmemData结构体</li>
<li>定义了**<strong>shared</strong> ncclShmemData ncclShmem**，shared修饰表示是GPU一个block中所有thread的共享内存</li>
<li>一些用于同步的汇编代码</li>
<li>定义了copyToShmem16函数，用于将一些数据（其实不是主要数据搬运，代码里可以看到，主要是用于搬运comm和channel这些控制信息的）从GPU的全局内存搬运到共享内存里</li>
<li>定义了loadWorkBatchToShmem函数，用于从全局内存加载工作批次到共享内存中。这些批次由 ncclShmem 中的 workStorage 存储，每个线程负责加载不同的部分。通过 tid 和 tn，每个线程计算自己需要处理的工作项，并将其加载到共享内存中</li>
<li>声明了RunWorkColl结构体，实际执行一个具体的集体操作计算</li>
<li>声明了专门用于P2P操作的RunWorkBatch结构体，在sendrecv.h里面实现</li>
<li>定义了通用的RunWorkBatch结构体，划分一下本线程要负责的work，然后调用RunWorkColl实际处理、</li>
<li>定义了ncclKernelMain函数，主函数，初始化该进程块负责哪个通道，然后执行每个批次的集体操作</li>
<li>宏定义DEFINE_ncclDevKernel和DEFINE_ncclDevFunc，见NCCL代码阅读-03</li>
</ul>
<h2 id="创建一个通信组-communicator"><a href="#创建一个通信组-communicator" class="headerlink" title="创建一个通信组(communicator)"></a>创建一个通信组(communicator)</h2><ul>
<li>创建一个通信组之前，每个CUDA设备都要被分配一个唯一的rank id</li>
<li>有了这个rank id和CUDA设备的静态映射，ncclCommInitRank(), ncclCommInitRankConfig() and ncclCommInitAll() 三个函数会创建communicator objects，每个communicator object会和一个固定的rank（及一个CUDA设备）关联。</li>
<li>在调用ncclCommInitRank之前，需要调用ncclGetUniqueId()来获取一个unique id，这个ID必须广播到所有参与通信的进程，让他们知道自己在communicator中</li>
<li>比如有四个GPU互相通信，加入了一个通信组，那么这个通信组就需要一个通信上下文记录所有的信息</li>
<li>类比四个人开会，那么这个通信上下文就是会议室</li>
</ul>
<h3 id="ncclCommInitRank"><a href="#ncclCommInitRank" class="headerlink" title="ncclCommInitRank"></a>ncclCommInitRank</h3><div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">ncclResult_t <span class="title function_">ncclCommInitRank</span><span class="params">(ncclComm_t* comm, <span class="type">int</span> nranks, ncclUniqueId commId, <span class="type">int</span> rank)</span></span><br></pre></td></tr></table></figure></div>
<ul>
<li>创建一个communicator object</li>
<li>里面调用ncclCommInitRankDev()</li>
</ul>
<h3 id="ncclCommInitAll"><a href="#ncclCommInitAll" class="headerlink" title="ncclCommInitAll"></a>ncclCommInitAll</h3><ul>
<li>在<strong>一个CPU进程</strong>里面执行(<strong>因此他后面所调用的所有函数都是在这一个进程，一个线程里面执行的</strong>)，创建多个communicator object</li>
<li>但是只能是单进程版本，也因此不支持多node通信</li>
<li>首先检查了各种数据的有效性</li>
<li>然后调用ncclGetUniqueId()获取一个unique id<ul>
<li>ncclGetUniqueId()首先调用ncclInit()初始化NCCL</li>
</ul>
</li>
</ul>
<h3 id="ncclInit"><a href="#ncclInit" class="headerlink" title="ncclInit()"></a>ncclInit()</h3><ul>
<li>这是一个在所有线程中只会执行一次的函数</li>
<li>在两个地方被调用：ncclGetUniqueId和ncclCommInitRankDev</li>
<li>如果是ncclGetUniqueId调用的，那么分两种情况：<ul>
<li>在ncclCommInitAll中调用，那其实就一个进程，一个线程，不用担心会被多次调用</li>
<li>在ncclCommInitRank前面调用，那么就要限制只有第一个线程调用，后面的线程不会再调用</li>
</ul>
</li>
</ul>
<h2 id="SendRecv的调用流程"><a href="#SendRecv的调用流程" class="headerlink" title="SendRecv的调用流程"></a>SendRecv的调用流程</h2><ul>
<li><p>nccl-test中，sendrecv.cu</p>
<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">testResult_t <span class="title function_">SendRecvRunColl</span><span class="params">(<span class="type">void</span> *sendbuff, <span class="type">void</span> *recvbuff, <span class="type">size_t</span> count, ncclDataType_t type, ncclRedOp_t op, <span class="type">int</span> root, ncclComm_t comm, cudaStream_t stream)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> nRanks;</span><br><span class="line">    NCCLCHECK(ncclCommCount(comm, &amp;nRanks));</span><br><span class="line">    <span class="type">int</span> rank;</span><br><span class="line">    NCCLCHECK(ncclCommUserRank(comm, &amp;rank));</span><br><span class="line">    <span class="type">int</span> recvPeer = (rank - <span class="number">1</span> + nRanks) % nRanks;</span><br><span class="line">    <span class="type">int</span> sendPeer = (rank + <span class="number">1</span>) % nRanks;</span><br><span class="line"></span><br><span class="line">    NCCLCHECK(ncclGroupStart());</span><br><span class="line">    <span class="comment">// 显式对sendPeer和recvPeer进行send和recv操作</span></span><br><span class="line">    NCCLCHECK(ncclSend(sendbuff, count, type, sendPeer, comm, stream));</span><br><span class="line">    NCCLCHECK(ncclRecv(recvbuff, count, type, recvPeer, comm, stream));</span><br><span class="line">    NCCLCHECK(ncclGroupEnd());</span><br><span class="line">    <span class="keyword">return</span> testSuccess;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></li>
<li><p>nccl的collectives.cc中，注意在info里面传递的第一个参数是comm，也就是操作类型，后续在enqueucheck里面把操作类型用info传进去了</p>
<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">ncclResult_t <span class="title function_">ncclSend</span><span class="params">(<span class="type">const</span> <span class="type">void</span> *sendbuff, <span class="type">size_t</span> count, ncclDataType_t datatype, <span class="type">int</span> peer,</span></span><br><span class="line"><span class="params">                      ncclComm_t comm, cudaStream_t stream)</span></span><br><span class="line">&#123;</span><br><span class="line">    NvtxParamsSendRecv payload&#123;count * <span class="title function_">ncclTypeSize</span><span class="params">(datatype)</span>, peer&#125;;</span><br><span class="line">    NVTX3_FUNC_WITH_PARAMS(Send, SendRecvSchema, payload)</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclInfo</span> <span class="title">info</span> =</span> &#123;ncclFuncSend, <span class="string">&quot;Send&quot;</span>,</span><br><span class="line">                            <span class="literal">NULL</span>, (<span class="type">void</span> *)sendbuff, count, datatype, ncclSum, peer, comm, stream, <span class="comment">/* Args */</span></span><br><span class="line">                            <span class="number">1</span>, <span class="number">1</span>&#125;;</span><br><span class="line">    ncclResult_t ret;</span><br><span class="line">    NCCLCHECK(ncclGroupStart());</span><br><span class="line">    NCCLCHECKGOTO(ncclEnqueueCheck(&amp;info), ret, <span class="built_in">exit</span>);</span><br><span class="line"><span class="built_in">exit</span>:</span><br><span class="line">    NCCLCHECK(ncclGroupEnd());</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></li>
<li><p>nccl的enqueue.cc中</p>
<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">ncclResult_t <span class="title function_">ncclEnqueueCheck</span><span class="params">(<span class="keyword">struct</span> ncclInfo *info)</span></span><br><span class="line">&#123;</span><br><span class="line">    NCCLCHECK(ncclGroupStartInternal());</span><br><span class="line">    ncclResult_t ret = ncclSuccess;</span><br><span class="line">    <span class="type">int</span> devOld = <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    NCCLCHECKGOTO(CommCheck(info-&gt;comm, info-&gt;opName, <span class="string">&quot;comm&quot;</span>), ret, fail);</span><br><span class="line">    <span class="comment">// Check whether communicator is ready to communicate</span></span><br><span class="line">    NCCLCHECKGOTO(ncclCommEnsureReady(info-&gt;comm), ret, fail);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (info-&gt;comm-&gt;checkPointers)</span><br><span class="line">    &#123;</span><br><span class="line">        CUDACHECKGOTO(cudaGetDevice(&amp;devOld), ret, fail);</span><br><span class="line">        CUDACHECKGOTO(cudaSetDevice(info-&gt;comm-&gt;cudaDev), ret, fail);</span><br><span class="line">    &#125;</span><br><span class="line">    NCCLCHECKGOTO(ArgsCheck(info), ret, fail);</span><br><span class="line"></span><br><span class="line">    INFO(NCCL_COLL, <span class="string">&quot;%s: opCount %lx sendbuff %p recvbuff %p count %zu datatype %d op %d root %d comm %p [nranks=%d] stream %p&quot;</span>,</span><br><span class="line">         info-&gt;opName, info-&gt;comm-&gt;opCount, info-&gt;sendbuff, info-&gt;recvbuff, info-&gt;count,</span><br><span class="line">         info-&gt;datatype, info-&gt;op, info-&gt;root, info-&gt;comm, info-&gt;comm-&gt;nRanks, info-&gt;stream);</span><br><span class="line">    TRACE_CALL(<span class="string">&quot;nccl%s(%&quot;</span> PRIx64 <span class="string">&quot;,%&quot;</span> PRIx64 <span class="string">&quot;,%zu,%d,%d,%d,%p,%p)&quot;</span>, info-&gt;opName, reinterpret_cast&lt;<span class="type">int64_t</span>&gt;(info-&gt;sendbuff), reinterpret_cast&lt;<span class="type">int64_t</span>&gt;(info-&gt;recvbuff), info-&gt;count, info-&gt;datatype, info-&gt;op, info-&gt;root, info-&gt;comm, info-&gt;stream);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ！！！！！！！！！！！！！！！！！！！！！！！！！！</span></span><br><span class="line">    <span class="comment">// taskAppend把info转换成一个task，然后加入到comm-&gt;planner</span></span><br><span class="line">    <span class="comment">// 调用 ncclGroupCommJoin 将当前任务加入线程本地的通信组</span></span><br><span class="line">    <span class="comment">// 从内存池中分配一个 P2P 任务结构 (ncclTaskP2p) 并初始化任务</span></span><br><span class="line">    <span class="comment">// 将 P2P 任务添加到对应 peer 的发送队列或接收队列</span></span><br><span class="line">    <span class="comment">// 任务放入队列后，下面的groupEndInternal可见</span></span><br><span class="line">    NCCLCHECKGOTO(taskAppend(info-&gt;comm, info), ret, fail);</span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span>:</span><br><span class="line">    <span class="keyword">if</span> (devOld != <span class="number">-1</span>)</span><br><span class="line">        CUDACHECK(cudaSetDevice(devOld));</span><br><span class="line">    ncclGroupErrCheck(ret);</span><br><span class="line">    NCCLCHECK(ncclGroupEndInternal());</span><br><span class="line">    <span class="comment">/* if depth is 1, ncclGroupEndInternal() will trigger group ops. The state can change</span></span><br><span class="line"><span class="comment">     * so we have to check state here. */</span></span><br><span class="line">    <span class="keyword">if</span> (info-&gt;comm &amp;&amp; !info-&gt;comm-&gt;config.blocking)</span><br><span class="line">    &#123;</span><br><span class="line">        NCCLCHECK(ncclCommGetAsyncError(info-&gt;comm, &amp;ret));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">fail:</span><br><span class="line">    <span class="keyword">if</span> (info-&gt;comm &amp;&amp; !info-&gt;comm-&gt;config.blocking)</span><br><span class="line">        (<span class="type">void</span>)ncclCommSetAsyncError(info-&gt;comm, ret);</span><br><span class="line">    <span class="keyword">goto</span> <span class="built_in">exit</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>nccl的group.cc中</p>
<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">ncclResult_t <span class="title function_">ncclGroupEndInternal</span><span class="params">(ncclSimInfo_t *simInfo)</span></span><br><span class="line">&#123;</span><br><span class="line">    ncclResult_t ret = ncclSuccess;</span><br><span class="line">    ncclSimInfo_t internalSimInfo = NCCL_SIM_INFO_INITIALIZER;</span><br><span class="line">    ncclSimInfo_t *internalSimInfoPtr = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="type">size_t</span> realSize = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    internalSimInfo.magic = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ncclGroupDepth == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        WARN(<span class="string">&quot;ncclGroupEnd: not in a group call.&quot;</span>);</span><br><span class="line">        ret = ncclInvalidUsage;</span><br><span class="line">        <span class="keyword">goto</span> <span class="built_in">exit</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ((--ncclGroupDepth) &gt; <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">goto</span> <span class="built_in">exit</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ((ret = ncclGroupError) != ncclSuccess)</span><br><span class="line">        <span class="keyword">goto</span> fail;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (simInfo)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">memcpy</span>((<span class="type">void</span> *)&amp;realSize, (<span class="type">void</span> *)&amp;simInfo-&gt;size, <span class="keyword">sizeof</span>(<span class="type">size_t</span>));</span><br><span class="line">        realSize = realSize &gt; <span class="keyword">sizeof</span>(ncclSimInfo_t) ? <span class="keyword">sizeof</span>(ncclSimInfo_t) : realSize;</span><br><span class="line">        <span class="built_in">memcpy</span>((<span class="type">void</span> *)&amp;internalSimInfo, (<span class="type">void</span> *)simInfo, realSize);</span><br><span class="line">        <span class="keyword">if</span> (internalSimInfo.magic != <span class="number">0x74685283</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            WARN(<span class="string">&quot;ncclSimInfo_t argument not initialized via NCCL_SIM_INFO_INITIALIZER&quot;</span>);</span><br><span class="line">            ret = ncclInvalidArgument;</span><br><span class="line">            <span class="keyword">goto</span> fail;</span><br><span class="line">        &#125;</span><br><span class="line">        internalSimInfoPtr = &amp;internalSimInfo;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ncclGroupCommHead != nullptr || !ncclIntruQueueEmpty(&amp;ncclAsyncJobs) || ncclGroupCommPreconnectHead != nullptr)</span><br><span class="line">    &#123;</span><br><span class="line">        ncclGroupJobMain.groupCommHeadPtr = &amp;ncclGroupCommHead;</span><br><span class="line">        ncclGroupJobMain.groupCommPreconnectHeadPtr = &amp;ncclGroupCommPreconnectHead;</span><br><span class="line">        ncclGroupJobMain.groupErrorPtr = &amp;ncclGroupError;</span><br><span class="line">        ncclGroupJobMain.asyncJobsPtr = &amp;ncclAsyncJobs;</span><br><span class="line">        ncclGroupJobMain.abortFlagPtr = &amp;ncclGroupJobAbortFlag;</span><br><span class="line">        ncclGroupJobMain.groupBlockingPtr = &amp;ncclGroupBlocking;</span><br><span class="line">        ncclGroupJobMain.initialized = <span class="literal">true</span>;</span><br><span class="line">        ncclGroupJobMainPtr = &amp;ncclGroupJobMain;</span><br><span class="line">        <span class="comment">/* make sure ncclGroupBlocking has been set. */</span></span><br><span class="line">        assert(ncclGroupBlocking == <span class="number">0</span> || ncclGroupBlocking == <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span> (ncclGroupBlocking == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">/* nonblocking group */</span></span><br><span class="line">            <span class="keyword">if</span> (!ncclIntruQueueEmpty(&amp;ncclAsyncJobs))</span><br><span class="line">            &#123;</span><br><span class="line">                ncclAsyncJob *job = ncclIntruQueueHead(&amp;ncclAsyncJobs);</span><br><span class="line">                <span class="keyword">do</span></span><br><span class="line">                &#123;</span><br><span class="line">                    NCCLCHECKGOTO(ncclCommSetAsyncError(job-&gt;comm, ncclInProgress), ret, fail);</span><br><span class="line">                    job-&gt;comm-&gt;groupJob = ncclGroupJobMainPtr;</span><br><span class="line">                    job = job-&gt;next;</span><br><span class="line">                &#125; <span class="keyword">while</span> (job);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (ncclGroupCommHead)</span><br><span class="line">            &#123;</span><br><span class="line">                ncclComm_t comm = ncclGroupCommHead;</span><br><span class="line">                <span class="keyword">do</span></span><br><span class="line">                &#123;</span><br><span class="line">                    NCCLCHECKGOTO(ncclCommSetAsyncError(comm, ncclInProgress), ret, fail);</span><br><span class="line">                    <span class="comment">/* link group job to communicators. */</span></span><br><span class="line">                    comm-&gt;groupJob = ncclGroupJobMainPtr;</span><br><span class="line">                    comm = comm-&gt;groupNext;</span><br><span class="line">                &#125; <span class="keyword">while</span> (comm);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            ncclGroupJobMainPtr-&gt;base.func = groupLaunchNonBlocking;</span><br><span class="line">            PTHREADCHECKGOTO(pthread_create(&amp;ncclGroupJobMainPtr-&gt;base.thread, <span class="literal">NULL</span>, ncclAsyncJobMain, (<span class="type">void</span> *)&amp;ncclGroupJobMainPtr-&gt;base), <span class="string">&quot;pthread_create&quot;</span>, ret, fail);</span><br><span class="line">            ret = ncclInProgress;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">/* blocking group */</span></span><br><span class="line">            NCCLCHECKGOTO(groupLaunch(&amp;ncclGroupJobMainPtr-&gt;base, internalSimInfoPtr), ret, fail);</span><br><span class="line">            <span class="keyword">if</span> (simInfo)</span><br><span class="line">                <span class="built_in">memcpy</span>((<span class="type">void</span> *)simInfo, (<span class="type">void</span> *)internalSimInfoPtr, realSize);</span><br><span class="line">            groupResetJobState(ncclGroupJobMainPtr);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span>:</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">fail:</span><br><span class="line">    groupCleanup(&amp;ncclGroupCommHead, &amp;ncclGroupCommPreconnectHead, &amp;ncclAsyncJobs, &amp;ncclGroupError, &amp;ncclGroupBlocking, &amp;ncclGroupJobAbortFlag, ret);</span><br><span class="line">    <span class="keyword">goto</span> <span class="built_in">exit</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></li>
<li><p>group.cc中</p>
<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">static</span> ncclResult_t <span class="title function_">groupLaunch</span><span class="params">(<span class="keyword">struct</span> ncclAsyncJob *job_, ncclSimInfo_t *simInfo = <span class="literal">NULL</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> savedDev;</span><br><span class="line">    ncclResult_t ret = ncclSuccess;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclGroupJob</span> *<span class="title">gjob</span> =</span> (<span class="keyword">struct</span> ncclGroupJob *)job_;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclComm</span> *<span class="title">groupCommHeadMain</span> =</span> *gjob-&gt;groupCommHeadPtr;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclComm</span> *<span class="title">groupCommPreconnectHeadMain</span> =</span> *gjob-&gt;groupCommPreconnectHeadPtr;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclIntruQueue</span>&lt;</span><span class="class"><span class="keyword">struct</span> <span class="title">ncclAsyncJob</span>, &amp;<span class="title">ncclAsyncJob</span>:</span>:next&gt; *asyncJobsMain = gjob-&gt;asyncJobsPtr;</span><br><span class="line"></span><br><span class="line">    <span class="type">bool</span> *groupAbortFlag = gjob-&gt;abortFlagPtr;</span><br><span class="line"></span><br><span class="line">    CUDACHECKGOTO(cudaGetDevice(&amp;savedDev), ret, fail);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!simInfo &amp;&amp; groupCommPreconnectHeadMain != nullptr)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ncclComm</span> *<span class="title">comm</span> =</span> groupCommPreconnectHeadMain;</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="class"><span class="keyword">struct</span> <span class="title">ncclPreconnectJob</span> *<span class="title">job</span>;</span></span><br><span class="line">            NCCLCHECKGOTO(ncclCalloc(&amp;job, <span class="number">1</span>), ret, fail);</span><br><span class="line">            job-&gt;base.func = ncclP2PPreconnectFunc;</span><br><span class="line">            job-&gt;base.undo = nullptr;</span><br><span class="line">            job-&gt;base.destructor = <span class="built_in">free</span>;</span><br><span class="line">            job-&gt;base.state = ncclGroupJobRunning;</span><br><span class="line">            job-&gt;base.abortFlag = comm-&gt;abortFlag;</span><br><span class="line">            job-&gt;base.abortFlagDev = comm-&gt;abortFlagDev;</span><br><span class="line">            job-&gt;comm = comm;</span><br><span class="line">            ncclIntruQueueEnqueue(asyncJobsMain, (<span class="keyword">struct</span> ncclAsyncJob *)job);</span><br><span class="line"></span><br><span class="line">            <span class="class"><span class="keyword">struct</span> <span class="title">ncclComm</span> *<span class="title">next</span> =</span> comm-&gt;preconnectNext;</span><br><span class="line">            comm-&gt;preconnectNext = reinterpret_cast&lt;<span class="keyword">struct</span> ncclComm *&gt;(<span class="number">0x1</span>);</span><br><span class="line">            comm = next;</span><br><span class="line">        &#125; <span class="keyword">while</span> (comm != nullptr);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    NCCLCHECKGOTO(asyncJobLaunch(asyncJobsMain, groupAbortFlag), ret, fail);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Connect channels at runtime if cumem is supported */</span></span><br><span class="line">    <span class="keyword">if</span> (groupCommHeadMain != nullptr)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ncclComm</span> *<span class="title">comm</span> =</span> groupCommHeadMain;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ncclIntruQueue</span>&lt;</span><span class="class"><span class="keyword">struct</span> <span class="title">ncclAsyncJob</span>, &amp;<span class="title">ncclAsyncJob</span>:</span>:next&gt; asyncCollJobs;</span><br><span class="line">        ncclIntruQueueConstruct(&amp;asyncCollJobs);</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">bool</span> needConnect = <span class="literal">false</span>;</span><br><span class="line">            <span class="type">bool</span> algoNeedConnect[NCCL_NUM_ALGORITHMS];</span><br><span class="line">            <span class="built_in">memset</span>(algoNeedConnect, <span class="number">0</span>, <span class="keyword">sizeof</span>(<span class="type">bool</span>) * NCCL_NUM_ALGORITHMS);</span><br><span class="line"></span><br><span class="line">            CUDACHECKGOTO(cudaSetDevice(comm-&gt;cudaDev), ret, fail);</span><br><span class="line">            NCCLCHECKGOTO(ncclPrepareTasks(comm, algoNeedConnect, &amp;needConnect, simInfo), ret, fail);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (comm-&gt;cuMemSupport &amp;&amp; needConnect)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="class"><span class="keyword">struct</span> <span class="title">ncclPreconnectJob</span> *<span class="title">job</span>;</span></span><br><span class="line">                NCCLCHECKGOTO(ncclCalloc(&amp;job, <span class="number">1</span>), ret, fail);</span><br><span class="line">                job-&gt;base.func = ncclCollPreconnectFunc;</span><br><span class="line">                job-&gt;base.undo = nullptr;</span><br><span class="line">                job-&gt;base.destructor = <span class="built_in">free</span>;</span><br><span class="line">                job-&gt;base.state = ncclGroupJobRunning;</span><br><span class="line">                job-&gt;base.abortFlag = comm-&gt;abortFlag;</span><br><span class="line">                job-&gt;base.abortFlagDev = comm-&gt;abortFlagDev;</span><br><span class="line">                job-&gt;comm = comm;</span><br><span class="line">                NCCLCHECKGOTO(ncclCalloc(&amp;job-&gt;algoNeedConnect, NCCL_NUM_ALGORITHMS), ret, fail);</span><br><span class="line">                <span class="built_in">memcpy</span>(job-&gt;algoNeedConnect, algoNeedConnect, <span class="keyword">sizeof</span>(<span class="type">bool</span>) * NCCL_NUM_ALGORITHMS);</span><br><span class="line">                ncclIntruQueueEnqueue(&amp;asyncCollJobs, &amp;job-&gt;base);</span><br><span class="line">            &#125;</span><br><span class="line">            comm = comm-&gt;groupNext;</span><br><span class="line">        &#125; <span class="keyword">while</span> (comm);</span><br><span class="line"></span><br><span class="line">        NCCLCHECKGOTO(asyncJobLaunch(&amp;asyncCollJobs, groupAbortFlag), ret, fail);</span><br><span class="line">        <span class="keyword">while</span> (!ncclIntruQueueEmpty(&amp;asyncCollJobs))</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="class"><span class="keyword">struct</span> <span class="title">ncclAsyncJob</span> *<span class="title">job</span> =</span> ncclIntruQueueDequeue(&amp;asyncCollJobs);</span><br><span class="line">            <span class="keyword">if</span> (job-&gt;destructor)</span><br><span class="line">                job-&gt;destructor((<span class="type">void</span> *)job);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ((!simInfo) &amp;&amp; (groupCommHeadMain != nullptr))</span><br><span class="line">    &#123;</span><br><span class="line">        NCCLCHECKGOTO(doLaunches(groupCommHeadMain), ret, fail);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (!ncclIntruQueueEmpty(asyncJobsMain))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ncclAsyncJob</span> *<span class="title">job</span> =</span> ncclIntruQueueDequeue(asyncJobsMain);</span><br><span class="line">        <span class="keyword">if</span> (!job-&gt;destroyFlag &amp;&amp; job-&gt;comm &amp;&amp; !job-&gt;comm-&gt;config.blocking)</span><br><span class="line">            (<span class="type">void</span>)ncclCommSetAsyncError(job-&gt;comm, ret);</span><br><span class="line">        <span class="keyword">if</span> (job-&gt;destructor)</span><br><span class="line">            job-&gt;destructor((<span class="type">void</span> *)job);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (groupCommHeadMain != nullptr)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ncclComm</span> *<span class="title">comm</span> =</span> groupCommHeadMain;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ncclComm</span> *<span class="title">next</span> =</span> comm-&gt;groupNext;</span><br><span class="line">        (<span class="type">void</span>)ncclGroupCommLeave(comm);</span><br><span class="line">        <span class="keyword">if</span> (!comm-&gt;config.blocking)</span><br><span class="line">        &#123;</span><br><span class="line">            (<span class="type">void</span>)ncclCommSetAsyncError(comm, ret);</span><br><span class="line">        &#125;</span><br><span class="line">        groupCommHeadMain = next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    CUDACHECK(cudaSetDevice(savedDev));</span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span>:</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">fail:</span><br><span class="line">    groupCleanup(gjob-&gt;groupCommHeadPtr, gjob-&gt;groupCommPreconnectHeadPtr, gjob-&gt;asyncJobsPtr, gjob-&gt;groupErrorPtr, gjob-&gt;groupBlockingPtr, gjob-&gt;abortFlagPtr, ret);</span><br><span class="line">    <span class="keyword">goto</span> <span class="built_in">exit</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></li>
<li><p>group.cc中</p>
<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">static</span> ncclResult_t <span class="title function_">doLaunches</span><span class="params">(<span class="keyword">struct</span> ncclComm *head)</span></span><br><span class="line">&#123;</span><br><span class="line">    ncclResult_t result = ncclSuccess;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclComm</span> *<span class="title">cliqueComm0</span> =</span> head-&gt;intraComm0;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclComm</span> *<span class="title">cliqueHead</span> =</span> head;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclComm</span> *<span class="title">cliqueNextHead</span>;</span></span><br><span class="line">    <span class="type">bool</span> useBarrier = ncclParamLaunchMode == ncclLaunchModeGroup;</span><br><span class="line">    <span class="comment">// This outer loop iterates over cliques of comms which are siblings of the</span></span><br><span class="line">    <span class="comment">// same global entity. We calculate a clique as all comms which have the same</span></span><br><span class="line">    <span class="comment">// `intraComm0` value.</span></span><br><span class="line">    <span class="keyword">do</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ncclComm</span> *<span class="title">comm</span> =</span> cliqueHead;</span><br><span class="line">        <span class="type">bool</span> capturingYes = <span class="literal">false</span>, capturingNo = <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">        &#123;</span><br><span class="line">            (ncclCudaGraphValid(comm-&gt;planner.capturingGraph) ? capturingYes : capturingNo) = <span class="literal">true</span>;</span><br><span class="line">            CUDACHECKGOTO(cudaSetDevice(comm-&gt;cudaDev), result, failure);</span><br><span class="line">            <span class="comment">/* 在ncclLaunchPrepare里面:</span></span><br><span class="line"><span class="comment">             *      if (planner-&gt;nTasksColl == 0 &amp;&amp; planner-&gt;nTasksP2p != 0)</span></span><br><span class="line"><span class="comment">                    &#123;</span></span><br><span class="line"><span class="comment">                        NCCLCHECKGOTO(scheduleP2pTasksToPlan(comm, plan, &amp;budget), result, failure);</span></span><br><span class="line"><span class="comment">                    &#125;</span></span><br><span class="line"><span class="comment">            * 这个scheduleP2pTasksToPlan里面会把p2p的任务加入到planner里面设置了plan-&gt;kernelFn</span></span><br><span class="line"><span class="comment">            */</span></span><br><span class="line">            NCCLCHECKGOTO(ncclLaunchPrepare(comm), result, failure);</span><br><span class="line">            <span class="keyword">if</span> (useBarrier)</span><br><span class="line">                ncclCommIntraBarrierIn(comm, <span class="number">1</span>);</span><br><span class="line">            comm = comm-&gt;groupNext;</span><br><span class="line">        &#125; <span class="keyword">while</span> (comm != nullptr &amp;&amp; comm-&gt;intraComm0 == cliqueComm0);</span><br><span class="line">        cliqueNextHead = comm;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (capturingYes &amp;&amp; capturingNo)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// We have entered barriers but are aborting without leaving them. Thus</span></span><br><span class="line">            <span class="comment">// these comms are permanently trashed. We need a good mechanism for</span></span><br><span class="line">            <span class="comment">// tracking and reporting that.</span></span><br><span class="line">            WARN(<span class="string">&quot;Either none or all communicators in a ncclGroup() can be CUDA graph captured.&quot;</span>);</span><br><span class="line">            result = ncclInvalidUsage;</span><br><span class="line">            <span class="keyword">goto</span> failure;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>)</span><br><span class="line">        &#123; <span class="comment">// Iterate rounds of launches for clique.</span></span><br><span class="line">            <span class="type">bool</span> moreRounds = <span class="literal">false</span>;</span><br><span class="line">            comm = cliqueHead;</span><br><span class="line">            <span class="keyword">do</span></span><br><span class="line">            &#123; <span class="comment">// Iterate clique members.</span></span><br><span class="line">                <span class="class"><span class="keyword">struct</span> <span class="title">ncclComm</span> *<span class="title">next</span> =</span> comm-&gt;groupNext;</span><br><span class="line">                <span class="keyword">if</span> (useBarrier)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// Barrier reduction result tells us if this was the final round.</span></span><br><span class="line">                    moreRounds = <span class="number">0</span> != ncclCommIntraBarrierOut(comm);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    moreRounds |= comm-&gt;planner.unlaunchedPlansHead != nullptr;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (moreRounds)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// Pop next unlaunched kernel</span></span><br><span class="line">                    <span class="class"><span class="keyword">struct</span> <span class="title">ncclKernelPlan</span> *<span class="title">plan</span> =</span> comm-&gt;planner.unlaunchedPlansHead;</span><br><span class="line">                    <span class="keyword">if</span> (plan != nullptr)</span><br><span class="line">                    &#123;</span><br><span class="line">                        comm-&gt;planner.unlaunchedPlansHead = plan-&gt;next;</span><br><span class="line">                        CUDACHECKGOTO(cudaSetDevice(comm-&gt;cudaDev), result, failure);</span><br><span class="line">                        NCCLCHECKGOTO(ncclLaunchKernelBefore_NoUncapturedCuda(comm, plan), result, failure);</span><br><span class="line">                        NCCLCHECKGOTO(ncclLaunchKernel(comm, plan), result, failure);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="comment">// Barrier reduction input indicates if we require further rounds.</span></span><br><span class="line">                    <span class="keyword">if</span> (useBarrier)</span><br><span class="line">                        ncclCommIntraBarrierIn(comm, comm-&gt;planner.unlaunchedPlansHead != nullptr ? <span class="number">1</span> : <span class="number">0</span>);</span><br><span class="line">                    <span class="keyword">if</span> (plan != nullptr)</span><br><span class="line">                    &#123;</span><br><span class="line">                        NCCLCHECKGOTO(ncclLaunchKernelAfter_NoCuda(comm, plan), result, failure);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123; <span class="comment">// Final round.</span></span><br><span class="line">                    CUDACHECKGOTO(cudaSetDevice(comm-&gt;cudaDev), result, failure);</span><br><span class="line">                    NCCLCHECKGOTO(ncclLaunchFinish(comm), result, failure);</span><br><span class="line">                &#125;</span><br><span class="line">                comm = next;</span><br><span class="line">            &#125; <span class="keyword">while</span> (comm != cliqueNextHead);</span><br><span class="line">            <span class="keyword">if</span> (!moreRounds)</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cliqueHead = cliqueNextHead;</span><br><span class="line">    &#125; <span class="keyword">while</span> (cliqueHead != nullptr);</span><br><span class="line">failure:</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>enqueue.cc中</p>
<ul>
<li><strong>最后调用cuda执行kernel的时候，fn告知了线程要使用的device.h里面的什么函数</strong><div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">ncclResult_t <span class="title function_">ncclLaunchKernel</span><span class="params">(<span class="keyword">struct</span> ncclComm *comm, <span class="keyword">struct</span> ncclKernelPlan *plan)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclKernelPlanner</span> *<span class="title">planner</span> =</span> &amp;comm-&gt;planner;</span><br><span class="line">    <span class="type">int</span> nChannels = countOneBits(plan-&gt;channelMask);</span><br><span class="line">    <span class="type">void</span> *sym = plan-&gt;kernelFn;</span><br><span class="line">    dim3 grid = &#123;(<span class="type">unsigned</span>)nChannels, <span class="number">1</span>, <span class="number">1</span>&#125;;</span><br><span class="line">    dim3 block = &#123;(<span class="type">unsigned</span>)plan-&gt;threadPerBlock, <span class="number">1</span>, <span class="number">1</span>&#125;;</span><br><span class="line">    <span class="type">int</span> smem = ncclShmemDynamicSize(comm-&gt;cudaArch);</span><br><span class="line">    cudaStream_t launchStream = planner-&gt;streams-&gt;stream;</span><br><span class="line">    <span class="type">void</span> *extra[] = &#123;</span><br><span class="line">        CU_LAUNCH_PARAM_BUFFER_POINTER, plan-&gt;kernelArgs,</span><br><span class="line">        CU_LAUNCH_PARAM_BUFFER_SIZE, &amp;plan-&gt;kernelArgsSize,</span><br><span class="line">        CU_LAUNCH_PARAM_END&#125;;</span><br><span class="line"></span><br><span class="line">    CUfunction fn;</span><br><span class="line">    CUDACHECK(cudaGetFuncBySymbol(&amp;fn, sym));</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">if</span> CUDART_VERSION &gt;= 11080</span></span><br><span class="line">    <span class="type">int</span> driverVersion;</span><br><span class="line">    NCCLCHECK(ncclCudaDriverVersion(&amp;driverVersion));</span><br><span class="line">    <span class="keyword">if</span> (driverVersion &gt;= <span class="number">11080</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> compCap = comm-&gt;compCap;</span><br><span class="line">        <span class="type">unsigned</span> <span class="type">int</span> clusterSize = (compCap == <span class="number">90</span>) ? comm-&gt;config.cgaClusterSize : <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        CUlaunchConfig launchConfig = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">        CUlaunchAttribute launchAttrs[<span class="number">3</span>];</span><br><span class="line">        <span class="type">int</span> attrs = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">/* Cooperative Group Array (CGA)</span></span><br><span class="line"><span class="comment">         * On sm90 and later we have an extra level of hierarchy where we</span></span><br><span class="line"><span class="comment">         * can group together several blocks within the Grid, called</span></span><br><span class="line"><span class="comment">         * Thread Block Clusters.</span></span><br><span class="line"><span class="comment">         * Clusters enable multiple thread blocks running concurrently</span></span><br><span class="line"><span class="comment">         * across multiple SMs to synchronize and collaboratively fetch</span></span><br><span class="line"><span class="comment">         * and exchange data. A cluster of blocks are guaranteed to be</span></span><br><span class="line"><span class="comment">         * concurrently scheduled onto a group of SMs.</span></span><br><span class="line"><span class="comment">         * The maximum value is 8 and it must be divisible into the grid dimensions</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">if</span> (clusterSize)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// Grid dimension must be divisible by clusterSize</span></span><br><span class="line">            <span class="keyword">if</span> (grid.x % clusterSize)</span><br><span class="line">                clusterSize = <span class="number">1</span>;</span><br><span class="line">            launchAttrs[attrs].id = CU_LAUNCH_ATTRIBUTE_CLUSTER_DIMENSION;</span><br><span class="line">            launchAttrs[attrs++].value.clusterDim = &#123;clusterSize, <span class="number">1</span>, <span class="number">1</span>&#125;;</span><br><span class="line">            launchAttrs[attrs].id = CU_LAUNCH_ATTRIBUTE_CLUSTER_SCHEDULING_POLICY_PREFERENCE;</span><br><span class="line">            launchAttrs[attrs++].value.clusterSchedulingPolicyPreference = CU_CLUSTER_SCHEDULING_POLICY_SPREAD;</span><br><span class="line">        &#125;</span><br><span class="line"><span class="meta">#<span class="keyword">if</span> CUDART_VERSION &gt;= 12000</span></span><br><span class="line">        <span class="keyword">if</span> (compCap &gt;= <span class="number">90</span> &amp;&amp; driverVersion &gt;= <span class="number">12000</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// Set the NCCL Mem Sync domain on CUDA 12.0 and later (sm90)</span></span><br><span class="line">            launchAttrs[attrs].id = CU_LAUNCH_ATTRIBUTE_MEM_SYNC_DOMAIN;</span><br><span class="line">            launchAttrs[attrs++].value.memSyncDomain = (CUlaunchMemSyncDomain)ncclParamMemSyncDomain();</span><br><span class="line">        &#125;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">        launchConfig.gridDimX = grid.x;</span><br><span class="line">        launchConfig.gridDimY = grid.y;</span><br><span class="line">        launchConfig.gridDimZ = grid.z;</span><br><span class="line">        launchConfig.blockDimX = block.x;</span><br><span class="line">        launchConfig.blockDimY = block.y;</span><br><span class="line">        launchConfig.blockDimZ = block.z;</span><br><span class="line">        launchConfig.sharedMemBytes = smem;</span><br><span class="line">        launchConfig.attrs = launchAttrs;</span><br><span class="line">        launchConfig.numAttrs = attrs;</span><br><span class="line">        launchConfig.hStream = launchStream;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// CUDACHECK(cudaLaunchKernelExC(&amp;launchConfig, fnAddr, args));</span></span><br><span class="line">        CUCHECK(cuLaunchKernelEx(&amp;launchConfig, fn, nullptr, extra));</span><br><span class="line">        <span class="keyword">return</span> ncclSuccess;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">    <span class="comment">// Standard kernel launch</span></span><br><span class="line">    CUCHECK(cuLaunchKernel(fn, grid.x, grid.y, grid.z, block.x, block.y, block.z, smem, launchStream, nullptr, extra));</span><br><span class="line">    <span class="comment">// CUDACHECK(cudaLaunchKernel(fnAddr, grid, block, args, smem, launchStream));</span></span><br><span class="line">    <span class="keyword">return</span> ncclSuccess;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></li>
</ul>
</li>
<li><p>注意！！<strong>cuLaunchKernel是异步的</strong>，所以CPU侧的逻辑到此，发了任务就返回了，不用等待CUDA执行完。这也就是nccl-test里面的CPU时间和端到端时间的差别(-C参数是否设置)，CPU的任务到此基本上就结束了，然后开始返回返回返回，端到端时间则是要等待线程同步完成的。这个函数传入的参数，就是后续所有任务的所有参数了。</p>
</li>
<li><p>很抽象的一个点，调用sendrecv.h里面函数的地方是一个在make之前不存在的文件，在make的时候generate.py生成了一个nccl&#x2F;build&#x2F;obj&#x2F;device&#x2F;gensrc&#x2F;sendrecv.cu，这里面使用了之前代码里声明的一个宏：</p>
<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 原代码</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DEFINE_ncclDevKernel(suffix, coll, redop, ty, algo, proto, specializedFnId)                    \</span></span><br><span class="line"><span class="meta">    __global__ void ncclDevKernel_##suffix(ncclDevKernelArgs4K NCCL_GRID_CONSTANT const args4K)        \</span></span><br><span class="line"><span class="meta">    &#123;                                                                                                  \</span></span><br><span class="line"><span class="meta">        ncclKernelMain<span class="string">&lt;specializedFnId, RunWorkBatch&lt;coll, ty, redop&lt;ty&gt;</span>, algo, proto&gt;&gt;(&amp;args4K.args); \</span></span><br><span class="line"><span class="meta">    &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DEFINE_ncclDevFunc(suffix, coll, redop, ty, algo, proto) \</span></span><br><span class="line"><span class="meta">    __device__ void ncclDevFunc_##suffix()                       \</span></span><br><span class="line"><span class="meta">    &#123;                                                            \</span></span><br><span class="line"><span class="meta">        RunWorkBatch<span class="string">&lt;coll, ty, redop&lt;ty&gt;</span>, algo, proto&gt;().run();  \</span></span><br><span class="line"><span class="meta">    &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 生成的代码</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;common.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;sendrecv.h&quot;</span></span></span><br><span class="line">DEFINE_ncclDevKernel(SendRecv, ncclFuncSendRecv, FuncCopy, <span class="type">int8_t</span>, NCCL_ALGO_RING, NCCL_PROTO_SIMPLE, <span class="number">589</span>)</span><br><span class="line">DEFINE_ncclDevFunc(SendRecv, ncclFuncSendRecv, FuncCopy, <span class="type">int8_t</span>, NCCL_ALGO_RING, NCCL_PROTO_SIMPLE)</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>

</li>
<li><p>ncclDevFuncRowToId也是用generate.py生成到host_table.cc里的</p>
</li>
</ul>
]]></content>
      <categories>
        <category>NCCL</category>
      </categories>
      <tags>
        <tag>NCCL</tag>
        <tag>代码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>tmux使用</title>
    <url>/2024/12/15/tmux%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><ul>
<li>我想在服务器上运行一个程序，但是这个程序耗时很长，我想断掉我本地机器和服务器的连接然后关机睡觉，但是我希望服务器上继续在跑</li>
<li>解决方案：<strong>tmux</strong></li>
</ul>
<h2 id="tmux安装"><a href="#tmux安装" class="headerlink" title="tmux安装"></a>tmux安装</h2><ul>
<li>因为我用的服务器是组里面的多人服务器，所以我不能用root权限直接安装，又懒得用源码，于是就用conda虚拟环境安装<div class="code-container" data-rel="Shell"><figure class="iseeu highlight shell"><table><tr><td class="code"><pre><span class="line">conda install -c conda-forge tmux</span><br><span class="line">tmux -V</span><br></pre></td></tr></table></figure></div></li>
</ul>
<h2 id="tmux使用"><a href="#tmux使用" class="headerlink" title="tmux使用"></a>tmux使用</h2><h3 id="创建一个新的tmux会话"><a href="#创建一个新的tmux会话" class="headerlink" title="创建一个新的tmux会话"></a>创建一个新的tmux会话</h3><div class="code-container" data-rel="Shell"><figure class="iseeu highlight shell"><table><tr><td class="code"><pre><span class="line">tmux new-session -s mysession</span><br></pre></td></tr></table></figure></div>
<ul>
<li>mysession是给这个会话起的名字，可以随便取</li>
<li>然后就进入了这个tmux会话，正常输入要跑的程序命令，就开始跑了</li>
</ul>
<h3 id="断开tmux会话（保持程序在后台运行）"><a href="#断开tmux会话（保持程序在后台运行）" class="headerlink" title="断开tmux会话（保持程序在后台运行）"></a>断开tmux会话（保持程序在后台运行）</h3><ul>
<li>快捷键：ctrl+B，然后松开，再按D</li>
<li>然后就被带回到了原来的shell界面，但是tmux会话还在后台运行</li>
<li>这个时候就可以断掉本地机器和服务器的连接了，关机美美睡觉</li>
</ul>
<h3 id="重新连接tmux会话"><a href="#重新连接tmux会话" class="headerlink" title="重新连接tmux会话"></a>重新连接tmux会话</h3><div class="code-container" data-rel="Shell"><figure class="iseeu highlight shell"><table><tr><td class="code"><pre><span class="line">tmux attach-session -t mysession</span><br></pre></td></tr></table></figure></div>
<ul>
<li>如果忘记了会话名字，可以用<code>tmux ls</code>查看所有会话</li>
</ul>
<h3 id="退出tmux会话"><a href="#退出tmux会话" class="headerlink" title="退出tmux会话"></a>退出tmux会话</h3><ul>
<li>在tmux会话中输入<code>exit</code>，或者直接关闭终端窗口</li>
</ul>
]]></content>
      <categories>
        <category>实验室实践</category>
      </categories>
      <tags>
        <tag>tmux</tag>
      </tags>
  </entry>
  <entry>
    <title>NCCL代码阅读-05</title>
    <url>/2024/12/19/NCCL%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB-05/</url>
    <content><![CDATA[<h2 id="AllReduce操作流程-从ncclLaunchKernel开始"><a href="#AllReduce操作流程-从ncclLaunchKernel开始" class="headerlink" title="AllReduce操作流程(从ncclLaunchKernel开始)"></a>AllReduce操作流程(从ncclLaunchKernel开始)</h2><ul>
<li>因为我的项目基本上只用allreduce，所以我就重点关注了一下这个操作</li>
<li>具体来说我用的是allreduce的u32的sum操作</li>
<li>ncclLaunchKernel前面的内容和《NCCL代码阅读-01》里面记录的sendrecv操作差不多，就不过多解释了</li>
</ul>
<hr>
<h3 id="准备工作：launchKernel的前夜"><a href="#准备工作：launchKernel的前夜" class="headerlink" title="准备工作：launchKernel的前夜"></a>准备工作：launchKernel的前夜</h3><ul>
<li>这部分本来应该在01里面解释的，但是放在这里，更有助于理解kernel中的workbatch, task等概念</li>
</ul>
<h4 id="taskAppend"><a href="#taskAppend" class="headerlink" title="taskAppend"></a>taskAppend</h4><ul>
<li>回顾一下，<strong>taskAppend</strong>函数会在<strong>ncclEnqueueCheck</strong>中被调用，而ncclEnqueueCheck只需要传入一个参数，叫<strong>info</strong>，这个info来自于ncclAllReduce（nccl做AllReduce操作的最上面入口处，nccl-test可以调用的API）<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ncclInfo</span> <span class="title">info</span> =</span> &#123; ncclFuncAllReduce, <span class="string">&quot;AllReduce&quot;</span>,</span><br><span class="line">  sendbuff, recvbuff, count, datatype, op, <span class="number">0</span>, comm, stream, <span class="comment">/* Args */</span></span><br><span class="line">  ALLREDUCE_CHUNKSTEPS, ALLREDUCE_SLICESTEPS &#125;;</span><br></pre></td></tr></table></figure></div></li>
<li>下面我将接着《NCCL代码阅读-nccltest篇》来分析，上面info里面的几个参数如果看过那篇都很熟悉</li>
<li>这边单独拿出taskAppend分析一下，这个task会被一直传递到下面的kernel中</li>
<li>分析写在注释里了<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 把info转换成一个ncclTaskColl（task），把task加到comm-&gt;planner里</span></span><br><span class="line"><span class="type">static</span> ncclResult_t <span class="title function_">taskAppend</span><span class="params">(<span class="keyword">struct</span> ncclComm* comm, <span class="keyword">struct</span> ncclInfo* info)</span> &#123;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">ncclKernelPlanner</span> *<span class="title">planner</span> =</span> &amp;comm-&gt;planner;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (info-&gt;coll == ncclFuncSend || info-&gt;coll == ncclFuncRecv) &#123;</span><br><span class="line">    <span class="comment">// 我们只分析我们的例子，我们的coll是ncclFuncAllReduce</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Empty collectives can be discarded.</span></span><br><span class="line">    <span class="keyword">if</span> (info-&gt;count == <span class="number">0</span>) <span class="keyword">return</span> ncclSuccess;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将主机端的 info-&gt;op（操作符）转换成设备端格式，存入 opDev 结构体中，供后续计算使用</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclDevRedOpFull</span> <span class="title">opDev</span>;</span></span><br><span class="line">    NCCLCHECK(hostToDevRedOp(&amp;opDev, info-&gt;op, info-&gt;datatype, comm));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (comm-&gt;nRanks == <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="comment">//我们的例子中comm-&gt;nRanks是2</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Must be in thread local group before tasks can be alloc&#x27;d in `comm-&gt;memScoped`.</span></span><br><span class="line">      <span class="comment">// 这里，我们的情况下，是每个进程的一个comm单独成一个group，ncclGroupCommHead就指向各自进程的那一个comm</span></span><br><span class="line">      <span class="comment">// 这里的group，可以查看《NCCL代码阅读-02》</span></span><br><span class="line">      ncclGroupCommJoin(info-&gt;comm);</span><br><span class="line">      <span class="comment">// 分配一个ncclTaskColl结构体</span></span><br><span class="line">      <span class="class"><span class="keyword">struct</span> <span class="title">ncclTaskColl</span>* <span class="title">t</span> =</span> ncclMemoryPoolAlloc&lt;<span class="class"><span class="keyword">struct</span> <span class="title">ncclTaskColl</span>&gt;</span>(&amp;comm-&gt;memPool_ncclTaskColl, &amp;comm-&gt;memPermanent);</span><br><span class="line">      t-&gt;func = info-&gt;coll;</span><br><span class="line">      t-&gt;sendbuff = info-&gt;sendbuff;</span><br><span class="line">      t-&gt;recvbuff = info-&gt;recvbuff;</span><br><span class="line">      t-&gt;count = info-&gt;count;</span><br><span class="line">      t-&gt;root = info-&gt;root;</span><br><span class="line">      t-&gt;datatype = info-&gt;datatype;</span><br><span class="line">      <span class="type">size_t</span> elementSize = ncclTypeSize(t-&gt;datatype);</span><br><span class="line">      <span class="keyword">if</span> (t-&gt;func == ncclFuncAllGather || t-&gt;func == ncclFuncBroadcast) &#123;</span><br><span class="line">        t-&gt;count *= elementSize;</span><br><span class="line">        t-&gt;datatype = ncclInt8;</span><br><span class="line">        elementSize = <span class="number">1</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 这边可以点进去看一下，AllReduce操作的ncclFuncTrafficPerByte是2，因为每个字节都要发出一次，收回一次</span></span><br><span class="line">      t-&gt;trafficBytes = t-&gt;count*elementSize*ncclFuncTrafficPerByte(t-&gt;func, comm-&gt;nRanks);</span><br><span class="line">      t-&gt;opHost = info-&gt;op;</span><br><span class="line">      t-&gt;opDev = opDev; <span class="comment">// C++ struct assignment</span></span><br><span class="line">      t-&gt;chunkSteps = info-&gt;chunkSteps;</span><br><span class="line">      t-&gt;sliceSteps = info-&gt;sliceSteps;</span><br><span class="line"></span><br><span class="line">      <span class="comment">//更新当前任务的总数 nTasksColl。</span></span><br><span class="line">      <span class="comment">//将新任务按流量大小插入任务队列中，ncclTaskCollSorterInsert 根据任务的流量字节数排序。</span></span><br><span class="line">      planner-&gt;nTasksColl += <span class="number">1</span>;</span><br><span class="line">      ncclTaskCollSorterInsert(&amp;planner-&gt;collSorter, t, t-&gt;trafficBytes);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (info-&gt;stream != planner-&gt;streamRecent || planner-&gt;streams == nullptr) &#123;</span><br><span class="line">    planner-&gt;streamRecent = info-&gt;stream;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclCudaStreamList</span>* <span class="title">l</span> =</span> planner-&gt;streams;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (l == nullptr) &#123; <span class="comment">// Got to the end, this must be a new stream.</span></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ncclCudaGraph</span> <span class="title">graph</span>;</span></span><br><span class="line">        NCCLCHECK(ncclCudaGetCapturingGraph(&amp;graph, info-&gt;stream));</span><br><span class="line">        <span class="keyword">if</span> (planner-&gt;streams != nullptr &amp;&amp; !ncclCudaGraphSame(planner-&gt;capturingGraph, graph)) &#123;</span><br><span class="line">          WARN(<span class="string">&quot;Streams given to a communicator within a NCCL group must either be all uncaptured or all captured by the same graph.&quot;</span>);</span><br><span class="line">          <span class="keyword">return</span> ncclInvalidUsage;</span><br><span class="line">        &#125;</span><br><span class="line">        planner-&gt;capturingGraph = graph; <span class="comment">// C++ struct assignment</span></span><br><span class="line">        <span class="comment">// Add stream to list</span></span><br><span class="line">        l = ncclMemoryStackAlloc&lt;<span class="keyword">struct</span> ncclCudaStreamList&gt;(&amp;comm-&gt;memScoped);</span><br><span class="line">        l-&gt;stream = info-&gt;stream;</span><br><span class="line">        l-&gt;next = planner-&gt;streams;</span><br><span class="line">        planner-&gt;streams = l;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (l-&gt;stream == info-&gt;stream)</span><br><span class="line">        <span class="keyword">break</span>; <span class="comment">// Already seen stream.</span></span><br><span class="line">      l = l-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> ncclSuccess;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></div></li>
<li>我们在这里暂停一下，总结一下。appendTask之后，我们的一个task（AllReduce，数据元素个数为count个）就被插入comm的planner里面了，所有的命令信息都在这里面。</li>
<li>从这里往后，都是以group为单位进行操作，我们这边只要记得ncclGroupCommHead就是指向group中第一个comm的指针即可，在我们的场景下，就指向每个进程那唯一的一个comm</li>
</ul>
<h4 id="ncclGroupEndInternal"><a href="#ncclGroupEndInternal" class="headerlink" title="ncclGroupEndInternal"></a>ncclGroupEndInternal</h4><div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">ncclResult_t <span class="title function_">ncclGroupEndInternal</span><span class="params">(ncclSimInfo_t* simInfo)</span> &#123;</span><br><span class="line">  ncclResult_t ret = ncclSuccess;</span><br><span class="line">  ncclSimInfo_t internalSimInfo = NCCL_SIM_INFO_INITIALIZER;</span><br><span class="line">  ncclSimInfo_t* internalSimInfoPtr = <span class="literal">NULL</span>;</span><br><span class="line">  <span class="type">size_t</span> realSize = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  internalSimInfo.magic = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (ncclGroupDepth == <span class="number">0</span>) &#123;</span><br><span class="line">    WARN(<span class="string">&quot;ncclGroupEnd: not in a group call.&quot;</span>);</span><br><span class="line">    ret = ncclInvalidUsage;</span><br><span class="line">    <span class="keyword">goto</span> <span class="built_in">exit</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> ((--ncclGroupDepth) &gt; <span class="number">0</span>) <span class="keyword">goto</span> <span class="built_in">exit</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> ((ret = ncclGroupError) != ncclSuccess) <span class="keyword">goto</span> fail;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (simInfo) &#123;</span><br><span class="line">    <span class="comment">// 不用管，用不到</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (ncclGroupCommHead != nullptr || !ncclIntruQueueEmpty(&amp;ncclAsyncJobs) || ncclGroupCommPreconnectHead != nullptr) &#123;</span><br><span class="line">    <span class="comment">// 记得上面的，ncclGroupCommHead就是comm</span></span><br><span class="line">    ncclGroupJobMain.groupCommHeadPtr = &amp;ncclGroupCommHead;</span><br><span class="line">    ncclGroupJobMain.groupCommPreconnectHeadPtr = &amp;ncclGroupCommPreconnectHead;</span><br><span class="line">    ncclGroupJobMain.groupErrorPtr = &amp;ncclGroupError;</span><br><span class="line">    ncclGroupJobMain.asyncJobsPtr = &amp;ncclAsyncJobs;</span><br><span class="line">    ncclGroupJobMain.abortFlagPtr = &amp;ncclGroupJobAbortFlag;</span><br><span class="line">    ncclGroupJobMain.groupBlockingPtr = &amp;ncclGroupBlocking;</span><br><span class="line">    ncclGroupJobMain.initialized = <span class="literal">true</span>;</span><br><span class="line">    ncclGroupJobMainPtr = &amp;ncclGroupJobMain;</span><br><span class="line">    <span class="comment">/* make sure ncclGroupBlocking has been set. */</span></span><br><span class="line">    assert(ncclGroupBlocking == <span class="number">0</span> || ncclGroupBlocking == <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">if</span> (ncclGroupBlocking == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">/* nonblocking group */</span></span><br><span class="line">      <span class="comment">// 但我们是blocking group，暂时不看这部分</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">/* blocking group */</span></span><br><span class="line">      NCCLCHECKGOTO(groupLaunch(&amp;ncclGroupJobMainPtr-&gt;base, internalSimInfoPtr), ret, fail);</span><br><span class="line">      <span class="keyword">if</span> (simInfo) <span class="built_in">memcpy</span>((<span class="type">void</span>*)simInfo, (<span class="type">void</span>*)internalSimInfoPtr, realSize);</span><br><span class="line">      groupResetJobState(ncclGroupJobMainPtr);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span>:</span><br><span class="line">  <span class="keyword">return</span> ret;</span><br><span class="line">fail:</span><br><span class="line">  groupCleanup(&amp;ncclGroupCommHead, &amp;ncclGroupCommPreconnectHead, &amp;ncclAsyncJobs, &amp;ncclGroupError, &amp;ncclGroupBlocking, &amp;ncclGroupJobAbortFlag, ret);</span><br><span class="line">  <span class="keyword">goto</span> <span class="built_in">exit</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<h4 id="groupLaunch"><a href="#groupLaunch" class="headerlink" title="groupLaunch"></a>groupLaunch</h4><h5 id="groupLaunch-1"><a href="#groupLaunch-1" class="headerlink" title="groupLaunch"></a>groupLaunch</h5><div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">传入的参数中，job_指向这一个group里面所有的异步任务的链表</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">static</span> ncclResult_t <span class="title function_">groupLaunch</span><span class="params">(<span class="keyword">struct</span> ncclAsyncJob *job_, ncclSimInfo_t* simInfo = <span class="literal">NULL</span>)</span> &#123;</span><br><span class="line">  <span class="type">int</span> savedDev;</span><br><span class="line">  ncclResult_t ret = ncclSuccess;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">ncclGroupJob</span> *<span class="title">gjob</span> =</span> (<span class="keyword">struct</span> ncclGroupJob*) job_;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">ncclComm</span> *<span class="title">groupCommHeadMain</span> =</span> *gjob-&gt;groupCommHeadPtr;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">ncclComm</span> *<span class="title">groupCommPreconnectHeadMain</span> =</span> *gjob-&gt;groupCommPreconnectHeadPtr;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">ncclIntruQueue</span>&lt;</span><span class="class"><span class="keyword">struct</span> <span class="title">ncclAsyncJob</span>, &amp;<span class="title">ncclAsyncJob</span>:</span>:next&gt; *asyncJobsMain = gjob-&gt;asyncJobsPtr;</span><br><span class="line"></span><br><span class="line">  <span class="type">bool</span> *groupAbortFlag = gjob-&gt;abortFlagPtr;</span><br><span class="line"></span><br><span class="line">  CUDACHECKGOTO(cudaGetDevice(&amp;savedDev), ret, fail);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!simInfo &amp;&amp; groupCommPreconnectHeadMain != nullptr) &#123;</span><br><span class="line">    <span class="comment">//  不走这一段</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 没有预连接任务的话，这个函数也是直接返回的</span></span><br><span class="line">  NCCLCHECKGOTO(asyncJobLaunch(asyncJobsMain, groupAbortFlag), ret, fail);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* Connect channels at runtime if cumem is supported */</span></span><br><span class="line">  <span class="keyword">if</span> (groupCommHeadMain != nullptr) &#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclComm</span>* <span class="title">comm</span> =</span> groupCommHeadMain;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclIntruQueue</span>&lt;</span><span class="class"><span class="keyword">struct</span> <span class="title">ncclAsyncJob</span>, &amp;<span class="title">ncclAsyncJob</span>:</span>:next&gt; asyncCollJobs;</span><br><span class="line">    ncclIntruQueueConstruct(&amp;asyncCollJobs);</span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">      <span class="type">bool</span> needConnect = <span class="literal">false</span>;</span><br><span class="line">      <span class="type">bool</span> algoNeedConnect[NCCL_NUM_ALGORITHMS];</span><br><span class="line">      <span class="built_in">memset</span>(algoNeedConnect, <span class="number">0</span>, <span class="keyword">sizeof</span>(<span class="type">bool</span>) * NCCL_NUM_ALGORITHMS);</span><br><span class="line"></span><br><span class="line">      CUDACHECKGOTO(cudaSetDevice(comm-&gt;cudaDev), ret, fail);</span><br><span class="line">      NCCLCHECKGOTO(ncclPrepareTasks(comm, algoNeedConnect, &amp;needConnect, simInfo), ret, fail);</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (comm-&gt;cuMemSupport &amp;&amp; needConnect) &#123;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ncclPreconnectJob</span>* <span class="title">job</span>;</span></span><br><span class="line">        NCCLCHECKGOTO(ncclCalloc(&amp;job, <span class="number">1</span>), ret, fail);</span><br><span class="line">        job-&gt;base.func = ncclCollPreconnectFunc;</span><br><span class="line">        job-&gt;base.undo = nullptr;</span><br><span class="line">        job-&gt;base.destructor = <span class="built_in">free</span>;</span><br><span class="line">        job-&gt;base.state = ncclGroupJobRunning;</span><br><span class="line">        job-&gt;base.abortFlag = comm-&gt;abortFlag;</span><br><span class="line">        job-&gt;base.abortFlagDev = comm-&gt;abortFlagDev;</span><br><span class="line">        job-&gt;comm = comm;</span><br><span class="line">        NCCLCHECKGOTO(ncclCalloc(&amp;job-&gt;algoNeedConnect, NCCL_NUM_ALGORITHMS), ret, fail);</span><br><span class="line">        <span class="built_in">memcpy</span>(job-&gt;algoNeedConnect, algoNeedConnect, <span class="keyword">sizeof</span>(<span class="type">bool</span>) * NCCL_NUM_ALGORITHMS);</span><br><span class="line">        ncclIntruQueueEnqueue(&amp;asyncCollJobs, &amp;job-&gt;base);</span><br><span class="line">      &#125;</span><br><span class="line">      comm = comm-&gt;groupNext;</span><br><span class="line">    &#125; <span class="keyword">while</span> (comm);</span><br><span class="line"></span><br><span class="line">    NCCLCHECKGOTO(asyncJobLaunch(&amp;asyncCollJobs, groupAbortFlag), ret, fail);</span><br><span class="line">    <span class="keyword">while</span> (!ncclIntruQueueEmpty(&amp;asyncCollJobs)) &#123;</span><br><span class="line">      <span class="class"><span class="keyword">struct</span> <span class="title">ncclAsyncJob</span>* <span class="title">job</span> =</span> ncclIntruQueueDequeue(&amp;asyncCollJobs);</span><br><span class="line">      <span class="keyword">if</span> (job-&gt;destructor) job-&gt;destructor((<span class="type">void</span>*)job);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> ((!simInfo) &amp;&amp; (groupCommHeadMain != nullptr)) &#123;</span><br><span class="line">    NCCLCHECKGOTO(doLaunches(groupCommHeadMain), ret, fail);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (!ncclIntruQueueEmpty(asyncJobsMain)) &#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclAsyncJob</span>* <span class="title">job</span> =</span> ncclIntruQueueDequeue(asyncJobsMain);</span><br><span class="line">    <span class="keyword">if</span> (!job-&gt;destroyFlag &amp;&amp; job-&gt;comm &amp;&amp; !job-&gt;comm-&gt;config.blocking)</span><br><span class="line">      (<span class="type">void</span>) ncclCommSetAsyncError(job-&gt;comm, ret);</span><br><span class="line">    <span class="keyword">if</span> (job-&gt;destructor) job-&gt;destructor((<span class="type">void</span>*)job);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (groupCommHeadMain != nullptr) &#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclComm</span>* <span class="title">comm</span> =</span> groupCommHeadMain;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclComm</span>* <span class="title">next</span> =</span> comm-&gt;groupNext;</span><br><span class="line">    (<span class="type">void</span>) ncclGroupCommLeave(comm);</span><br><span class="line">    <span class="keyword">if</span> (!comm-&gt;config.blocking) &#123;</span><br><span class="line">      (<span class="type">void</span>) ncclCommSetAsyncError(comm, ret);</span><br><span class="line">    &#125;</span><br><span class="line">    groupCommHeadMain = next;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  CUDACHECK(cudaSetDevice(savedDev));</span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span>:</span><br><span class="line">  <span class="keyword">return</span> ret;</span><br><span class="line">fail:</span><br><span class="line">  groupCleanup(gjob-&gt;groupCommHeadPtr, gjob-&gt;groupCommPreconnectHeadPtr, gjob-&gt;asyncJobsPtr, gjob-&gt;groupErrorPtr, gjob-&gt;groupBlockingPtr, gjob-&gt;abortFlagPtr, ret);</span><br><span class="line">  <span class="keyword">goto</span> <span class="built_in">exit</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<ul>
<li>接下来，就进入了doLaunches，带着的参数说白了就是comm</li>
<li>看groupLaunch之前，要先看一下ncclAsyncJob这个结构体</li>
<li>以及，ncclGroupJob这个结构体是继承自ncclAsyncJob的</li>
</ul>
<h5 id="ncclAsyncJob"><a href="#ncclAsyncJob" class="headerlink" title="ncclAsyncJob"></a>ncclAsyncJob</h5><div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ncclAsyncJob</span> &#123;</span></span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">ncclAsyncJob</span>* <span class="title">next</span>;</span></span><br><span class="line">  <span class="type">pthread_t</span> thread;</span><br><span class="line">  ncclResult_t result;</span><br><span class="line">  ncclResult_t(*func)(<span class="keyword">struct</span> ncclAsyncJob*);</span><br><span class="line">  <span class="type">void</span>(*undo)(<span class="keyword">struct</span> ncclAsyncJob*);</span><br><span class="line">  <span class="type">void</span>(*destructor)(<span class="type">void</span>*);</span><br><span class="line">  ncclGroupJobState_t state;</span><br><span class="line">  <span class="type">uint32_t</span>* abortFlag; <span class="comment">/* point to comm abortFlag */</span></span><br><span class="line">  <span class="type">uint32_t</span>* abortFlagDev; <span class="comment">/* point to comm abortFlagDev */</span></span><br><span class="line">  <span class="type">uint32_t</span>* childAbortFlag; <span class="comment">/* point to child abortFlag */</span></span><br><span class="line">  <span class="type">uint32_t</span>* childAbortFlagDev; <span class="comment">/* point to child abortFlagDev */</span></span><br><span class="line">  <span class="comment">// 应该还记得，我们的实际任务被插入comm的planner里面了</span></span><br><span class="line">  ncclComm_t comm;</span><br><span class="line">  <span class="type">int</span> destroyFlag;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></div>
<h5 id="ncclPrepareTasks"><a href="#ncclPrepareTasks" class="headerlink" title="ncclPrepareTasks"></a>ncclPrepareTasks</h5><ul>
<li>还要再看一下ncclPrepareTasks函数，在groupLaunch里面被调用的，这里面:<ul>
<li>确定了nTasksPerChannel</li>
<li>一系列操作（这边我没具体分析，后面如果有需要我会回来补充），将tasks加入了planner-&gt;collTaskQueue中</li>
<li>下面出现了两个结构：ncclDevWorkColl devWork和ncclWorkList* workNode</li>
<li>devWork 是一个包含集体通信任务的详细信息的结构体（ncclDevWorkColl）。</li>
<li>workNode 是一个 ncclWorkList 结构体，它包含一个类型字段（workType）和一个大小字段（size），以及存储任务数据的区域</li>
<li>workNode 包含了 devWork 作为它的数据部分</li>
<li>把task转换成了ncclDevWorkColl，然后加入workNode，最后加入了队列：  <div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">ncclIntruQueueEnqueue(&amp;planner-&gt;collWorkQueue, workNode)</span><br></pre></td></tr></table></figure></div></li>
<li>在我的场景中，两个comm只有第一次要初始化algo channel，所以needConnect各自只有一次是true</li>
</ul>
</li>
</ul>
<h4 id="doLaunches"><a href="#doLaunches" class="headerlink" title="doLaunches"></a>doLaunches</h4><ul>
<li>主要功能就是两层循环，外层遍历每个comm组，内层处理组里面每个comm</li>
<li>对每个comm做初始化：ncclLaunchPrepare</li>
<li>从 comm-&gt;planner.unlaunchedPlansHead 中获取下一个待执行的内核计划（ncclKernelPlan）。</li>
<li>执行 ncclLaunchKernelBefore_NoUncapturedCuda(comm, plan) 和 ncclLaunchKernel(comm, plan) 来启动内核。</li>
</ul>
<h5 id="ncclLaunchPrepare"><a href="#ncclLaunchPrepare" class="headerlink" title="ncclLaunchPrepare"></a>ncclLaunchPrepare</h5><ul>
<li>又出来一个新的结构：ncclKernelPlan…….（怎么这么多结构啊</li>
<li>分配了一个plan结构</li>
<li>总之我们是集合操作，所以进入了scheduleCollTasksToPlan(comm, plan, &amp;budget)，下面有这个函数的基本内容。这个函数主要干了这些事：<ul>
<li>划分了channel和chunk</li>
<li>把task从comm的planner队列中取出，加到plan-&gt;collTaskQueue里</li>
<li>在workNode后面带着的devWork里面存入划分出的chunk、channel等信息，最后把workNode从comm的planner队列中取出，加到plan-&gt;WorkQueue里</li>
<li>在plan里面存入channelMask等信息</li>
</ul>
</li>
<li>然后把这个plan加入comm的planQueue</li>
<li>设置了流之间的依赖关系</li>
</ul>
<h6 id="scheduleCollTasksToPlan"><a href="#scheduleCollTasksToPlan" class="headerlink" title="scheduleCollTasksToPlan"></a>scheduleCollTasksToPlan</h6><div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">static</span> ncclResult_t <span class="title function_">scheduleCollTasksToPlan</span><span class="params">(</span></span><br><span class="line"><span class="params">    <span class="keyword">struct</span> ncclComm *comm, <span class="keyword">struct</span> ncclKernelPlan *plan, <span class="keyword">struct</span> ncclKernelPlanBudget *budget)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ncclKernelPlanner</span> *<span class="title">planner</span> =</span> &amp;comm-&gt;planner;</span><br><span class="line">    <span class="comment">// Estimate number of tasks that will fit in this plan.</span></span><br><span class="line">    <span class="type">int</span> nPlanColls = <span class="number">0</span>;</span><br><span class="line">    <span class="type">size_t</span> trafficBytes[<span class="number">2</span> * <span class="number">2</span>] = &#123;<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>&#125;;                            <span class="comment">// [collnet][nvls]</span></span><br><span class="line">    <span class="type">int</span> nChannels[<span class="number">2</span> * <span class="number">2</span>] = &#123;<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>&#125;;                                  <span class="comment">// [collnet][nvls]</span></span><br><span class="line">    <span class="type">int</span> <span class="type">const</span> nMaxChannels[<span class="number">2</span> * <span class="number">2</span>] = &#123;comm-&gt;nChannels, comm-&gt;nvlsChannels, <span class="comment">// [collnet][nvls]</span></span><br><span class="line">                                     comm-&gt;nChannels, comm-&gt;nvlsChannels&#125;;</span><br><span class="line">    <span class="type">constexpr</span> <span class="type">size_t</span> MinTrafficPerChannel = <span class="number">16</span> &lt;&lt; <span class="number">10</span>; <span class="comment">// 16K traffic as minimal</span></span><br><span class="line">    <span class="keyword">do</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">size_t</span> workBytes = <span class="number">0</span>;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ncclTaskColl</span> *<span class="title">task</span> =</span> ncclIntruQueueHead(&amp;planner-&gt;collTaskQueue);</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ncclWorkList</span> *<span class="title">workNode</span> =</span> ncclIntruQueueHead(&amp;planner-&gt;collWorkQueue);</span><br><span class="line">        <span class="keyword">while</span> (task != nullptr)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> nBatches = divUp(nPlanColls, <span class="number">4</span>); <span class="comment">// Rough guess: 4 colls per batch.</span></span><br><span class="line">            <span class="keyword">if</span> (!testBudget(budget, nBatches, workBytes + workNode-&gt;size))</span><br><span class="line">                <span class="keyword">goto</span> plan_full;</span><br><span class="line"></span><br><span class="line">            nPlanColls += <span class="number">1</span>;</span><br><span class="line">            workBytes += workNode-&gt;size;</span><br><span class="line">            <span class="type">int</span> kind = <span class="number">2</span> * task-&gt;isCollnet + task-&gt;isNvls;</span><br><span class="line">            trafficBytes[kind] += <span class="built_in">std</span>::max(MinTrafficPerChannel, task-&gt;trafficBytes);<span class="comment">//2M</span></span><br><span class="line">            nChannels[kind] += task-&gt;nMaxChannels;</span><br><span class="line">            nChannels[kind] = <span class="built_in">std</span>::min(nChannels[kind], nMaxChannels[kind]);<span class="comment">//2</span></span><br><span class="line">            task = task-&gt;next;</span><br><span class="line">            workNode = workNode-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">    plan_full:;</span><br><span class="line">    &#125; <span class="keyword">while</span> (<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> kindPrev = <span class="number">-1</span>;</span><br><span class="line">    <span class="type">size_t</span> trafficPerChannel = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> channelId = <span class="number">0</span>;</span><br><span class="line">    <span class="type">size_t</span> currentTraffic = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (nPlanColls != <span class="number">0</span> &amp;&amp; !ncclIntruQueueEmpty(&amp;planner-&gt;collTaskQueue))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ncclTaskColl</span> *<span class="title">task</span> =</span> ncclIntruQueueHead(&amp;planner-&gt;collTaskQueue);</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ncclWorkList</span> *<span class="title">workNode</span> =</span> ncclIntruQueueHead(&amp;planner-&gt;collWorkQueue);</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ncclDevWorkColl</span> *<span class="title">devWork</span> =</span> (<span class="keyword">struct</span> ncclDevWorkColl *)(workNode + <span class="number">1</span>);</span><br><span class="line">        <span class="type">size_t</span> elementSize = ncclTypeSize(task-&gt;datatype);</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> kind = <span class="number">2</span> * task-&gt;isCollnet + task-&gt;isNvls;</span><br><span class="line">        <span class="keyword">if</span> (kind != kindPrev)</span><br><span class="line">        &#123;</span><br><span class="line">            trafficPerChannel = <span class="built_in">std</span>::max&lt;<span class="type">size_t</span>&gt;(MinTrafficPerChannel, trafficBytes[kind] / nChannels[kind]);<span class="comment">//1M</span></span><br><span class="line">            kindPrev = kind;</span><br><span class="line">            channelId = <span class="number">0</span>;</span><br><span class="line">            currentTraffic = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (task-&gt;isCollnet)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">//我们的实验里不是</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123; <span class="comment">// not task-&gt;isCollnet</span></span><br><span class="line">            <span class="type">int</span> trafficPerByte = ncclFuncTrafficPerByte(task-&gt;func, comm-&gt;nRanks);<span class="comment">//2</span></span><br><span class="line">            <span class="type">size_t</span> cellSize = divUp(divUp(MinTrafficPerChannel, (<span class="type">size_t</span>)trafficPerByte), <span class="number">16</span>) * <span class="number">16</span>;<span class="comment">//8192</span></span><br><span class="line">            <span class="type">int</span> elementsPerCell = cellSize / elementSize;<span class="comment">//2048</span></span><br><span class="line">            <span class="type">size_t</span> cells = divUp(task-&gt;count * elementSize, cellSize);<span class="comment">//128</span></span><br><span class="line">            <span class="type">size_t</span> trafficPerElement = elementSize * trafficPerByte;<span class="comment">//8</span></span><br><span class="line">            <span class="type">size_t</span> trafficPerCell = cellSize * trafficPerByte;<span class="comment">//16384</span></span><br><span class="line">            <span class="type">size_t</span> cellsPerChannel = <span class="built_in">std</span>::min(cells, divUp(trafficPerChannel, trafficPerCell));<span class="comment">//64</span></span><br><span class="line">            <span class="type">size_t</span> cellsLo;<span class="comment">//64</span></span><br><span class="line">            <span class="keyword">if</span> (channelId + <span class="number">1</span> == nMaxChannels[kind])</span><br><span class="line">            &#123; <span class="comment">// On last channel everything goes to &quot;lo&quot;</span></span><br><span class="line">                cellsLo = cells;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                cellsLo = <span class="built_in">std</span>::min(cells, divUp((trafficPerChannel - currentTraffic), trafficPerCell));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="type">int</span> nMidChannels = (cells - cellsLo) / cellsPerChannel;<span class="comment">//1</span></span><br><span class="line">            <span class="type">size_t</span> cellsHi = (cells - cellsLo) % cellsPerChannel;<span class="comment">//0</span></span><br><span class="line">            <span class="type">int</span> nChannels = (cellsLo != <span class="number">0</span> ? <span class="number">1</span> : <span class="number">0</span>) + nMidChannels + (cellsHi != <span class="number">0</span> ? <span class="number">1</span> : <span class="number">0</span>);<span class="comment">//2</span></span><br><span class="line">            <span class="keyword">if</span> (nMaxChannels[kind] &lt; channelId + nChannels)</span><br><span class="line">            &#123; <span class="comment">// Overflowed available channels</span></span><br><span class="line">                nMidChannels = nMaxChannels[kind] - channelId - <span class="number">2</span>;</span><br><span class="line">                cellsPerChannel = (cells - cellsLo) / (nMidChannels + <span class="number">1</span>);</span><br><span class="line">                cellsHi = cellsPerChannel + (cells - cellsLo) % (nMidChannels + <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (cellsHi == <span class="number">0</span> &amp;&amp; nMidChannels != <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                cellsHi = cellsPerChannel;</span><br><span class="line">                nMidChannels -= <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (cellsLo == <span class="number">0</span>)</span><br><span class="line">            &#123; <span class="comment">// Least channel skipped. Make the next channel the new least.</span></span><br><span class="line">                channelId += <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">if</span> (nMidChannels == <span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    cellsLo = cellsHi;</span><br><span class="line">                    cellsHi = <span class="number">0</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    cellsLo = cellsPerChannel;</span><br><span class="line">                    nMidChannels -= <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="type">size_t</span> countMid = nMidChannels != <span class="number">0</span> ? cellsPerChannel * elementsPerCell : <span class="number">0</span>;</span><br><span class="line">            <span class="type">size_t</span> countLo = cellsLo * elementsPerCell;</span><br><span class="line">            <span class="type">size_t</span> countHi = cellsHi * elementsPerCell;</span><br><span class="line">            <span class="comment">//countLo = 131072, countMid = 0, countHi = 131072</span></span><br><span class="line">            (countHi != <span class="number">0</span> ? countHi : countLo) -= cells * elementsPerCell - task-&gt;count;</span><br><span class="line"></span><br><span class="line">            nChannels = (countLo != <span class="number">0</span> ? <span class="number">1</span> : <span class="number">0</span>) + nMidChannels + (cellsHi != <span class="number">0</span> ? <span class="number">1</span> : <span class="number">0</span>);<span class="comment">//2</span></span><br><span class="line">            <span class="comment">// Ensure room for worst case of one new batch per channel</span></span><br><span class="line">            <span class="keyword">if</span> (!testBudget(budget, plan-&gt;nWorkBatches + nChannels, plan-&gt;workBytes + workNode-&gt;size))</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">return</span> ncclSuccess;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            devWork-&gt;channelLo = channelId;<span class="comment">//0</span></span><br><span class="line">            devWork-&gt;channelHi = channelId + nChannels - <span class="number">1</span>;<span class="comment">//1</span></span><br><span class="line">            devWork-&gt;cbd.countLo = countLo;</span><br><span class="line">            devWork-&gt;cbd.countMid = countMid;</span><br><span class="line">            devWork-&gt;cbd.countHi = countHi;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// calcCollChunking() uses global bytes instead of traffic which differs</span></span><br><span class="line">            <span class="comment">// in that allreduce isn&#x27;t multiplied by 2.</span></span><br><span class="line">            <span class="type">size_t</span> globalBytesPerElement = elementSize * ncclFuncMaxSendRecvCount(task-&gt;func, comm-&gt;nRanks, <span class="number">1</span>);<span class="comment">//4</span></span><br><span class="line">            <span class="class"><span class="keyword">struct</span> <span class="title">ncclProxyOp</span> <span class="title">proxyOpLo</span>, <span class="title">proxyOpMid</span>, <span class="title">proxyOpHi</span>;</span></span><br><span class="line"></span><br><span class="line">            <span class="type">uint32_t</span> chunkSize, directFlags = <span class="number">0</span>;</span><br><span class="line">            <span class="type">size_t</span> grainSize = ncclProtoGrainSize(task-&gt;protocol);<span class="comment">//1920</span></span><br><span class="line">            <span class="keyword">if</span> (countLo != <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                NCCLCHECK(calcCollChunking(comm, task, <span class="comment">/*nChannels=*/</span><span class="number">1</span>, globalBytesPerElement * countLo, &amp;chunkSize, &amp;directFlags, &amp;proxyOpLo));</span><br><span class="line">                devWork-&gt;cbd.chunkGrainsLo = chunkSize / grainSize;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (countHi != <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                NCCLCHECK(calcCollChunking(comm, task, <span class="comment">/*nChannels=*/</span><span class="number">1</span>, globalBytesPerElement * countHi, &amp;chunkSize, &amp;directFlags, &amp;proxyOpHi));</span><br><span class="line">                devWork-&gt;cbd.chunkGrainsHi = chunkSize / grainSize;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (nMidChannels != <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                NCCLCHECK(calcCollChunking(comm, task, <span class="comment">/*nChannels=*/</span><span class="number">1</span>, globalBytesPerElement * countMid, &amp;chunkSize, &amp;directFlags, &amp;proxyOpMid));</span><br><span class="line">                devWork-&gt;cbd.chunkGrainsMid = chunkSize / grainSize;</span><br><span class="line">            &#125;</span><br><span class="line">            devWork-&gt;direct = directFlags;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Update the current channel and vacant traffic budget.</span></span><br><span class="line">            <span class="keyword">if</span> (countHi != <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                channelId += nChannels - <span class="number">1</span>;</span><br><span class="line">                currentTraffic = cellsHi * elementsPerCell * trafficPerElement;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (nMidChannels != <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                channelId += nChannels;</span><br><span class="line">                currentTraffic = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                currentTraffic += cellsLo * elementsPerCell * trafficPerElement;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (currentTraffic &gt;= trafficPerChannel &amp;&amp; channelId + <span class="number">1</span> != nMaxChannels[kind])</span><br><span class="line">            &#123;</span><br><span class="line">                channelId += <span class="number">1</span>;</span><br><span class="line">                currentTraffic = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 下面这个循环里面本来有一大堆关于proxy的，但是我们采用的方法用不到proxy，过</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> c = devWork-&gt;channelLo; c &lt;= (<span class="type">int</span>)devWork-&gt;channelHi; c++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 每个channel都有一个workbatchqueue，每个workbatch的内容很少，只指定了操作类型（allreduce+ring）这种</span></span><br><span class="line">                addWorkBatchToPlan(comm, plan, c, workNode-&gt;workType, task-&gt;devFuncId, plan-&gt;workBytes);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 确定channelMask</span></span><br><span class="line">        plan-&gt;channelMask |= (<span class="number">2ull</span> &lt;&lt; devWork-&gt;channelHi) - (<span class="number">1ull</span> &lt;&lt; devWork-&gt;channelLo);</span><br><span class="line">        <span class="comment">// 确定一个线程块的线程数</span></span><br><span class="line">        plan-&gt;threadPerBlock = <span class="built_in">std</span>::max(plan-&gt;threadPerBlock, task-&gt;nWarps * WARP_SIZE);</span><br><span class="line">        <span class="keyword">if</span> (!plan-&gt;kernelSpecialized)</span><br><span class="line">        &#123;</span><br><span class="line">            plan-&gt;kernelFn = ncclDevKernelForFunc[task-&gt;devFuncId];</span><br><span class="line">            plan-&gt;kernelSpecialized = ncclDevKernelForFuncIsSpecialized[task-&gt;devFuncId];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (comm-&gt;rank == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (task-&gt;isCollnet)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 过</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                TRACE(NCCL_COLL, <span class="string">&quot;Collective %s(%s, %s, %s, %s) count=%ld devFuncId=%d channel&#123;Lo..Hi&#125;=&#123;%d..%d&#125; count&#123;Lo,Mid,Hi&#125;=&#123;%ld,%ld,%ld&#125; chunkBytes&#123;Lo,Mid,Hi&#125;=&#123;%d,%d,%d&#125;&quot;</span>,</span><br><span class="line">                      ncclFuncToString(task-&gt;func), ncclDevRedOpToString(task-&gt;opDev.op),</span><br><span class="line">                      ncclDatatypeToString(task-&gt;datatype), ncclAlgoToString(task-&gt;algorithm),</span><br><span class="line">                      ncclProtoToString(task-&gt;protocol),</span><br><span class="line">                      (<span class="type">long</span>)task-&gt;count, task-&gt;devFuncId, devWork-&gt;channelLo, devWork-&gt;channelHi,</span><br><span class="line">                      (<span class="type">long</span>)devWork-&gt;cbd.countLo, (<span class="type">long</span>)devWork-&gt;cbd.countMid, (<span class="type">long</span>)devWork-&gt;cbd.countHi,</span><br><span class="line">                      <span class="type">int</span>(devWork-&gt;cbd.chunkGrainsLo * ncclProtoGrainSize(task-&gt;protocol)),</span><br><span class="line">                      <span class="type">int</span>(devWork-&gt;cbd.chunkGrainsMid * ncclProtoGrainSize(task-&gt;protocol)),</span><br><span class="line">                      <span class="type">int</span>(devWork-&gt;cbd.chunkGrainsHi * ncclProtoGrainSize(task-&gt;protocol)));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; task-&gt;nCleanupQueueElts; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            ncclIntruQueueEnqueue(&amp;plan-&gt;cleanupQueue, ncclIntruQueueDequeue(&amp;planner-&gt;collCleanupQueue));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 从planner-&gt;collTaskQueue排出一个task元素</span></span><br><span class="line">        ncclIntruQueueDequeue(&amp;planner-&gt;collTaskQueue);</span><br><span class="line">        <span class="comment">// 从planner-&gt;collWorkQueue排出一个workNode</span></span><br><span class="line">        ncclIntruQueueDequeue(&amp;planner-&gt;collWorkQueue);</span><br><span class="line">        nPlanColls -= <span class="number">1</span>;</span><br><span class="line">        planner-&gt;nTasksColl -= <span class="number">1</span>;</span><br><span class="line">        <span class="comment">// 把排出的task和workNode加入plan的collTaskQueue和workQueue</span></span><br><span class="line">        ncclIntruQueueEnqueue(&amp;plan-&gt;collTaskQueue, task);</span><br><span class="line">        ncclIntruQueueEnqueue(&amp;plan-&gt;workQueue, workNode);</span><br><span class="line">        plan-&gt;workBytes += workNode-&gt;size;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ncclSuccess;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>

<h3 id="ncclLaunchKernel"><a href="#ncclLaunchKernel" class="headerlink" title="ncclLaunchKernel"></a>ncclLaunchKernel</h3><div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">ncclResult_t <span class="title function_">ncclLaunchKernel</span><span class="params">(<span class="keyword">struct</span> ncclComm* comm, <span class="keyword">struct</span> ncclKernelPlan* plan)</span> &#123;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">ncclKernelPlanner</span>* <span class="title">planner</span> =</span> &amp;comm-&gt;planner;</span><br><span class="line">  <span class="comment">// 开启channel的数量，在我们的例子里是2个</span></span><br><span class="line">  <span class="type">int</span> nChannels = countOneBits(plan-&gt;channelMask);</span><br><span class="line">  <span class="type">void</span>* sym = plan-&gt;kernelFn;</span><br><span class="line">  <span class="comment">// 可以看到，其实是一个一维的grid，有多少个channel就有几个block</span></span><br><span class="line">  dim3 grid = &#123;(<span class="type">unsigned</span>)nChannels, <span class="number">1</span>, <span class="number">1</span>&#125;;</span><br><span class="line">  dim3 block = &#123;(<span class="type">unsigned</span>)plan-&gt;threadPerBlock, <span class="number">1</span>, <span class="number">1</span>&#125;;</span><br><span class="line">  <span class="type">int</span> smem = ncclShmemDynamicSize(comm-&gt;cudaArch);</span><br><span class="line">  cudaStream_t launchStream = planner-&gt;streams-&gt;stream;</span><br><span class="line">  <span class="type">void</span>* extra[] = &#123;</span><br><span class="line">    CU_LAUNCH_PARAM_BUFFER_POINTER, plan-&gt;kernelArgs,</span><br><span class="line">    CU_LAUNCH_PARAM_BUFFER_SIZE, &amp;plan-&gt;kernelArgsSize,</span><br><span class="line">    CU_LAUNCH_PARAM_END</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">  CUfunction fn;</span><br><span class="line">  CUDACHECK(cudaGetFuncBySymbol(&amp;fn, sym));</span><br><span class="line"></span><br><span class="line">  <span class="meta">#<span class="keyword">if</span> CUDART_VERSION &gt;= 11080</span></span><br><span class="line">  <span class="type">int</span> driverVersion;</span><br><span class="line">  NCCLCHECK(ncclCudaDriverVersion(&amp;driverVersion));</span><br><span class="line">  <span class="keyword">if</span> (driverVersion &gt;= <span class="number">11080</span>) &#123;</span><br><span class="line">    <span class="type">int</span> compCap = comm-&gt;compCap;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> clusterSize = (compCap == <span class="number">90</span>) ? comm-&gt;config.cgaClusterSize : <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    CUlaunchConfig launchConfig = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    CUlaunchAttribute launchAttrs[<span class="number">3</span>];</span><br><span class="line">    <span class="type">int</span> attrs = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">/* Cooperative Group Array (CGA)</span></span><br><span class="line"><span class="comment">     * On sm90 and later we have an extra level of hierarchy where we</span></span><br><span class="line"><span class="comment">     * can group together several blocks within the Grid, called</span></span><br><span class="line"><span class="comment">     * Thread Block Clusters.</span></span><br><span class="line"><span class="comment">     * Clusters enable multiple thread blocks running concurrently</span></span><br><span class="line"><span class="comment">     * across multiple SMs to synchronize and collaboratively fetch</span></span><br><span class="line"><span class="comment">     * and exchange data. A cluster of blocks are guaranteed to be</span></span><br><span class="line"><span class="comment">     * concurrently scheduled onto a group of SMs.</span></span><br><span class="line"><span class="comment">     * The maximum value is 8 and it must be divisible into the grid dimensions</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">if</span> (clusterSize) &#123;</span><br><span class="line">      <span class="comment">// Grid dimension must be divisible by clusterSize</span></span><br><span class="line">      <span class="keyword">if</span> (grid.x % clusterSize) clusterSize = <span class="number">1</span>;</span><br><span class="line">      launchAttrs[attrs].id = CU_LAUNCH_ATTRIBUTE_CLUSTER_DIMENSION;</span><br><span class="line">      launchAttrs[attrs++].value.clusterDim = &#123;clusterSize, <span class="number">1</span>, <span class="number">1</span>&#125;;</span><br><span class="line">      launchAttrs[attrs].id = CU_LAUNCH_ATTRIBUTE_CLUSTER_SCHEDULING_POLICY_PREFERENCE;</span><br><span class="line">      launchAttrs[attrs++].value.clusterSchedulingPolicyPreference = CU_CLUSTER_SCHEDULING_POLICY_SPREAD;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">#<span class="keyword">if</span> CUDART_VERSION &gt;= 12000</span></span><br><span class="line">    <span class="keyword">if</span> (compCap &gt;= <span class="number">90</span> &amp;&amp; driverVersion &gt;= <span class="number">12000</span>) &#123;</span><br><span class="line">      <span class="comment">// Set the NCCL Mem Sync domain on CUDA 12.0 and later (sm90)</span></span><br><span class="line">      launchAttrs[attrs].id = CU_LAUNCH_ATTRIBUTE_MEM_SYNC_DOMAIN;</span><br><span class="line">      launchAttrs[attrs++].value.memSyncDomain = (CUlaunchMemSyncDomain) ncclParamMemSyncDomain();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">    launchConfig.gridDimX = grid.x;</span><br><span class="line">    launchConfig.gridDimY = grid.y;</span><br><span class="line">    launchConfig.gridDimZ = grid.z;</span><br><span class="line">    launchConfig.blockDimX = block.x;</span><br><span class="line">    launchConfig.blockDimY = block.y;</span><br><span class="line">    launchConfig.blockDimZ = block.z;</span><br><span class="line">    launchConfig.sharedMemBytes = smem;</span><br><span class="line">    launchConfig.attrs = launchAttrs;</span><br><span class="line">    launchConfig.numAttrs = attrs;</span><br><span class="line">    launchConfig.hStream = launchStream;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//CUDACHECK(cudaLaunchKernelExC(&amp;launchConfig, fnAddr, args));</span></span><br><span class="line">    CUCHECK(cuLaunchKernelEx(&amp;launchConfig, fn, nullptr, extra));</span><br><span class="line">    <span class="keyword">return</span> ncclSuccess;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">  <span class="comment">// Standard kernel launch</span></span><br><span class="line">  CUCHECK(cuLaunchKernel(fn, grid.x, grid.y, grid.z, block.x, block.y, block.z, smem, launchStream, nullptr, extra));</span><br><span class="line">  <span class="comment">//CUDACHECK(cudaLaunchKernel(fnAddr, grid, block, args, smem, launchStream));</span></span><br><span class="line">  <span class="keyword">return</span> ncclSuccess;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<ul>
<li>调用了cuLaunchKernel(Ex)，交给GPU异步处理，CPU可以返回了</li>
</ul>
<h3 id="生成文件里的内容"><a href="#生成文件里的内容" class="headerlink" title="生成文件里的内容"></a>生成文件里的内容</h3><div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;common.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;all_reduce.h&quot;</span></span></span><br><span class="line">DEFINE_ncclDevKernel(AllReduce_Sum_u32_RING_LL, ncclFuncAllReduce, FuncSum, <span class="type">uint32_t</span>, NCCL_ALGO_RING, NCCL_PROTO_LL, <span class="number">230</span>)</span><br><span class="line">DEFINE_ncclDevKernel(AllReduce_Sum_u32_TREE_LL, ncclFuncAllReduce, FuncSum, <span class="type">uint32_t</span>, NCCL_ALGO_TREE, NCCL_PROTO_LL, <span class="number">233</span>)</span><br><span class="line">DEFINE_ncclDevFunc(AllReduce_Sum_u32_COLLNET_CHAIN_SIMPLE, ncclFuncAllReduce, FuncSum, <span class="type">uint32_t</span>, NCCL_ALGO_COLLNET_CHAIN, NCCL_PROTO_SIMPLE)</span><br><span class="line">DEFINE_ncclDevFunc(AllReduce_Sum_u32_COLLNET_DIRECT_SIMPLE, ncclFuncAllReduce, FuncSum, <span class="type">uint32_t</span>, NCCL_ALGO_COLLNET_DIRECT, NCCL_PROTO_SIMPLE)</span><br><span class="line"><span class="meta">#<span class="keyword">if</span> CUDART_VERSION &gt;= 12010 &amp;&amp; __CUDA_ARCH__ &gt;= 900</span></span><br><span class="line">DEFINE_ncclDevFunc(AllReduce_Sum_u32_NVLS_SIMPLE, ncclFuncAllReduce, FuncSum, <span class="type">uint32_t</span>, NCCL_ALGO_NVLS, NCCL_PROTO_SIMPLE)</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">if</span> CUDART_VERSION &gt;= 12010 &amp;&amp; __CUDA_ARCH__ &gt;= 900</span></span><br><span class="line">DEFINE_ncclDevFunc(AllReduce_Sum_u32_NVLS_TREE_SIMPLE, ncclFuncAllReduce, FuncSum, <span class="type">uint32_t</span>, NCCL_ALGO_NVLS_TREE, NCCL_PROTO_SIMPLE)</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">DEFINE_ncclDevFunc(AllReduce_Sum_u32_RING_LL, ncclFuncAllReduce, FuncSum, <span class="type">uint32_t</span>, NCCL_ALGO_RING, NCCL_PROTO_LL)</span><br><span class="line">DEFINE_ncclDevFunc(AllReduce_Sum_u32_RING_LL128, ncclFuncAllReduce, FuncSum, <span class="type">uint32_t</span>, NCCL_ALGO_RING, NCCL_PROTO_LL128)</span><br><span class="line">DEFINE_ncclDevFunc(AllReduce_Sum_u32_RING_SIMPLE, ncclFuncAllReduce, FuncSum, <span class="type">uint32_t</span>, NCCL_ALGO_RING, NCCL_PROTO_SIMPLE)</span><br><span class="line">DEFINE_ncclDevFunc(AllReduce_Sum_u32_TREE_LL, ncclFuncAllReduce, FuncSum, <span class="type">uint32_t</span>, NCCL_ALGO_TREE, NCCL_PROTO_LL)</span><br><span class="line">DEFINE_ncclDevFunc(AllReduce_Sum_u32_TREE_LL128, ncclFuncAllReduce, FuncSum, <span class="type">uint32_t</span>, NCCL_ALGO_TREE, NCCL_PROTO_LL128)</span><br><span class="line">DEFINE_ncclDevFunc(AllReduce_Sum_u32_TREE_SIMPLE, ncclFuncAllReduce, FuncSum, <span class="type">uint32_t</span>, NCCL_ALGO_TREE, NCCL_PROTO_SIMPLE)</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>
<ul>
<li>按照《NCCL代码阅读-01》说的那样，接下来CUDA就开始查表，接着按照生成文件里的代码，进入了ncclKernelMain</li>
<li>注意是从cuLaunchKernel或者cuLaunchKernelEx进入的，在进入的时候，指定了grid和block的维度，后面就会分配多少线程块给他</li>
<li>以及，进入的时候，具体的操作内容都是放在extra里面传入的<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 下面是参数实例化之后的调用情况</span></span><br><span class="line">ncclKernelMain&lt;<span class="number">240</span>, RunWorkBatch&lt;ncclFuncAllReduce, <span class="type">uint64_t</span>, FuncSum&lt;<span class="type">uint64_t</span>&gt;, </span><br><span class="line">																NCCL_ALGO_RING, NCCL_PROTO_LL&gt;&gt;(&amp;args4K.args)</span><br></pre></td></tr></table></figure></div></li>
</ul>
<h3 id="ncclKernelMain"><a href="#ncclKernelMain" class="headerlink" title="ncclKernelMain"></a>ncclKernelMain</h3><ul>
<li>解释都写在注释里面了<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">template&lt;<span class="type">int</span> SpecializedFnId, typename SpecializedRunWorkBatch&gt;</span><br><span class="line">__device__ __forceinline__ <span class="type">void</span> <span class="title function_">ncclKernelMain</span><span class="params">(<span class="keyword">struct</span> ncclDevKernelArgs <span class="type">const</span>* args)</span> &#123;</span><br><span class="line">	<span class="comment">//////////////////////////////////////////////////////////////////</span></span><br><span class="line">	<span class="comment">// SpecializedFnId=240</span></span><br><span class="line">	<span class="comment">// SpecializedRunWorkBatch=RunWorkBatch&lt;ncclFuncAllReduce, uint64_t, FuncSum&lt;uint64_t&gt;, </span></span><br><span class="line">	<span class="comment">//                                      NCCL_ALGO_RING, NCCL_PROTO_LL&gt;&gt;(&amp;args4K.args)</span></span><br><span class="line">	<span class="comment">//////////////////////////////////////////////////////////////////</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// tid是该线程在线程块中的索引，关于CUDA的编程模型见《CUDA编程模型》章节</span></span><br><span class="line">	<span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="comment">// tn是该线程块一共有多少线程</span></span><br><span class="line">	<span class="type">int</span> tn = blockDim.x;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 把一些只读的kernel参数放到共享内存里，要是不显示放置的话，编译器会把这些参数放到线程自己的栈里面，很占地方</span></span><br><span class="line">	<span class="keyword">if</span> (tid &lt; <span class="keyword">sizeof</span>(ncclDevKernelArgs)/<span class="keyword">sizeof</span>(<span class="type">uint32_t</span>)) &#123;</span><br><span class="line">		((<span class="type">uint32_t</span>*)&amp;ncclShmem.args)[tid] = ((<span class="type">uint32_t</span>*)args)[tid];</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// args-&gt;channelMask是一个掩码，比如有64个通道，那就有64位掩码，第x位为1表示第x个通道启用了</span></span><br><span class="line">    <span class="comment">// (1ull&lt;&lt;tid)表示把1左移tid位，这里先把tid看成一个数字即可，所以(1ull&lt;&lt;tid)就是只有第tid位为1</span></span><br><span class="line">    <span class="comment">// 这个if条件表示，满足tid小于MAXCHANNELS（其实就是32）并且第tid位的通道是启用的，才进入if</span></span><br><span class="line">    <span class="comment">// ((1ull&lt;&lt;tid)-1)表示让小于tid位的位全部置1，其余位都是0</span></span><br><span class="line">    <span class="comment">// 比如原来(1ull&lt;&lt;tid)是0b0100，tid是2，那-1之后就是0b0011</span></span><br><span class="line">    <span class="comment">// args-&gt;channelMask &amp; ((1ull&lt;&lt;tid)-1)就是只保留小于tid的启用通道</span></span><br><span class="line">    <span class="comment">// __popcll(x)会统计x的二进制下有几个1</span></span><br><span class="line">    <span class="comment">// 所以__popcll(args-&gt;channelMask &amp; ((1ull&lt;&lt;tid)-1))统计了小于tid的通道号的通道有几个启用的</span></span><br><span class="line">    <span class="comment">// 最后如果n=线程块号，那就让这个线程块负责tid号channel</span></span><br><span class="line">    <span class="comment">// 其实这种分配方式就是为了做到动态分配线程块所负责的通道，同一个线程块只有一个线程的tid号会被选中作为该线程块的通道号，因为比如2号前面启用的个数一定和1号前面启用的个数不同（在1号启用的条件下），所以不可能同时让n=块号</span></span><br><span class="line">	<span class="keyword">if</span> (tid &lt; MAXCHANNELS &amp;&amp; (args-&gt;channelMask &amp; (<span class="number">1ull</span>&lt;&lt;tid))) &#123;</span><br><span class="line">		<span class="type">int</span> n = __popcll(args-&gt;channelMask &amp; ((<span class="number">1ull</span>&lt;&lt;tid)<span class="number">-1</span>));</span><br><span class="line">		<span class="keyword">if</span> (blockIdx.x == n) ncclShmem.channelId = tid;</span><br><span class="line">	&#125;</span><br><span class="line">	__syncthreads(); <span class="comment">// publish ncclShmem.&#123;args, channelId&#125;</span></span><br><span class="line">	<span class="comment">/* set abort flag to 0 */</span></span><br><span class="line">	<span class="keyword">if</span> (tid == <span class="number">0</span>) ncclShmem.aborted = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 用前两个warp来搬运comm和channel的控制信息，其余的warp用来搬运work batch</span></span><br><span class="line">	<span class="keyword">switch</span> (tid/WARP_SIZE) &#123;</span><br><span class="line">	<span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">		&#123; <span class="type">void</span>* dst = &amp;ncclShmem.comm;</span><br><span class="line">			<span class="type">void</span>* src = ncclShmem.args.comm;</span><br><span class="line">			<span class="type">int</span> bytes = <span class="keyword">sizeof</span>(ncclDevComm);</span><br><span class="line">			<span class="keyword">static_assert</span>(<span class="keyword">sizeof</span>(ncclDevComm) &lt;= <span class="number">16</span>*WARP_SIZE, <span class="string">&quot;ncclDevComm cannot be loaded by a single warp in one insn.&quot;</span>);</span><br><span class="line">			copyToShmem16(tid, dst, src, bytes);</span><br><span class="line">		&#125; <span class="keyword">break</span>;</span><br><span class="line">	<span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">		&#123; <span class="comment">// Get address of channel without incurring indirect load from ncclDevComm::channels</span></span><br><span class="line">			<span class="type">void</span>* dst = &amp;ncclShmem.channel;</span><br><span class="line">			<span class="type">void</span>* src = &amp;((ncclDevCommAndChannels*)ncclShmem.args.comm)-&gt;channels[ncclShmem.channelId];</span><br><span class="line">			<span class="type">int</span> bytes = <span class="keyword">sizeof</span>(ncclDevChannel);</span><br><span class="line">			<span class="keyword">static_assert</span>(<span class="keyword">sizeof</span>(ncclDevChannel) &lt;= <span class="number">16</span>*WARP_SIZE, <span class="string">&quot;ncclDevChannel cannot be loaded by a single warp in one insn.&quot;</span>);</span><br><span class="line">			copyToShmem16(tid-WARP_SIZE, dst, src, bytes);</span><br><span class="line">		&#125; <span class="keyword">break</span>;</span><br><span class="line">	<span class="keyword">default</span>:</span><br><span class="line">		&#123; <span class="type">int</span> subtid = tid - <span class="number">2</span>*WARP_SIZE;</span><br><span class="line">			<span class="type">int</span> subtn = tn - <span class="number">2</span>*WARP_SIZE;</span><br><span class="line">			<span class="comment">// Coverity reports a possible thread divergence due to not all threads participating in the collective.</span></span><br><span class="line">			<span class="comment">// However, the code ensures that the participation is on a per-warp basis.</span></span><br><span class="line">			<span class="comment">// coverity[device_thread_diverged:FALSE]</span></span><br><span class="line">			loadWorkBatchToShmem(subtid, subtn, args, <span class="comment">/*batchIx=*/</span>blockIdx.x);</span><br><span class="line">		&#125; <span class="keyword">break</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	__syncthreads(); <span class="comment">// publish ncclShmem</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (tid == <span class="number">0</span> &amp;&amp; ncclShmem.args.workStorageType == ncclDevWorkStorageTypeFifo) &#123;</span><br><span class="line">		<span class="comment">// ncclShmem.workConsumed written by loadWorkBatchToShmem before __syncthreads()</span></span><br><span class="line">		ncclShmem.comm.workConsumed[ncclShmem.channelId] = ncclShmem.workConsumed;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">		<span class="keyword">if</span> (<span class="number">0</span> &lt;= SpecializedFnId &amp;&amp; ncclShmem.funcId == (<span class="type">unsigned</span>)SpecializedFnId) &#123;</span><br><span class="line">            <span class="comment">// 实际执行，RunWorkBatch&lt;ncclFuncAllReduce, uint64_t, FuncSum&lt;uint64_t&gt;, NCCL_ALGO_RING, NCCL_PROTO_LL&gt;&gt;(&amp;args4K.args).run()</span></span><br><span class="line">			SpecializedRunWorkBatch().run();</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			ncclDevFuncTable[ncclShmem.funcId]();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (ncclShmem.nextBatchIx == <span class="number">-1</span>) <span class="keyword">break</span>;</span><br><span class="line">		<span class="type">int</span> batchIx = ncclShmem.nextBatchIx;</span><br><span class="line">		__syncthreads();</span><br><span class="line">		loadWorkBatchToShmem(tid, tn, args, batchIx);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// Check whether the last operation was aborted and make sure all threads exit</span></span><br><span class="line">		<span class="type">bool</span> aborted = <span class="literal">false</span>;</span><br><span class="line">		<span class="keyword">if</span> (tid == <span class="number">0</span>) aborted = *ncclShmem.comm.abortFlag;</span><br><span class="line">		aborted = barrier_red_or_aligned(aborted, <span class="number">0</span>); <span class="comment">// publish ncclShmem.work</span></span><br><span class="line">		<span class="keyword">if</span> (tid == <span class="number">0</span> &amp;&amp; ncclShmem.args.workStorageType == ncclDevWorkStorageTypeFifo) &#123;</span><br><span class="line">			<span class="comment">// ncclShmem.workConsumed written by loadWorkBatchToShmem before barrier_red_or()</span></span><br><span class="line">			ncclShmem.comm.workConsumed[ncclShmem.channelId] = ncclShmem.workConsumed;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> (aborted) <span class="keyword">break</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></div></li>
<li>可以看到，核心执行函数就是SpecializedRunWorkBatch().run()（上下两个一个意思，去01里提到的表里找一下即可）</li>
</ul>
<h3 id="RunWorkBatch"><a href="#RunWorkBatch" class="headerlink" title="RunWorkBatch"></a>RunWorkBatch</h3><div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">template&lt;ncclFunc_t Fn, typename T, typename RedOp, <span class="type">int</span> Algo, <span class="type">int</span> Proto&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">RunWorkBatch</span> &#123;</span></span><br><span class="line"><span class="comment">/////////////////////////////////////////////////////////////////////////////////////////</span></span><br><span class="line"><span class="comment">// Fn = ncclFuncAllReduce</span></span><br><span class="line"><span class="comment">// T = uint64_t</span></span><br><span class="line"><span class="comment">// RedOp = FuncSum&lt;uint64_t&gt;</span></span><br><span class="line"><span class="comment">// Algo = NCCL_ALGO_RING</span></span><br><span class="line"><span class="comment">// Proto = NCCL_PROTO_LL</span></span><br><span class="line"><span class="comment">/////////////////////////////////////////////////////////////////////////////////////////</span></span><br><span class="line">	__device__ __forceinline__ <span class="type">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 线程号</span></span><br><span class="line">		<span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">        <span class="comment">// block中线程总数</span></span><br><span class="line">		<span class="type">int</span> tn = blockDim.x;</span><br><span class="line">		<span class="keyword">if</span> (RedOpArg&lt;RedOp&gt;::ArgUsed) &#123;</span><br><span class="line">			<span class="type">int</span> nWorks = ncclShmem.nWorks;</span><br><span class="line">			<span class="keyword">for</span> (<span class="type">int</span> w=tid; w &lt; nWorks; w += tn) &#123;</span><br><span class="line">				<span class="class"><span class="keyword">struct</span> <span class="title">ncclDevWorkColl</span>* <span class="title">work</span> =</span> (ncclDevWorkColl*)(ncclShmem.workStorage + w*ncclShmem.workSize);</span><br><span class="line">				<span class="keyword">if</span> (work-&gt;redOpArgIsPtr) &#123;</span><br><span class="line">					work-&gt;redOpArg = RedOpArg&lt;RedOp&gt;::loadArg(reinterpret_cast&lt;<span class="type">void</span>*&gt;(work-&gt;redOpArg));</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">			__syncthreads();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="meta">#<span class="keyword">pragma</span> unroll 1</span></span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> w=<span class="number">0</span>; w &lt; ncclShmem.nWorks; w++) &#123;</span><br><span class="line">			<span class="class"><span class="keyword">struct</span> <span class="title">ncclDevWorkColl</span>* <span class="title">work</span> =</span> (<span class="keyword">struct</span> ncclDevWorkColl*)(ncclShmem.workStorage + w*ncclShmem.workSize);</span><br><span class="line">			<span class="keyword">if</span> (w != <span class="number">0</span>) &#123;</span><br><span class="line">				<span class="class"><span class="keyword">struct</span> <span class="title">ncclDevWorkColl</span>* <span class="title">workPrev</span> =</span> (<span class="keyword">struct</span> ncclDevWorkColl*)(ncclShmem.workStorage + (w<span class="number">-1</span>)*ncclShmem.workSize);</span><br><span class="line">				<span class="keyword">if</span> (work-&gt;nWarps != workPrev-&gt;nWarps) __syncthreads();</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="type">int</span> subtn = work-&gt;nWarps*WARP_SIZE;</span><br><span class="line">			<span class="comment">// Coverity reports a possible thread divergence due to not all threads participating in the collective.</span></span><br><span class="line">			<span class="comment">// However, the code ensures that the participation is on a per-warp basis.</span></span><br><span class="line">			<span class="comment">// coverity[device_thread_diverged:FALSE]</span></span><br><span class="line">			<span class="keyword">if</span> (tid &lt; subtn) RunWorkColl&lt;Fn, T, RedOp, Algo, Proto&gt;().run(tid, subtn, work);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></div>]]></content>
      <categories>
        <category>NCCL</category>
      </categories>
      <tags>
        <tag>NCCL</tag>
        <tag>代码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>pragma weak初遇</title>
    <url>/2024/12/22/pragma-weak%E5%88%9D%E9%81%87/</url>
    <content><![CDATA[<h2 id="第一次见到pragma-weak的位置"><a href="#第一次见到pragma-weak的位置" class="headerlink" title="第一次见到pragma weak的位置"></a>第一次见到pragma weak的位置</h2><ul>
<li>我在看nccl-test代码的时候，在src下面有很多集合操作的源代码，项目结构是这样的：<div class="code-container" data-rel="Shell"><figure class="iseeu highlight shell"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── build</span><br><span class="line">│   ├── all_gather_perf</span><br><span class="line">│   ├── all_reduce_perf</span><br><span class="line">│   ├── alltoall_perf</span><br><span class="line">│   ├── broadcast_perf</span><br><span class="line">│   ├── gather_perf</span><br><span class="line">│   ├── hypercube_perf</span><br><span class="line">│   ├── reduce_perf</span><br><span class="line">│   ├── reduce_scatter_perf</span><br><span class="line">│   ├── scatter_perf</span><br><span class="line">│   ├── sendrecv_perf</span><br><span class="line">│   ├── timer.o</span><br><span class="line">│   └── verifiable</span><br><span class="line">│       └── verifiable.o</span><br><span class="line">├── doc</span><br><span class="line">│   └── PERFORMANCE.md</span><br><span class="line">├── gotAcc.py</span><br><span class="line">├── LICENSE.txt</span><br><span class="line">├── Makefile</span><br><span class="line">├── README.md</span><br><span class="line">├── src</span><br><span class="line">│   ├── all_gather.cu</span><br><span class="line">│   ├── all_reduce.cu</span><br><span class="line">│   ├── alltoall.cu</span><br><span class="line">│   ├── broadcast.cu</span><br><span class="line">│   ├── common.cu</span><br><span class="line">│   ├── common.h</span><br><span class="line">│   ├── gather.cu</span><br><span class="line">│   ├── hypercube.cu</span><br><span class="line">│   ├── Makefile</span><br><span class="line">│   ├── nccl1_compat.h</span><br><span class="line">│   ├── reduce.cu</span><br><span class="line">│   ├── reduce_scatter.cu</span><br><span class="line">│   ├── scatter.cu</span><br><span class="line">│   ├── sendrecv.cu</span><br><span class="line">│   ├── timer.cc</span><br><span class="line">│   └── timer.h</span><br><span class="line">└── verifiable</span><br><span class="line">    ├── inexact_regress.cu</span><br><span class="line">    ├── Makefile</span><br><span class="line">    ├── verifiable.cu</span><br><span class="line">    ├── verifiable.h</span><br><span class="line">    └── verifiable.mk</span><br></pre></td></tr></table></figure></div></li>
<li>all_gather.cu, all_reduce.cu等等那一大堆文件里面，最后都会有这样一句话：<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> weak ncclTestEngine=<span class="comment">/*操作名*/</span>Engine</span></span><br><span class="line"><span class="comment">// 下面是all_reduce.cu里的例子</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> weak ncclTestEngine=allReduceEngine</span></span><br></pre></td></tr></table></figure></div></li>
<li>搜了一下，pragma weak是这样的意思，以下摘自GPT</li>
</ul>
<h3 id="GPT的答案"><a href="#GPT的答案" class="headerlink" title="GPT的答案"></a>GPT的答案</h3><p><code>#pragma weak</code> 是一个预处理指令，用于声明一个符号的弱链接（weak linkage）。这意味着，如果在链接时没有找到该符号的定义，链接器将使用该弱符号声明所提供的定义作为默认值。</p>
<h4 id="这行代码的作用："><a href="#这行代码的作用：" class="headerlink" title="这行代码的作用："></a>这行代码的作用：</h4><ol>
<li><p><strong>声明 <code>ncclTestEngine</code> 为弱符号（Weak Symbol）</strong>：</p>
<ul>
<li>该行代码告诉编译器，<code>ncclTestEngine</code> 是一个弱符号，并且它的默认值是 <code>sendRecvEngine</code>。这意味着，链接器会在链接时优先使用 <code>sendRecvEngine</code> 作为 <code>ncclTestEngine</code> 的定义，如果链接器找不到 <code>ncclTestEngine</code> 的其他定义。</li>
</ul>
</li>
<li><p><strong>符号的替代：</strong></p>
<ul>
<li><code>ncclTestEngine</code> 在其他地方可能会被定义为一个不同的符号，通常它是指向某个具体测试引擎实现的函数指针结构体。通过 <code>#pragma weak</code>，你可以在某些情况下使用 <code>sendRecvEngine</code> 来代替 <code>ncclTestEngine</code>，特别是在没有明确提供 <code>ncclTestEngine</code> 定义的情况下。</li>
</ul>
</li>
<li><p><strong>动态选择测试引擎：</strong></p>
<ul>
<li>如果其他地方（可能是链接时的设置）提供了对 <code>ncclTestEngine</code> 的定义，那么编译器会优先使用其他定义。否则，<code>sendRecvEngine</code> 将成为 <code>ncclTestEngine</code> 的默认值，提供一组测试引擎实现。这样就可以根据实际情况灵活选择不同的引擎实现，而不需要修改代码中的其他部分。</li>
</ul>
</li>
</ol>
<h4 id="具体场景："><a href="#具体场景：" class="headerlink" title="具体场景："></a>具体场景：</h4><ol>
<li><p><strong>默认引擎实现：</strong><br><code>sendRecvEngine</code> 作为默认的测试引擎包含了四个主要的函数指针：<code>SendRecvGetBuffSize</code>、<code>SendRecvRunTest</code> 等。当 <code>ncclTestEngine</code> 没有被其他地方定义时，<code>sendRecvEngine</code> 将会作为默认的实现使用。</p>
</li>
<li><p><strong>符号覆盖：</strong><br>如果在程序的其他地方（可能是其他测试模块或者测试配置中）定义了 <code>ncclTestEngine</code>，那么编译器将使用该定义。如果没有定义 <code>ncclTestEngine</code>，那么 <code>sendRecvEngine</code> 将自动成为 <code>ncclTestEngine</code> 的实现。</p>
</li>
</ol>
<h4 id="为什么使用-pragma-weak："><a href="#为什么使用-pragma-weak：" class="headerlink" title="为什么使用 #pragma weak："></a>为什么使用 <code>#pragma weak</code>：</h4><p><code>#pragma weak</code> 通常用于以下场景：</p>
<ul>
<li><strong>库的可扩展性</strong>：允许不同的模块或库扩展并替换默认的符号定义，而不影响现有的代码结构。这种方式允许根据需要替换实现，例如切换到不同的测试引擎或不同的算法实现。</li>
<li><strong>避免链接错误</strong>：如果某些符号没有明确的定义，而你又不想因为没有定义而导致链接失败，使用 <code>#pragma weak</code> 可以提供一个默认实现。</li>
</ul>
<hr>
<h3 id="在nccl-test中的使用"><a href="#在nccl-test中的使用" class="headerlink" title="在nccl-test中的使用"></a>在nccl-test中的使用</h3><ul>
<li>OK，现在我们知道了，pragma weak会在链接的时候给出一个实现，那这么多个弱链接，到底用哪个呢？</li>
<li>以sendrecv.cu为例，我们如果想要跑sendrecv，那么我们会输入类似这样的命令：<div class="code-container" data-rel="Shell"><figure class="iseeu highlight shell"><table><tr><td class="code"><pre><span class="line">./build/sendrecv_perf -f 2 -b 1k -e 1k</span><br></pre></td></tr></table></figure></div></li>
<li>那这个sendrecv_perf就是可执行文件，他的链接用到了哪些文件呢？通过查看makefile，我们可以得知使用了<ul>
<li>sendrecv.o</li>
<li>common.o</li>
<li>timer.o</li>
<li>来自 verifiable.mk 的目标文件（例如 verifiable.o）</li>
</ul>
</li>
<li>那可以看到了，链接的时候就用到了sendrecv.o这一个，而通篇都没有对ncclTestEngine的强定义，那么就会使用sendrecv中给的弱链接定义啦</li>
</ul>
]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>代码阅读</tag>
        <tag>pragma-weak</tag>
      </tags>
  </entry>
  <entry>
    <title>同步机制</title>
    <url>/2024/12/08/%E5%90%8C%E6%AD%A5%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h2 id="barrier"><a href="#barrier" class="headerlink" title="barrier"></a>barrier</h2><p>在计算机科学和并行计算中，<strong>barrier</strong>（屏障）是一种同步机制，用于确保一组线程或进程在某个特定点之前都完成其任务，然后才能继续执行后续操作。它的核心功能是强制所有线程或进程“汇合”到某个同步点，并等待所有参与者都到达该点后，才能继续执行。</p>
<h3 id="详细解释"><a href="#详细解释" class="headerlink" title="详细解释"></a>详细解释</h3><h4 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h4><ol>
<li><strong>屏障点</strong>:<ul>
<li>程序中设置一个“屏障点”，所有线程或进程在到达这个点后都会停下来。</li>
<li>只有当所有线程或进程都到达这个屏障点后，它们才可以继续执行。</li>
</ul>
</li>
<li><strong>同步控制</strong>:<ul>
<li>如果有任何线程或进程未到达屏障点，已经到达的线程或进程会被阻塞（等待）。</li>
<li>当最后一个线程或进程到达屏障点时，屏障被解除，所有线程或进程继续执行。</li>
</ul>
</li>
</ol>
<h4 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h4><ul>
<li>确保多个线程或进程在并行计算中同步某个步骤。</li>
<li>协调计算任务的不同阶段，例如：<ul>
<li>数据准备</li>
<li>中间计算</li>
<li>结果合并</li>
</ul>
</li>
</ul>
<h4 id="示例场景"><a href="#示例场景" class="headerlink" title="示例场景"></a>示例场景</h4><p>在并行程序中，每个线程可能负责不同的数据块进行计算。例如，进行矩阵乘法时，每个线程计算一部分矩阵。为了确保所有线程都完成其部分计算（阶段1），可以在阶段结束时使用屏障。只有所有线程都完成了阶段1，程序才能进入阶段2。</p>
<hr>
<h3 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a>实现方式</h3><h4 id="1-线程级屏障"><a href="#1-线程级屏障" class="headerlink" title="1. 线程级屏障"></a>1. <strong>线程级屏障</strong></h4><p>   在多线程程序中（如使用 <code>pthreads</code> 或 OpenMP），可以使用内置屏障机制：</p>
<ul>
<li><strong>Pthreads</strong>:<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">pthread_barrier_t</span> barrier;</span><br><span class="line">pthread_barrier_init(&amp;barrier, <span class="literal">NULL</span>, num_threads);</span><br><span class="line">pthread_barrier_wait(&amp;barrier);</span><br></pre></td></tr></table></figure></div></li>
<li><strong>OpenMP</strong>:<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> omp barrier</span></span><br></pre></td></tr></table></figure></div></li>
</ul>
<h4 id="2-进程级屏障"><a href="#2-进程级屏障" class="headerlink" title="2. 进程级屏障"></a>2. <strong>进程级屏障</strong></h4><p>   在分布式计算中（如使用 MPI），可以使用通信库提供的屏障函数：</p>
<ul>
<li><strong>MPI</strong>:<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">MPI_Barrier(MPI_COMM_WORLD);</span><br></pre></td></tr></table></figure></div></li>
</ul>
<h4 id="3-CUDA-中的屏障"><a href="#3-CUDA-中的屏障" class="headerlink" title="3. CUDA 中的屏障"></a>3. <strong>CUDA 中的屏障</strong></h4><p>   在 CUDA 程序中，可以通过以下方式实现屏障：</p>
<ul>
<li><strong>线程块内屏障</strong>:<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">__syncthreads();</span><br></pre></td></tr></table></figure></div></li>
<li>注意：<code>__syncthreads</code> 只能用于同一线程块中的线程同步，不能跨线程块。</li>
</ul>
<hr>
<h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ol>
<li><strong>性能问题</strong>: <ul>
<li>屏障可能会引入性能瓶颈，因为所有线程必须等待最慢的线程到达屏障点。</li>
<li>如果线程工作负载不均衡，屏障可能导致资源浪费。</li>
</ul>
</li>
<li><strong>死锁风险</strong>:<ul>
<li>如果部分线程无法到达屏障点（如因错误退出或逻辑问题），整个程序会挂起。</li>
</ul>
</li>
<li><strong>多级屏障</strong>:<ul>
<li>在复杂并行任务中，可以需要设置多级屏障以协调不同的同步点。</li>
</ul>
</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Barrier 是一种用来同步线程或进程的机制，常用于并行和分布式计算中，确保所有参与者都完成某一阶段任务后再进入下一阶段。这在高性能计算中是一个非常重要的概念。</p>
]]></content>
      <categories>
        <category>操作系统知识</category>
      </categories>
      <tags>
        <tag>系统</tag>
      </tags>
  </entry>
  <entry>
    <title>实验室服务器nccl部署命令</title>
    <url>/2024/12/04/%E5%AE%9E%E9%AA%8C%E5%AE%A4%E6%9C%8D%E5%8A%A1%E5%99%A8nccl%E9%83%A8%E7%BD%B2%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h1 id="nccl编译"><a href="#nccl编译" class="headerlink" title="nccl编译"></a>nccl编译</h1><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">make src.build CUDA_HOME=/usr/lib/nvidia-cuda-toolkit/ NVCC_GENCODE=<span class="string">&quot;-gencode=arch=compute_80,code=sm_80&quot;</span></span><br></pre></td></tr></table></figure></div>

<h1 id="查看各种库的安装路径"><a href="#查看各种库的安装路径" class="headerlink" title="查看各种库的安装路径"></a>查看各种库的安装路径</h1><ul>
<li>由于管理员最初好像是用apt安装的，所以可以这样查找</li>
<li>以查找mpi为例<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">dpkg -S mpicc</span><br></pre></td></tr></table></figure></div></li>
</ul>
<h1 id="nccl-test编译"><a href="#nccl-test编译" class="headerlink" title="nccl-test编译"></a>nccl-test编译</h1><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">make MPI=1 MPI_HOME=/usr/mpi/gcc/openmpi-4.1.7a1 CUDA_HOME=/usr/lib/nvidia-cuda-toolkit/ NCCL_HOME=/home/cyu/tccl-2024/nccl/build</span><br></pre></td></tr></table></figure></div>

<h1 id="nccl-test测试"><a href="#nccl-test测试" class="headerlink" title="nccl-test测试"></a>nccl-test测试</h1><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/home/cyu/tccl-2024/nccl/build/lib:<span class="variable">$LD_LIBRARY_PATH</span></span><br><span class="line">./build/all_reduce_perf -b 8 -e 128M -f 2 -g 2</span><br></pre></td></tr></table></figure></div>]]></content>
      <categories>
        <category>实验室实践</category>
      </categories>
      <tags>
        <tag>nccl</tag>
        <tag>实验室</tag>
      </tags>
  </entry>
  <entry>
    <title>NCCL代码阅读-02</title>
    <url>/2024/12/09/NCCL%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB-02/</url>
    <content><![CDATA[<h1 id="NCCL中重要的数据结构（持续更新）"><a href="#NCCL中重要的数据结构（持续更新）" class="headerlink" title="NCCL中重要的数据结构（持续更新）"></a>NCCL中重要的数据结构（持续更新）</h1><h2 id="struct-ncclComm"><a href="#struct-ncclComm" class="headerlink" title="struct ncclComm"></a>struct ncclComm</h2><p>实际使用的时候是</p>
<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">ncclComm</span>* <span class="title">ncclComm_t</span>;</span></span><br></pre></td></tr></table></figure></div>
<ul>
<li>在src\nccl.h.in</li>
<li>通信上下文</li>
<li>比如两张GPU通信，每个GPU上都有一个comm，每个GPU都有一个rank，他们俩共享一个uniqueId，这个uniqueId是由root GPU生成的，然后广播给其他GPU，这样其他GPU就知道了这个通信上下文的uniqueId</li>
</ul>
<h2 id="struct-ncclInfo"><a href="#struct-ncclInfo" class="headerlink" title="struct ncclInfo"></a>struct ncclInfo</h2><ul>
<li>这个结构一直用到最后….功能很多，后面慢慢加</li>
</ul>
<h2 id="struct-ncclKernelPlanner"><a href="#struct-ncclKernelPlanner" class="headerlink" title="struct ncclKernelPlanner"></a>struct ncclKernelPlanner</h2><h2 id="BytePack"><a href="#BytePack" class="headerlink" title="BytePack"></a>BytePack</h2><p>这个 <code>BytePack&lt;16&gt;</code> 是一个 <strong>联合体</strong> (union)，主要用于在内存中以对齐和打包的方式操作数据。下面是对这个联合体的详细解析：</p>
<hr>
<h3 id="1-用途"><a href="#1-用途" class="headerlink" title="1. 用途"></a>1. <strong>用途</strong></h3><p><code>BytePack&lt;16&gt;</code> 是一个能够表示 <strong>16 字节（128 位）数据块</strong> 的数据结构，提供了多种访问方式，适用于高性能代码中高效的内存操作和数据对齐。</p>
<hr>
<h3 id="2-结构解析"><a href="#2-结构解析" class="headerlink" title="2. 结构解析"></a>2. <strong>结构解析</strong></h3><h4 id="alignas-16"><a href="#alignas-16" class="headerlink" title="alignas(16)"></a><strong><code>alignas(16)</code></strong></h4><ul>
<li><strong>含义</strong>：该联合体的起始地址在内存中会被对齐到 <strong>16 字节边界</strong>。</li>
<li><strong>目的</strong>：确保硬件在访问该数据时，可以利用内存对齐的特性实现更高效的读写操作（特别是在 SIMD 或 GPU 中很重要）。</li>
</ul>
<h4 id="成员列表"><a href="#成员列表" class="headerlink" title="成员列表"></a><strong>成员列表</strong></h4><ol>
<li><p><strong><code>BytePack&lt;8&gt; half[2]</code></strong></p>
<ul>
<li><strong>含义</strong>：将 16 字节的数据分为两个 8 字节的 <code>BytePack&lt;8&gt;</code>。</li>
<li><strong>用途</strong>：可以将 16 字节的数据一分为二进行单独处理。</li>
</ul>
</li>
<li><p><strong><code>uint8_t u8[16]</code></strong></p>
<ul>
<li><strong>含义</strong>：按字节访问 16 字节的数据。</li>
<li><strong>用途</strong>：用于逐字节操作，例如处理非对齐的原始数据流。</li>
</ul>
</li>
<li><p><strong><code>uint16_t u16[8]</code></strong></p>
<ul>
<li><strong>含义</strong>：按 2 字节（16 位）为单位访问数据，总共有 8 个 16 位数据。</li>
<li><strong>用途</strong>：适合操作 16 位的数据类型，如半精度浮点数（fp16）。</li>
</ul>
</li>
<li><p><strong><code>uint32_t u32[4]</code></strong></p>
<ul>
<li><strong>含义</strong>：按 4 字节（32 位）为单位访问数据，总共有 4 个 32 位数据。</li>
<li><strong>用途</strong>：适合操作 32 位数据类型，例如单精度浮点数（fp32）或整数。</li>
</ul>
</li>
<li><p><strong><code>uint64_t u64[2]</code></strong></p>
<ul>
<li><strong>含义</strong>：按 8 字节（64 位）为单位访问数据，总共有 2 个 64 位数据。</li>
<li><strong>用途</strong>：适合操作 64 位的数据类型，例如双精度浮点数（fp64）。</li>
</ul>
</li>
<li><p><strong><code>ulong2 ul2, native</code></strong></p>
<ul>
<li><strong>含义</strong>：将 16 字节的数据看作两个 64 位的值组成的矢量（<code>ulong2</code> 是 CUDA&#x2F;NVIDIA 数据类型，表示 2 个 64 位无符号整数的结构体）。</li>
<li><strong>用途</strong>：适合矢量化操作，例如在 GPU 上使用 <code>vectorized</code> 数据访问。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="3-为何使用联合体"><a href="#3-为何使用联合体" class="headerlink" title="3. 为何使用联合体"></a>3. <strong>为何使用联合体</strong></h3><ul>
<li><p><strong>联合体的特点</strong>：<br>联合体中的所有成员共享同一段内存空间（16 字节），但提供了多种不同的访问方式。</p>
</li>
<li><p><strong>目的</strong>：</p>
<ul>
<li>通过共享内存节省空间。</li>
<li>提供灵活的内存操作接口：<ul>
<li>可以以 8 位、16 位、32 位、64 位或矢量的方式操作数据。</li>
</ul>
</li>
<li>对齐至 16 字节边界，优化硬件访问性能。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-典型用途"><a href="#4-典型用途" class="headerlink" title="4. 典型用途"></a>4. <strong>典型用途</strong></h3><h4 id="高性能数据传输"><a href="#高性能数据传输" class="headerlink" title="高性能数据传输"></a><strong>高性能数据传输</strong></h4><p>在 NCCL 或类似 GPU 通信库中，常见的需求是以大块数据为单位（如 16 字节或 128 位）进行高效的数据传输，同时能够支持多种数据类型。</p>
<ul>
<li>例如：<ul>
<li>将 16 字节数据传输到 GPU 的全局内存时：<ul>
<li><code>u64[2]</code> 可以以两个 64 位为单位高效操作。</li>
</ul>
</li>
<li>处理每个字节的内容时：<ul>
<li>使用 <code>u8[16]</code> 逐字节访问。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="SIMD-和-GPU-内存访问"><a href="#SIMD-和-GPU-内存访问" class="headerlink" title="SIMD 和 GPU 内存访问"></a><strong>SIMD 和 GPU 内存访问</strong></h4><ul>
<li>在 SIMD（单指令多数据）或 GPU 中，硬件通常要求对齐访问。例如，某些 GPU 指令只能对齐到 16 字节。</li>
<li>使用 <code>BytePack&lt;16&gt;</code> 确保数据对齐，并提供多种访问方式，适合不同场景下的优化。</li>
</ul>
<hr>
<h3 id="5-示例"><a href="#5-示例" class="headerlink" title="5. 示例"></a>5. <strong>示例</strong></h3><p>假设我们有一个 <code>BytePack&lt;16&gt;</code> 对象 <code>pack</code>：</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="code"><pre><span class="line">BytePack&lt;<span class="number">16</span>&gt; pack;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过不同的方式访问数据</span></span><br><span class="line">pack.u8[<span class="number">0</span>] = <span class="number">0xFF</span>;           <span class="comment">// 按字节访问第 1 个字节</span></span><br><span class="line">pack.u16[<span class="number">1</span>] = <span class="number">0xABCD</span>;        <span class="comment">// 按 2 字节单位访问第 2 个数据</span></span><br><span class="line">pack.u32[<span class="number">0</span>] = <span class="number">0x12345678</span>;    <span class="comment">// 按 4 字节单位访问第 1 个数据</span></span><br><span class="line">pack.u64[<span class="number">1</span>] = <span class="number">0x1122334455667788</span>; <span class="comment">// 按 8 字节单位访问第 2 个数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 按矢量方式访问</span></span><br><span class="line">pack.ul<span class="number">2.</span>x = <span class="number">0xAAAAAAAAAAAAAAAA</span>;</span><br><span class="line">pack.ul<span class="number">2.</span>y = <span class="number">0xBBBBBBBBBBBBBBBB</span>;</span><br></pre></td></tr></table></figure></div>

<hr>
<h3 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. <strong>总结</strong></h3><ul>
<li><p><strong><code>BytePack&lt;16&gt;</code> 的功能：</strong></p>
<ul>
<li>提供灵活的接口操作 <strong>16 字节的数据块</strong>。</li>
<li>确保对齐性能，适配高效内存访问。</li>
</ul>
</li>
<li><p><strong>典型场景：</strong></p>
<ul>
<li>高性能计算中的数据传输、规约。</li>
<li>GPU 上的矢量化操作。</li>
<li>SIMD 或其他对齐要求的硬件加速任务。</li>
</ul>
</li>
</ul>
<h1 id="NCCL代码中常用的函数和宏定义"><a href="#NCCL代码中常用的函数和宏定义" class="headerlink" title="NCCL代码中常用的函数和宏定义"></a>NCCL代码中常用的函数和宏定义</h1><h2 id="NCCLCHECK"><a href="#NCCLCHECK" class="headerlink" title="NCCLCHECK"></a>NCCLCHECK</h2><p><code>NCCLCHECK</code> 是一个宏，用于简化 NCCL 函数调用后的错误检查。在 NCCL 和许多 C&#x2F;C++ 编程环境中，错误处理通常是一个关键部分，而通过宏封装可以使代码更加简洁和易于维护。</p>
<hr>
<h3 id="NCCLCHECK-的典型定义"><a href="#NCCLCHECK-的典型定义" class="headerlink" title="NCCLCHECK 的典型定义"></a><strong>NCCLCHECK 的典型定义</strong></h3><p>在 NCCL 的代码中，<code>NCCLCHECK</code> 通常是定义为类似下面的宏：</p>
<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> NCCLCHECK(call) do &#123; \</span></span><br><span class="line"><span class="meta">  ncclResult_t result = call; \</span></span><br><span class="line"><span class="meta">  <span class="keyword">if</span> (result != ncclSuccess) &#123; \</span></span><br><span class="line"><span class="meta">    printf(<span class="string">&quot;NCCL error at %s:%d: %s\n&quot;</span>, __FILE__, __LINE__, ncclGetErrorString(result)); \</span></span><br><span class="line"><span class="meta">    return result; \</span></span><br><span class="line"><span class="meta">  &#125; \</span></span><br><span class="line"><span class="meta">&#125; while(0)</span></span><br></pre></td></tr></table></figure></div>

<h3 id="功能"><a href="#功能" class="headerlink" title="功能"></a><strong>功能</strong></h3><ul>
<li><strong>执行函数调用并捕获返回值</strong>  <ul>
<li><code>call</code> 是需要执行的 NCCL 函数，比如 <code>ncclInit()</code> 或 <code>PtrCheck(out, &quot;GetUniqueId&quot;, &quot;out&quot;)</code>。这些函数通常返回一个类型为 <code>ncclResult_t</code> 的结果，用于指示是否成功。</li>
</ul>
</li>
</ul>
<h3 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a><strong>使用示例</strong></h3><p>在代码中，<code>NCCLCHECK</code> 的作用是捕获和处理 NCCL 函数的错误。例如：</p>
<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">NCCLCHECK(ncclInit());</span><br></pre></td></tr></table></figure></div>

<p>等价于：</p>
<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  ncclResult_t result = ncclInit();</span><br><span class="line">  <span class="keyword">if</span> (result != ncclSuccess) &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;NCCL error at %s:%d: %s\n&quot;</span>, __FILE__, __LINE__, ncclGetErrorString(result));</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<hr>
<h2 id="cudaSetDevice"><a href="#cudaSetDevice" class="headerlink" title="cudaSetDevice"></a>cudaSetDevice</h2><div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaSetDevice</span><span class="params">(<span class="type">int</span> device)</span>;</span><br></pre></td></tr></table></figure></div>
<ul>
<li>其实这并不是一个NCCL的函数，而是一个CUDA runtime的API</li>
<li>用于设置当前线程的CUDA设备(GPU)</li>
<li>就是说，我现在如果调用了cudaSetDevice(1)，那么接下来的CUDA函数调用都会在GPU 1上执行（我在操作1号设备），直到我再次对另一个设备调用cudaSetDevice</li>
</ul>
<hr>
<h2 id="cudaMalloc"><a href="#cudaMalloc" class="headerlink" title="cudaMalloc"></a>cudaMalloc</h2><div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaMalloc</span><span class="params">(<span class="type">void</span>** devPtr, <span class="type">size_t</span> size)</span>;</span><br></pre></td></tr></table></figure></div>
<ul>
<li>为设备分配内存，这个设备就是之前用cudaSetDevice设置的设备</li>
<li>devPtr是一个指向指针的指针，指向的指针存的是分配的内存的地址</li>
<li>举例：<div class="code-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// allocating and initializing device buffers</span></span><br><span class="line"><span class="type">float</span> **sendbuff = (<span class="type">float</span> **)<span class="built_in">malloc</span>(nDev * <span class="keyword">sizeof</span>(<span class="type">float</span> *));</span><br><span class="line"><span class="type">float</span> **recvbuff = (<span class="type">float</span> **)<span class="built_in">malloc</span>(nDev * <span class="keyword">sizeof</span>(<span class="type">float</span> *));</span><br><span class="line">cudaStream_t *s = (cudaStream_t *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(cudaStream_t) * nDev);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nDev; ++i)</span><br><span class="line">&#123;</span><br><span class="line">    CUDACHECK(cudaSetDevice(i));</span><br><span class="line">    CUDACHECK(cudaMalloc((<span class="type">void</span> **)sendbuff + i, size * <span class="keyword">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    CUDACHECK(cudaMalloc((<span class="type">void</span> **)recvbuff + i, size * <span class="keyword">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    CUDACHECK(cudaMemset(sendbuff[i], <span class="number">1</span>, size * <span class="keyword">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    CUDACHECK(cudaMemset(recvbuff[i], <span class="number">0</span>, size * <span class="keyword">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    CUDACHECK(cudaStreamCreate(s + i));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></li>
</ul>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/3a3ee784d1c88e46f7bd139614358f46.jpg"
                      width="50%"
                >


<h2 id="group-API"><a href="#group-API" class="headerlink" title="group API"></a>group API</h2><h3 id="1-Group-Calls-组调用-的概念"><a href="#1-Group-Calls-组调用-的概念" class="headerlink" title="1. Group Calls (组调用) 的概念"></a>1. <strong>Group Calls (组调用) 的概念</strong></h3><p><code>ncclGroupStart()</code> 和 <code>ncclGroupEnd()</code> 是 NCCL 提供的两个函数，用于将多个 NCCL 操作合并成一个操作进行执行。这些操作会在同一个 <strong>NCCL group</strong> 内顺序执行，从而减少了多次启动 NCCL 操作时的开销。通过使用组调用，NCCL 可以更高效地管理并发操作，尤其是在涉及多个 GPU 或多线程的场景下。</p>
<ul>
<li><p>**<code>ncclGroupStart()</code>**：启动一个 NCCL 操作组。所有在这个调用后到 <code>ncclGroupEnd()</code> 之前的 NCCL 操作都会被视作同一个组的一部分。</p>
</li>
<li><p>**<code>ncclGroupEnd()</code>**：结束 NCCL 操作组，并提交所有在 <code>ncclGroupStart()</code> 和 <code>ncclGroupEnd()</code> 之间的操作。调用这个函数后，NCCL 会将所有操作打包在一起，并尽可能高效地执行。</p>
</li>
</ul>
<p>下面如果不使用组调用（<code>ncclGroupStart()</code> 和 <code>ncclGroupEnd()</code>），会发生什么，执行的具体过程是怎样的。</p>
<h3 id="2-组调用作用之一：管理多个-GPU-的通信操作"><a href="#2-组调用作用之一：管理多个-GPU-的通信操作" class="headerlink" title="2. 组调用作用之一：管理多个 GPU 的通信操作"></a>2. <strong>组调用作用之一：管理多个 GPU 的通信操作</strong></h3><h4 id="示例：多个设备上的-ncclAllReduce"><a href="#示例：多个设备上的-ncclAllReduce" class="headerlink" title="示例：多个设备上的 ncclAllReduce"></a>示例：多个设备上的 <code>ncclAllReduce</code></h4><p>假设我们有多个 GPU（例如 4 个），并希望在每个 GPU 上执行相同的 NCCL 操作（例如 <code>ncclAllReduce</code>）。不使用 <code>ncclGroupStart()</code> 和 <code>ncclGroupEnd()</code> 时，你可能会在每个 GPU 上执行一次 <code>ncclAllReduce</code> 操作，每次都可能会等待前一个操作完成，这样会增加执行时间和延迟。</p>
<h5 id="使用-ncclGroupStart-和-ncclGroupEnd"><a href="#使用-ncclGroupStart-和-ncclGroupEnd" class="headerlink" title="使用 ncclGroupStart() 和 ncclGroupEnd()"></a>使用 <code>ncclGroupStart()</code> 和 <code>ncclGroupEnd()</code></h5><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">ncclGroupStart</span>();  <span class="comment">// 开始一个 NCCL 操作组</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nLocalDevs; i++) &#123;</span><br><span class="line">  <span class="built_in">ncclAllReduce</span>(..., comm[i], stream[i]);  <span class="comment">// 在多个 GPU 上执行操作</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">ncclGroupEnd</span>();  <span class="comment">// 结束并执行所有操作</span></span><br></pre></td></tr></table></figure></div>

<ul>
<li>在这里，<code>ncclGroupStart()</code> 和 <code>ncclGroupEnd()</code> 包围了所有的 <code>ncclAllReduce</code> 调用，所有操作会被视为同一个组的一部分。</li>
<li><strong>执行顺序</strong>：NCCL 会在后台调度这些操作并行执行。每个 GPU 上的操作并不会阻塞其他操作的执行。</li>
<li><strong>性能</strong>：所有 GPU 上的 <code>ncclAllReduce</code> 操作可以并行执行，减少了同步和启动开销。</li>
</ul>
<h5 id="不使用-ncclGroupStart-和-ncclGroupEnd"><a href="#不使用-ncclGroupStart-和-ncclGroupEnd" class="headerlink" title="不使用 ncclGroupStart() 和 ncclGroupEnd()"></a>不使用 <code>ncclGroupStart()</code> 和 <code>ncclGroupEnd()</code></h5><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nLocalDevs; i++) &#123;</span><br><span class="line">  <span class="built_in">ncclAllReduce</span>(..., comm[i], stream[i]);  <span class="comment">// 在多个 GPU 上执行操作</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<ul>
<li><p><strong>执行顺序</strong>：如果不使用组调用，NCCL 会逐个执行这些 <code>ncclAllReduce</code> 操作，<strong>等待每个操作完成后再执行下一个操作</strong>。</p>
<ul>
<li>比如，假设有 4 个 GPU，<code>ncclAllReduce</code> 操作会依次执行，每次执行时都会等待前一个操作完成，然后才会开始下一个操作。这种顺序执行会导致明显的延迟。</li>
</ul>
</li>
<li><p><strong>死锁风险</strong>：如果在每个操作中都需要同步，且这些操作依赖于其他线程&#x2F;进程的结果，可能会导致死锁或不必要的阻塞。例如，<code>ncclAllReduce</code> 在每个设备上执行时，可能需要等待所有设备的操作完成。如果按代码顺序挨个执行（其实就是阻塞），那后一个操作必须等前一个做完才能进行，但实际上，他俩应该同时执行。</p>
</li>
</ul>
<h3 id="3-组调用作用之二：在创建通信器时使用-Group-Calls"><a href="#3-组调用作用之二：在创建通信器时使用-Group-Calls" class="headerlink" title="3. 组调用作用之二：在创建通信器时使用 Group Calls"></a>3. <strong>组调用作用之二：在创建通信器时使用 Group Calls</strong></h3><h4 id="示例：在一个线程中管理多个-GPU"><a href="#示例：在一个线程中管理多个-GPU" class="headerlink" title="示例：在一个线程中管理多个 GPU"></a>示例：在一个线程中管理多个 GPU</h4><p>假设你有一个线程需要初始化多个 GPU 上的 NCCL 通信器。初始化操作（如 <code>ncclCommInitRank</code>）通常是一个阻塞操作，如果不使用 <code>ncclGroupStart()</code> 和 <code>ncclGroupEnd()</code>，这些初始化操作会依次执行，每次操作都必须等待前一个操作完成，这可能会浪费时间。</p>
<h5 id="使用-ncclGroupStart-和-ncclGroupEnd-1"><a href="#使用-ncclGroupStart-和-ncclGroupEnd-1" class="headerlink" title="使用 ncclGroupStart() 和 ncclGroupEnd()"></a>使用 <code>ncclGroupStart()</code> 和 <code>ncclGroupEnd()</code></h5><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">ncclGroupStart</span>();  <span class="comment">// 开始一个 NCCL 操作组</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nLocalDevs; i++) &#123;</span><br><span class="line">  <span class="built_in">cudaSetDevice</span>(device[i]);  <span class="comment">// 设置当前 GPU</span></span><br><span class="line">  <span class="built_in">ncclCommInitRank</span>(comms + i, nranks, commId, rank[i]);  <span class="comment">// 初始化通信器</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">ncclGroupEnd</span>();  <span class="comment">// 结束并执行所有操作</span></span><br></pre></td></tr></table></figure></div>

<ul>
<li><strong>执行顺序</strong>：<code>ncclCommInitRank</code> 调用会并行地在每个 GPU 上执行。</li>
<li><strong>性能</strong>：因为通信器的初始化操作是通过组调用来管理的，NCCL 可以在后台并行处理所有设备的初始化操作，而不是一个接一个地执行。这减少了初始化的时间。</li>
</ul>
<h5 id="不使用-ncclGroupStart-和-ncclGroupEnd-1"><a href="#不使用-ncclGroupStart-和-ncclGroupEnd-1" class="headerlink" title="不使用 ncclGroupStart() 和 ncclGroupEnd()"></a>不使用 <code>ncclGroupStart()</code> 和 <code>ncclGroupEnd()</code></h5><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nLocalDevs; i++) &#123;</span><br><span class="line">  <span class="built_in">cudaSetDevice</span>(device[i]);  <span class="comment">// 设置当前 GPU</span></span><br><span class="line">  <span class="built_in">ncclCommInitRank</span>(comms + i, nranks, commId, rank[i]);  <span class="comment">// 初始化通信器</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<ul>
<li><strong>执行顺序</strong>：<code>ncclCommInitRank</code> 会按顺序执行，等待每个设备的初始化完成后再继续执行下一个设备的初始化。</li>
<li><strong>性能</strong>：没有组调用，通信器初始化会串行执行，导致设备初始化时间更长。如果有多个 GPU，这会浪费很多时间在设备间的同步和等待上。</li>
</ul>
<h3 id="3-组调用作用之三：聚合通信操作"><a href="#3-组调用作用之三：聚合通信操作" class="headerlink" title="3. 组调用作用之三：聚合通信操作"></a>3. <strong>组调用作用之三：聚合通信操作</strong></h3><h4 id="示例：多个集体操作（ncclBroadcast-和-ncclAllReduce）聚合"><a href="#示例：多个集体操作（ncclBroadcast-和-ncclAllReduce）聚合" class="headerlink" title="示例：多个集体操作（ncclBroadcast 和 ncclAllReduce）聚合"></a>示例：多个集体操作（<code>ncclBroadcast</code> 和 <code>ncclAllReduce</code>）聚合</h4><p>假设你想在多个 GPU 上执行多个不同的 NCCL 集体操作（例如，一个 <code>ncclBroadcast</code> 和两个 <code>ncclAllReduce</code>）。如果不使用组调用，NCCL 会为每个操作单独启动一次通信。</p>
<h5 id="使用-ncclGroupStart-和-ncclGroupEnd-2"><a href="#使用-ncclGroupStart-和-ncclGroupEnd-2" class="headerlink" title="使用 ncclGroupStart() 和 ncclGroupEnd()"></a>使用 <code>ncclGroupStart()</code> 和 <code>ncclGroupEnd()</code></h5><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">ncclGroupStart</span>();  <span class="comment">// 开始 NCCL 操作组</span></span><br><span class="line"><span class="built_in">ncclBroadcast</span>(sendbuff1, recvbuff1, count1, datatype, root, comm, stream);</span><br><span class="line"><span class="built_in">ncclAllReduce</span>(sendbuff2, recvbuff2, count2, datatype, comm, stream);</span><br><span class="line"><span class="built_in">ncclAllReduce</span>(sendbuff3, recvbuff3, count3, datatype, comm, stream);</span><br><span class="line"><span class="built_in">ncclGroupEnd</span>();  <span class="comment">// 结束并执行所有操作</span></span><br></pre></td></tr></table></figure></div>

<ul>
<li><strong>执行顺序</strong>：所有的集体操作（<code>ncclBroadcast</code> 和 <code>ncclAllReduce</code>）会被合并到一个组中执行。NCCL 会将这些操作作为一个批次提交，减少了每个操作单独启动时的开销。</li>
<li><strong>性能</strong>：通过将多个操作合并成一个组，NCCL 只需要发起一次通信并等待完成，从而减少了启动和同步的延迟。</li>
</ul>
<h5 id="不使用-ncclGroupStart-和-ncclGroupEnd-2"><a href="#不使用-ncclGroupStart-和-ncclGroupEnd-2" class="headerlink" title="不使用 ncclGroupStart() 和 ncclGroupEnd()"></a>不使用 <code>ncclGroupStart()</code> 和 <code>ncclGroupEnd()</code></h5><div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">ncclBroadcast</span>(sendbuff1, recvbuff1, count1, datatype, root, comm, stream);</span><br><span class="line"><span class="built_in">ncclAllReduce</span>(sendbuff2, recvbuff2, count2, datatype, comm, stream);</span><br><span class="line"><span class="built_in">ncclAllReduce</span>(sendbuff3, recvbuff3, count3, datatype, comm, stream);</span><br></pre></td></tr></table></figure></div>

<ul>
<li><strong>执行顺序</strong>：每个 <code>ncclBroadcast</code> 和 <code>ncclAllReduce</code> 操作都会单独执行，NCCL 会依次启动每个操作并等待前一个操作完成。这样，可能会在每个操作之间产生额外的延迟，尤其是在启动多个 NCCL 操作时。</li>
<li><strong>性能</strong>：每个操作都会带来额外的启动开销，导致总体性能下降。</li>
</ul>
<h3 id="4-额外补充：阻塞组和非阻塞组"><a href="#4-额外补充：阻塞组和非阻塞组" class="headerlink" title="4. 额外补充：阻塞组和非阻塞组"></a>4. <strong>额外补充：阻塞组和非阻塞组</strong></h3><h4 id="阻塞组（Blocking-Group）"><a href="#阻塞组（Blocking-Group）" class="headerlink" title="阻塞组（Blocking Group）"></a><strong>阻塞组（Blocking Group）</strong></h4><p><strong>阻塞组</strong>意味着当调用 <code>ncclGroupEnd()</code> 时，NCCL 会<strong>等待</strong>所有在 <code>ncclGroupStart()</code> 和 <code>ncclGroupEnd()</code> 之间的 NCCL 操作完全完成（包括启动、执行和同步）。在这种模式下，<code>ncclGroupEnd()</code> 会在所有操作完成后返回，意味着直到所有操作完成，你才能继续执行后续的代码。</p>
<h5 id="阻塞组的特点："><a href="#阻塞组的特点：" class="headerlink" title="阻塞组的特点："></a>阻塞组的特点：</h5><ul>
<li>所有组中的 NCCL 操作会按顺序依次发起并等待完成。</li>
<li><code>ncclGroupEnd()</code> 会<strong>阻塞</strong>直到所有 NCCL 操作都完成。</li>
<li>阻塞组适用于你希望在继续执行其他任务之前等待所有 NCCL 操作完成的场景。</li>
</ul>
<p><strong>示例</strong>：</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">ncclGroupStart</span>();  <span class="comment">// 开始一个 NCCL 操作组</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nLocalDevs; i++) &#123;</span><br><span class="line">  <span class="built_in">ncclAllReduce</span>(..., comm[i], stream[i]);  <span class="comment">// 在多个 GPU 上执行操作</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">ncclGroupEnd</span>();  <span class="comment">// 等待所有操作完成</span></span><br></pre></td></tr></table></figure></div>

<p>在这个例子中，<code>ncclGroupEnd()</code> 会阻塞，直到所有的 <code>ncclAllReduce</code> 操作完成。这样可以确保在 <code>ncclGroupEnd()</code> 返回之前，所有的 NCCL 操作都已经被提交和执行。</p>
<h4 id="2-非阻塞组（Non-blocking-Group）"><a href="#2-非阻塞组（Non-blocking-Group）" class="headerlink" title="2. 非阻塞组（Non-blocking Group）"></a>2. <strong>非阻塞组（Non-blocking Group）</strong></h4><p><strong>非阻塞组</strong>指的是当你调用 <code>ncclGroupEnd()</code> 时，NCCL 并不会阻塞直到所有操作完成。相反，<code>ncclGroupEnd()</code> 会尽快返回，并表示操作组已被提交，但后台的 NCCL 操作可能仍在执行中。这种模式允许你执行其他任务，同时在后台继续完成 NCCL 操作。</p>
<ul>
<li><strong>非阻塞组</strong>的核心是当 <code>ncclGroupEnd()</code> 返回时，NCCL 操作可能仍在后台执行。这时你可以检查操作是否完成（通过查看返回状态或使用异步错误检查），而不是等待它们同步完成。</li>
<li>非阻塞组适用于你希望进行并行操作或不希望阻塞主线程的场景，例如，你希望继续执行其他计算或通信操作，而不是等待每个 NCCL 操作完成。</li>
</ul>
<h5 id="非阻塞组的特点："><a href="#非阻塞组的特点：" class="headerlink" title="非阻塞组的特点："></a>非阻塞组的特点：</h5><ul>
<li><code>ncclGroupEnd()</code> 会尽快返回，不会等待所有操作完成。</li>
<li>当组中的操作还在后台执行时，<code>ncclGroupEnd()</code> 会返回 <code>ncclInProgress</code>，表示操作仍在进行。</li>
<li>你可以通过调用 <code>ncclCommGetAsyncError()</code> 来检查操作的状态，以便确认 NCCL 操作是否完成。</li>
<li>如果使用非阻塞组，通常需要在后续代码中进行同步（如调用 <code>cudaStreamSynchronize()</code>）来确保所有操作完成。</li>
</ul>
<p><strong>示例</strong>：</p>
<div class="code-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">ncclGroupStart</span>();  <span class="comment">// 开始一个 NCCL 操作组</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nLocalDevs; i++) &#123;</span><br><span class="line">  <span class="built_in">ncclAllReduce</span>(..., comm[i], stream[i]);  <span class="comment">// 在多个 GPU 上执行操作</span></span><br><span class="line">&#125;</span><br><span class="line">ret = <span class="built_in">ncclGroupEnd</span>();  <span class="comment">// 不等待所有操作完成，尽快返回</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 非阻塞操作完成时的处理</span></span><br><span class="line"><span class="keyword">if</span> (ret == ncclInProgress) &#123;</span><br><span class="line">  <span class="comment">// 检查操作是否完成</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nLocalDevs; i++) &#123;</span><br><span class="line">    <span class="built_in">ncclCommGetAsyncError</span>(comm[i], &amp;state);</span><br><span class="line">    <span class="keyword">while</span> (state == ncclInProgress) &#123;</span><br><span class="line">      <span class="comment">// 等待操作完成</span></span><br><span class="line">      <span class="built_in">ncclCommGetAsyncError</span>(comm[i], &amp;state);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>在这个例子中，<code>ncclGroupEnd()</code> 会尽快返回，并可能返回 <code>ncclInProgress</code>，表示 NCCL 操作仍在后台进行。你可以通过调用 <code>ncclCommGetAsyncError()</code> 来轮询每个操作的状态，并在操作完成时继续执行后续代码。</p>
]]></content>
      <categories>
        <category>NCCL</category>
      </categories>
      <tags>
        <tag>NCCL</tag>
        <tag>代码阅读</tag>
      </tags>
  </entry>
</search>
